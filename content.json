{"pages":[{"title":"關於","permalink":"https://jiangink.github.io/about/index.html","text":"個人信息： 網絡暱稱：Ink Jiang . 年齡區間：二十來歲 . 目前職業：服務端程序猿 . 專攻領域：Node.JS、 Express、 MySQL、 Redis、 LinuxC 工作經歷： 2016.02～至今成都某醫藥行業互聯網公司從事於服務器後台醫藥業務功能開發 2013.04～2015.12北京某網絡信息安全公司從事於內網安全接入網關服務器後台研發 2012.05～2013.02北京某通用無線電軟件公司從事Linux平台下軟件無線電桌面應用控制軟件研發 聯繫方式：&#106;&#105;&#x61;&#x6e;&#x67;&#105;&#110;&#x6b;&#64;&#x66;&#111;&#120;&#x6d;&#97;&#105;&#108;&#46;&#99;&#111;&#109;"},{"title":"categories","permalink":"https://jiangink.github.io/categories/index.html","text":""},{"title":"tags","permalink":"https://jiangink.github.io/tags/index.html","text":""}],"posts":[{"title":"Python开发环境预备","permalink":"https://jiangink.github.io/2018/01/28/PythonEnv/","text":"以pyenv及pipenv + autoenv的方式开发Python; 所涉及到的工具有： pyenv 版本管理 pipenv 开发工作流管理 autoenv 基于目录的环境自动构建 Python版本管理Pyenv 是一种可以实现安装不同版本Python并可在不同版本间任意切换的管理工具。其功能类似于在Node.JS中的NVM版本管理工具； Python项目的环境及包管理Pipenv是python项目中的包管理工具，通过pipenv安装的pyhon包将被记录于pipfile当中，方便项目中包的依赖管理及安装部署;Pipenv工具是集pip, pipfile, virtualenv于一身，故它也实现了项目目录的虚拟环境创建，使得其项目的运行环境能够满足独立且隔离，不受系统及其他环境的影响;Pipenv功能类似于在Node.JS中的NPM包管理工具;123456# 1. 在第一次执行`pipenv install`时，会在该项目创建对应的虚拟环境;# 2. 同时生成Pipfile及Pipfile.lock文件，用于管理项目中Python包依赖;# 3. 第一次安装依赖包,需要指定`--two`或`--three`参数指定python2/3.x的运行环境$ pipenv install --three &lt;packageName&gt; Python项目环境的自启动Autoenv可以实现在进入项目目录时，将自启动需要的运行环境; 1 . 安装autoenv使用pip方式安装存在问题，会导致terminal异常退出，之后的版本可能会有更新;目前是以git方式进行安装：12$ git clone git://github.com/kennethreitz/autoenv.git ~/.autoenv$ echo &apos;source ~/.autoenv/activate.sh&apos; &gt;&gt; ~/.bashrc 2 . 创建.env文件在已有的pipenv项目中运行pipenv shell,会得到虚拟环境路径信息，通过信息创建.env运行环境文件; 1$ echo &quot;source /home/&#123;userName&#125;/.local/share/virtualenvs/&#123;projectName&#125;/bin/activate&quot; &gt; &#123;projectName&#125;/.env 3 . 再次进入项目目录，将自启动该项目的虚拟环境"},{"title":"Serverless技术相关梳理","permalink":"https://jiangink.github.io/2017/12/14/Serverless/","text":"在Serverless架构中，应用业务逻辑将基于FaaS（function as a service）结构形成多个相互独立的功能组件，并以API服务的形式向外提供服务；serverless让开发者无需关注资源的获取和运维，按需调用付费,节省成本； 注：FaaS属于serverless的子集，也是实现serverless化的核心服务；FaaS关键特征：事件驱动、细粒度调用、实现弹性伸缩、无需管理服务器等底层资源； 一、Serverless 优势： 节约使用成本：根据调用次数计费，节省成本，业务高峰弹性扩容，将资源分享给其他客户； 简化设备运维：底层硬件透明化，无需运维； 提升可维护性：serverless架构采用多种第三方功能服务模块组合，结构清晰，维护性高； 二、Serverless 行业场景： 低频请求场景：（物联网行业）物联网中，设备传输数据量小，且往往数据传输频率固定，所以采购serverless低频的资源即可满足计算需求； 流量突发场景：（移动互联网行业）互联网中，经常会遇到突发流量场景，如电商网站促销活动期间流量爆增，但平时的流量会回到均值，传统架构需要扩展硬件能力以支持突发情况，但平时这些资源会被浪费，采用serverless的弹性扩展，可以在高峰时扩容，高峰后释放不用的资源，节省成本； 基于事件的数据处理：在图片视频处理后端进行视频转码、图片处理等等任务时，可结合对象存储的触发器，执行severless的事件处理函数进行对应任务的处理； 三、Severless 与 微服务：两者都强调系统的解耦，微服务是以业务的粒度进行拆分，而serverless则是以函数function的粒度进行拆分，serverless的粒度更加细化； 四、serverless 与 运维：Serverless并不等于没有运维，只是运维的工作由机器设备运维转变为业务运维，运维不再关心过去的配置、操作系统、内核网络等问题，而去关心系统业务的合理性、业务间的依赖关系、业务的报警监控等，serverless将推动 DevOps 的发展； 五、Serverless 业内竞品: 亚马逊 AWS Lambda 微软 Azure Function 谷歌 Google Cloud Functions 阿里云 Function Compute 六、基于 serverless 的程序框架 https://github.com/serverless/serverless 七、阿里云 Serverless 生态：目前阿里云围绕着 Serverless 推出的平台服务： 1. 函数计算(function computer)事件驱动的全托管计算服务， 无需管理服务器基础设施：无需考虑服务器规格、配置、升级、宕机、负载均衡、等等因素； 弹性伸缩，按需付费 以事件驱动方式与对象存储、api网关无缝链接； 2. 对象存储 (Object Storage Service) 图片、音视频、日志等海量文件存储； 网页或移动应用静态文件的分离； 可对OSS云端数据进行媒体转码或图片处理； 对象存储触发函数计算事件处理函数： https://help.aliyun.com/document_detail/53097.html?spm=5176.doc51784.6.553.nvicSg 3. API网关 (API Gateway) API 录入维护统一管理 按实际调用付费 采用分布式部署，自动扩展，能够满足大规模访问场景 提供权限管理、流量管控、访问监控、安全预警、计量统计等功能 以函数计算作为Api网关的后端服务: 流程：创建事件处理函数 -&gt; 将函数授权给API网关 -&gt; 配置API https://help.aliyun.com/document_detail/54788.html?spm=5176.product29462.6.584.7YpH2m 4. 表格存储 (Table Store)一种NoSQL数据存储服务，提供海量结构化数据存储和实时访问；特性： 表格存储以实例和表的形式组织数据，通过分片和负载均衡实现规模的无缝扩展； 屏蔽底层硬件故障及错误，可快速恢复，提供高可用性； 数据被存储在SSD中且有多个备份，提供快速访问性能及可靠性； 通过表格存储触发器，可实现table store stream 和函数计算的自动对接： 流程：创建开通Stream流的数据表 -&gt; 创建函数计算事件处理函数 -&gt;在数据表下创建触发器 https://help.aliyun.com/document_detail/60304.html?spm=5176.doc60304.6.593.IoKlsE 5. 日志服务 (Log Service)提供日志类数据的采集、消费、投递及查询分析等功能 函数计算访问日志服务：https://help.aliyun.com/document_detail/61023.html?spm=5176.doc60984.6.558.fjRIfN 6. 批量计算 (BatchCompute)一种适用于大规模并行批处理作业的分布式云服务。可支持海量作业并发规模，系统自动完成资源管理、作业调度和数据加载，并按实际使用量计费； 批量计算采用对象存储服务(OSS)作为持久化存储，同时批量计算可以通过文件接口访问OSS中的资源； 批量计算的程序默认运行于VM中，也支持采用Docker容器； 批量计算中提交的作业(Job)包含多个任务(Task); 八、函数计算无状态服务实现有状态逻辑大体思路： 在函数计算初始化阶段在其本地启动一个Express服务； 创建一个函数计算事件处理函数； 前置服务API Gateway将关联 该函数计算事件处理函数； 当前端或移动端发送请求给Api网关时，网关会将请求以事件触发给函数计算； 此时函数计算的事件处理函数接收到了event事件数据源； 函数计算对此事件数据源进行解析，需要解析出URL、Header、Method、queryParams、body等请求信息，并向本地Express服务发送该http请求； 函数计算中启动的Express服务，处理完请求后返回结果，事件处理函数会对该结果进行一次输出格式的封装，以便Api网关能够识别，最终结果通过callback返回给Api网关； 代码实例：1234567891011121314151617181920212223242526272829303132333435363738394041424344&apos;use strict&apos;require(__dirname + &quot;/bin/www&quot;); // 初始化阶段在本地启动Express服务const request = require(&apos;request&apos;);const Promise = require(&apos;bulebird&apos;);const httpRequest = Promise.promisify(request);// 函数计算 事件处理函数入口module.exports.handler = (event, ctx, cb) =&gt; &#123; // 1. 入参判空等校验容错处理 ... // 2. 获取事件请求参数 let options = &#123; url: event.url, headers: event.headers, method: event.httpMethod, qs: event.queryParams, body: event.body ... &#125;; // 3. 发送请求给本地Express服务 return httpRequest(options) .then(res =&gt; &#123; ... // 4. 按照API网关识别格式返回结果 cb(null, &#123; statusCode: res.statusCode, isBase64Encoded: isBase64Encoded, headers: res.headers, body: res.body &#125;); &#125;) .catch(err =&gt; &#123; ... cb(null, &#123; statusCode: 500, isBase64Encoded: false, headers: &#123;duration: duration&#125;, body: JSON.stringify(err) &#125;); &#125;); &#125;;"},{"title":"关于信息过载","permalink":"https://jiangink.github.io/2017/06/24/DesignP109/","text":"然后我明白了一些东西。虽然今天的世界据说在一种信息过剩的状态，实际上可能并不富裕。泛滥的只是媒介中零零碎碎的信息，每个碎片中的信息量实际上很小。在这种半生不熟的信息中，大脑怎么会不感到压抑呢？大脑的压力不是因为数量，而是因为有限的质量。在媒介进化及其对新闻材料和数据拼命搜集的背景下，世界上发生的所有事情都像草坪被割草机修剪一样，信息碎片如空中乱飞的草屑，在不同地方之间飞来飞去。这些破碎的信息附着在我们豆腐般的大脑上，像撒得太多的调料，把整个表面搞得含糊不清了。一时间，我们觉得自己知道了很多，但大脑表面的那些信息当被你加在一起时却并没有多少。相反，我们通过脚底感性，快乐的体验得到的信息量却是巨大的。人类的大脑喜欢任何需要大量信息的东西，其扩展能力焦急地等待着感知世界，以充分消耗它巨大的接受力。而此潜在能力在今天处于一种被极度压制的状态，这是我们都遭遇到的信息压力的一个来源。 《设计中的设计》原研哉 P109"},{"title":"Node 多进程初探","permalink":"https://jiangink.github.io/2017/05/12/NodeForkProcess/","text":"前段时间在使用 Node 实现一个系统的定时任务功能，定时任务数量较多，且其中存在一些需要 CPU 计算的任务，一开始这些任务被放置在 Node 单线程中运行，可是随着任务数的增长，单一线程渐渐吃不消，任务运行高发时段，CPU 会飙至百分之九十以上，同时造成 Web 页面查询请求卡顿甚者请求无响应； 问题分析众所周知，Node.JS 的事件循环对于处理 I/O 密集型任务十分擅长，但是当遇到 CPU 密集型任务，由于事件循环是按照队列顺序执行，若队列中任何一个事件任务没有被完成，那么其他待进入队列的事件任务将不被执行，换句话说，CPU 密集型任务的耗时操作阻塞了事件队列后，将会使得事件任务的处理效率下降，严重时会使得事件循环停滞不动； 处理方案Node 作为一种的单线程语言，先天的无法发挥服务器多核 CPU 的性能，而要处理上述问题，一般有两种方式： 多进程: child_process、 cluster 多线程: threads-a-gogo 作为初探，先来讲讲多进程的通用方案 child_process，我们这里通过 fork 的方式创建一个独立负责定时任务处理的子进程，这样一来将任务处理的逻辑从主进程中分离，然后主进程要做的只是运用 IPC 进程间通讯管理分发要执行的任务给子进程； 目录概述项目大致目录结构如下：1234567891011121314151617.├── app.js├── modules│ └── childProcess.js├── swagger│ └── controller.js├── usecase│ └── worker│ └── index.js├── datasource│ ├── worker│ │ ├── task1Handler.js│ │ ├── task2Handler.js│ │ ├── task3Handler.js│ │ ├── task4Handler.js│ │ └── task5Handler.js│ └── 子进程的创建 创建工作子进程，并将子进程实例带入对应入口的 req 属性，便于获取；12345678910// modules/childProcess.jsvar child_process = require('child_process');var forkChild = child_process.fork(process.cwd() + '/usecase/task/wokerIndex.js');module.exports = function(req, res, next) &#123; var urlPath = /^\\/api\\/v\\d+\\/subProcessHandler/; if (urlPath.test(req.path) &amp;&amp; (req.method === 'POST')) &#123; req.childProcess = forkChild; &#125; return next();&#125;; 中间件添加 将子进程创建的中间件加入到服务应用的实例中；123// app.jsvar childProcess = require(process.cwd() + '/modules/childProcess');app.use(childProcess); 应用层接口 任务处理入口，运用进程间通信将执行参数传入子进程；123456// swagger/controller.jsexports.postSubProcessHandler = (req, res, next) =&gt; &#123; let childProcess = req.childProcess; childProcess.send(req.body); res.end();&#125; 定时任务分发 工作子进程通过接收主进程执行参数，分发执行任务；123456789101112// usecase/worker/index.jslet workerTaskList = &#123; taskType1: require('../../datasource/worker/task1Handler'), taskType2: require('../../datasource/worker/task2Handler'), taskType3: require('../../datasource/worker/task3Handler'), taskType4: require('../../datasource/worker/task4Handler'), taskType5: require('../../datasource/worker/task5Handler')&#125;;process.on('message', msg =&gt; &#123; workerTaskList[msg.taskType](msg.params);&#125;); 多进程的集群管理采用 fork 单独子进程处理任务，以上述的方式即可满足，可若要充分利用 CPU 多核性能资源，就需要 fork 多个工作进程，但是子进程数量变多，就存在诸如进程协调管理、负载均衡等问题； 此时就需要考虑使用 Node Cluster 模块，该模块为 Node 自身内置模块，用于构建多进程并行化程序，实现用于负载均衡的集群； 当然使用基于 Cluster 模块的 PM2 集群控制工具也是一种很好的选择； 关于 Cluster 与 PM2 ，待后续文章有机会讲解，本文就此告一段落，如有不妥之处，望指出，感谢。"},{"title":"以 Node 方式, CSV 转 JSON","permalink":"https://jiangink.github.io/2017/05/07/NodeCsvToJson/","text":"简述需求:有时，我们会在项目中遇到导入文档数据的功能需求，具体的比如可能会需要将 excel 表格数据导入到项目系统的数据库中，通过读取文件内容，解析并转换数据，最终存储至数据库的表中； 可行方案：针对于上述的这一系列较为繁琐的处理过程，在 Node 开发中，NPM 提供了 csv to json 的工具模块，它能够将 CSV 标准文件转换成 JSON 数据，从而方便程序进一步的数据存储； 具体的思路： 需要将 Excel 表格文件转换为 CSV 标准文件； 使用 csvtojson 包对 CSV 标准文件进行 JSON 化数据转换； 将 JSON 数据存入数据库对应表当中； 注: 使用 Mac 版 MicroOffice 将 EXCEL 表格文件转为 CSV 格式文件后，在终端中打开 CSV 文件中文出现乱码? 也不知为何，微软在做 CSV 文件转换时，对字符集支持有限，导致中文乱码；该问题使用 Libre Office 则可以解决，Libreoffice 在转换 CSV 时提供字符编码转换设置，转换时设置 UTF-8 即可, 据说使用 Google Sheets 可以达到相同效果； 操作说明安装：npm install csvtojson --save 实例：123456789101112131415161718192021222324252627282930313233343536/** csv file account,password,company,username,phone test1@demo.com,123456,安徽公司,小明,13112310168 test2@demo.com,123456,浙江公司,小伟,13327223199**/const csvFilePath='&lt;path to csv file&gt;';const csv=require('csvtojson');let userData = [];csv().fromFile(csvFilePath).on('json',(jsonObj)=&gt;&#123; // 结合 CSV 头部属性逐行将内容转换为 JSON 对象 userData.push(jsonObj);&#125;).on('done',(error)=&gt;&#123; console.log('result: ' + JSON.stringify(userData)); /** [ &#123; account:'test1@demo.com', password:'123456', company:'安徽公司', username:'小明', phone: '13112310168' &#125;, &#123; account:'test2@demo.com', password:'123456', company:'浙江公司', username:'小伟', phone: '13327223199' &#125; ] **/&#125;)"},{"title":"Node 任务定时器的使用","permalink":"https://jiangink.github.io/2017/04/02/NodeSchedule/","text":"什么是任务定时器： 能够在指定时间定期地执行命令、脚本或者程序，可以被用于系统的自动化维护及管理的工具； 最常见的任务定时器当属 Cron ，该词源于希腊语 chronos，原意是时间；在类 Unix 系统中，可以通过 crontab 命令设置定时任务执行的时间周期，然后 cron 的守护进程会在后台实时的检测是否有需要执行的任务，通常这些需要执行的任务被称为 cron jobs; 定时器的参数说明：Cron 在类 Unix 系统中有很多的实现，如：cronie、bcron、dcron、fcron 等；虽然实现有多种，但是基于 cron 风格的时序参数结构却是相似的； 这里展示了一个 cron 的时序参数格式： 123456789* * * * * *┬ ┬ ┬ ┬ ┬ ┬│ │ │ │ │ |│ │ │ │ │ └ day of week (0 - 7) (0 or 7 is Sun)│ │ │ │ └───── month (1 - 12)│ │ │ └────────── day of month (1 - 31)│ │ └─────────────── hour (0 - 23)│ └──────────────────── minute (0 - 59)└───────────────────────── second (0 - 59, OPTIONAL) * 表示通配符，匹配任意，当秒为 ‘*’ 时，代表任意秒数都会触发执行； 观察这组参数说明的过程中，发现了一个有趣的问题: Q: 在 day of week 参数上，0 或 7 其实都可以代表星期日(Sunday)，WHY?A: 原因在于 cron 的众多实现版本里，对于这个参数，部分实现的设置为：0 - 6 =&gt; Sunday - Saturday，而另一部分实现的设置为：1 - 7 =&gt; Monday - Sunday，为了兼容，确保两种设置都正确，所以如上； 具体参数设置的试例：123456789101112131415161718192021222324* * * * * # 例1: 每1分钟执行一次3,15 * * * * # 例2: 每小时的第3和第15分钟执行3,15 8-11 * * * # 例3: 在每天上午8点到11点的第3和第15分钟执行3,15 8-11 */2 * * # 例4：每隔两天的上午8点到11点的第3和第15分钟执行3,15 8-11 * * 1 # 例5：每周一的上午8点到11点的第3和第15分钟执行30 21 * * * # 例6：每晚的21时30分执行45 4 1,10,22 * * # 例7：每月的1、10、22日的4时45分执行 10 1 * * 6,0 # 例8：每周六、周日的1时10分执行0,30 18-23 * * * # 例9：每天 18:00 至 23:00 之间每隔30分钟执行0 23 * * 6 # 例10：每星期六晚 23:00 执行* */1 * * * # 例11：每隔1小时执行* 23-7/1 * * * # 例12：晚上23时到次日7时之间每隔1小时执行 任务定时器在 Node 中的使用在 Node.JS 中，可以使用 Node-Schedule 这个 npm 包来进行任务定时器的操作： 描述： A cron-like and not-cron-like job scheduler for Node. 安装方式：1npm install node-schdule 如何使用：任务定时器的创建：123456var schedule = require('node-schedule');var j = schedule.scheduleJob('42 * * * *', function()&#123; // 每小时的第42分钟被执行 console.log('The answer to life, the universe, and everything!');&#125;); scheduleJob() 方法的回调函数用于实现具体定时任务执行的内容; 任务定时器的注销：1j.cancel(); 基于JS Date类型的时间参数设置：1234567var schedule = require('node-schedule');// 2012年12月21日5时30分执行var date = new Date(2012, 11, 21, 5, 30, 0);var j = schedule.scheduleJob(date, function()&#123; console.log('The world is going to end today.');&#125;); 注意： 在使用 Date 设置参数是，月份的设定范围是 0~11 ，其中0代表一月，11代表十二月； 4.2.4 递归循环任务的设置：1234567891011121314var schedule = require('node-schedule');var rule = new schedule.RecurrenceRule();// 每小时的第 42 分钟执行rule.minute = 42;// rule.dayOfWeek = 5;// rule.month = 6;// rule.dayOfMonth = 15;// rule.hour = 1;// rule.second = 0var j = schedule.scheduleJob(rule, function()&#123; console.log('The answer to life, the universe, and everything!');&#125;); 指定时间范围的设置：123456789var rule = new schedule.RecurrenceRule();// 每个月的星期四、五、六、日的17点整执行rule.dayOfWeek = [0, new schedule.Range(4, 6)];rule.hour = 17;rule.minute = 0;var j = schedule.scheduleJob(rule, function()&#123; console.log('Today is recognized by Rebecca Black!');&#125;); RecurrenceRule 实例的每个cron属性可接受以数组的形式添加多个时间数值，Range()方法可指定一个范围的开始值及结束值; 通过对象字面量的方式设置：1234// 每周日的14时30分执行var j = schedule.scheduleJob(&#123;hour: 14, minute: 30, dayOfWeek: 0&#125;, function()&#123; console.log('Time for tea!');&#125;); 任务定时器的开始及结束时间设置：12345678910// 5秒后任务开始执行且10秒后任务结束，任务在此过程中每秒执行一次let startTime = new Date(Date.now() + 5000);let endTime = new Date(startTime.getTime() + 5000);var j = schedule.scheduleJob(&#123; start: startTime, end: endTime, rule: '*/1 * * * * *' &#125;, function()&#123; console.log('Time for tea!');&#125;); 参考：http://www.cnblogs.com/zhongweiv/p/node_schedule.htmlhttp://www.codexiu.cn/javascript/blog/16175/https://zh.wikipedia.org/wiki/Cron"},{"title":"使用 Gulp 构建工程","permalink":"https://jiangink.github.io/2016/01/29/BuildingWithGulp/","text":"gulp 是什么？ 如果曾做过 Linux C 相关开发，一定对于编译构建工具 Automake 有一定的了解，我们通过在工程中编写 makefile 脚本文件，实现工程代码的检查、编译以及安装等等。 而在如今前端工程化的背景下，愈来愈强调前端的工作流程，如果有一种工具，能够如 automake 一般，能够帮助我们处理检测、编译、自动发布以及文件压缩、文件Hash、等等事宜，那一定能简化不少工作，而 gulp 即是这样的一种工具。 Gulp 是一个基于 Node.js 的流式构建工具，有别与 Grunt 基于 I/O 的构建方式，Grunt 在处理过程中会产生一些中间态的临时文件，而任务会最终基于这些临时文件生成构建后的结果文件。在这个过程中 Grunt 会进行大量的 I/O 读写操作，那么如果遇到大型项目工程，Grunt 的构建处理速度将大大拖长，这也就是为什么 Gulp 渐渐取代 Grunt 的原因之一，如果想了解更多，可以试着阅读 《 gulp vs grunt 》 这篇文章。 由于 Gulp 通过代码优于配置的策略不仅使得任务易于编写，同时也易于维护和阅读。Gulp 基于的 NodeJS 的流，使用管道 Pipe 的概念，这一级的输出作为下一级的输入，中间无需在硬盘上写入任何临时文件和目录，让任务构建的更加迅速。 Gulp 的基本安装 ：这里我默认您已经安装了 Node.js 以及 NPM 包管理工具，首先需要全局安装 Gulp 工具： $ npm install gulp -g 然后，需要在已有的项目中安装 gulp 的依赖包（前提是项目中已存在 package.json 文件）： $ npm init #如果项目中没有 package.json 文件，需要执行此命令生成，有则忽略； $ npm install gulp --save-dev #安装 gulp 依赖包 一切顺利，将会发现 package.json 文件中的 devDependencies 项里多了一个 gulp 子项，且 node_modules 中也多了一个 gulp 的依赖包； Gulp 插件的安装 ：要使用 Gulp 中的功能，必须依赖于 Gulp 的插件，在 Gulp 中每个插件完成一个具体的功能任务，而实际的构建工作往往是多个任务的流程，所以就要涉及到多个插件的安装，那么具体要用到哪些插件呢？这里我大概列出了常用的几项，仅供参考。 Sass compile (gulp-ruby-sass) # css 预处理器编译 Autoprefixer (gulp-autoprefixer) # css 浏览器兼容前缀处理 Minify CSS with cssnano (gulp-cssnano) # css 文本压缩 JSHint (gulp-jshint) # JS 代码检测 Concatenation (gulp-concat) # 代码文本合并 Uglify (gulp-uglify) # 代码文本压缩 Compress images (gulp-imagemin) # 图片资源压缩 LiveReload (gulp-livereload) # 文件变动页面重载 Caching of images so only changed images are compressed (gulp-cache) # 图片资源缓存 Notify of changes (gulp-notify) # 任务完成通知提示 Clean files for a clean build (del) # 清理构建目录及文件 安装方式与安装 Gulp 依赖包命令一致： $ npm install [package-name] --save-dev Gulp 任务文件的配置 ：这里我直接使用本人 Github 上的一个聊天室项目 node_chat 来进行讲解。 // 加载插件 var del = require(&apos;del&apos;), gulp = require(&apos;gulp&apos;), cache = require(&apos;gulp-cache&apos;), jshint = require(&apos;gulp-jshint&apos;), concat = require(&apos;gulp-concat&apos;), uglify = require(&apos;gulp-uglify&apos;), rename = require(&apos;gulp-rename&apos;), notify = require(&apos;gulp-notify&apos;), cssnano = require(&apos;gulp-cssnano&apos;), imagemin = require(&apos;gulp-imagemin&apos;), livereload = require(&apos;gulp-livereload&apos;), autoprefixer = require(&apos;gulp-autoprefixer&apos;); // 设置源路径 var srcPaths = { scripts: [&apos;./public/javascripts/*.js&apos;], images: &apos;./public/images/*&apos;, styles: &apos;./public/stylesheets/*.css&apos; }; // 设置目标路径 var destPaths = { all: &apos;./build/**&apos;, scripts: &apos;./build/script&apos;, images: &apos;./build/image&apos;, styles: &apos;./build/style&apos; }; // 任务：脚本代码验证 gulp.task(&apos;analysis&apos;, function() { return gulp.src(srcPaths.scripts) .pipe(jshint()) .pipe(jshint.reporter(&apos;default&apos;)) .pipe(notify({message: &apos;Analysis task complete.&apos;})); }); // 任务1. 脚本合并与压缩 gulp.task(&apos;scripts&apos;, [&apos;analysis&apos;], function() { return gulp.src(srcPaths.scripts) .pipe(concat(&apos;main.js&apos;)) .pipe(gulp.dest(destPaths.scripts)) .pipe(rename({suffix: &apos;.min&apos;})) .pipe(uglify()) .pipe(gulp.dest(destPaths.scripts)) .pipe(notify({message: &apos;Scripts task complete.&apos;})); }); // 任务2. 资源图片压缩 gulp.task(&apos;images&apos;, function() { return gulp.src(srcPaths.images) // 压缩 imagemin 并缓存 cache 图片 .pipe(cache(imagemin({ optimizationLevel: 3, progressive: true, interlaced: true }))) .pipe(gulp.dest(destPaths.images)) .pipe(notify({message: &apos;Images task complete.&apos;})); }); // 任务3. 样式表的压缩 gulp.task(&apos;styles&apos;, function() { return gulp.src(srcPaths.styles) .pipe(autoprefixer(&apos;last 2 version&apos;, &apos;safari 5&apos;, &apos;ie 8&apos;, &apos;ie 9&apos;, &apos;opera 12.1&apos;, &apos;ios 6&apos;, &apos;android 4&apos;)) .pipe(gulp.dest(destPaths.styles)) .pipe(rename({suffix: &apos;.min&apos;})) .pipe(cssnano()) .pipe(gulp.dest(destPaths.styles)) .pipe(notify({message: &apos;Styles task complete.&apos;})); }); // 任务：清理构建目录 gulp.task(&apos;clean&apos;, function(cb) { // 清空 build 目录 return del([destPaths.scripts, destPaths.images, destPaths.styles], cb); }); // 默认任务 gulp.task(&apos;default&apos;, [&apos;clean&apos;], function() { gulp.start(&apos;scripts&apos;, &apos;images&apos;, &apos;styles&apos;); }); // 任务：监听文件变动 gulp.task(&apos;watch&apos;, function() { // 监听文件变动 gulp.watch(srcPaths.scripts, [&apos;scripts&apos;]); gulp.watch(srcPaths.images, [&apos;images&apos;]); gulp.watch(srcPaths.styles, [&apos;styles&apos;]); // 创建 livereload 监听服务 livereload.listen(); // 若有改变自动重载刷新 gulp.watch([destPaths.all]).on(&apos;change&apos;, livereload.changed); });"},{"title":"WebSocket学习笔记","permalink":"https://jiangink.github.io/2016/01/19/WebSocketNote/","text":"HTTP 协议是一种无状态协议，服务器端本身不具备识别客户端的能力，必须借助外部机制，比如 session 和 cookie 才能与特定客户端保持对话。那么如果我们遇到一种需求，它要求保证服务器端与客户端持续的数据交换，比如网络聊天，那再使用 HTTP 协议就会有不便，这时我们就需要用到 Websocket API了。 概述 ：WebSocket 的主要作用：允许服务器端与客户端进行全双工（full-duplex）的通信。举例来说，HTTP 协议有点像发电子邮件，发出后必须等待对方回信；WebSocket 则是像打电话，服务器端和客户端可以同时向对方发送数据，它们之间存着一条持续打开的数据通道。 WebSocket 协议完全可以取代 Ajax 方法，用来向服务器端发送文本和二进制数据，而且还没有“同域限制”。 WebSocket 不使用 HTTP 协议，浏览器在发送 Websocket 请求时，会将 HTTP 协议升级（Upgrade）为 webSocket 自己的协议。 WebSocket 协议用 ws 表示。此外，还有 wss 协议，表示加密的 WebSocket 协议，对应 HTTPs 协议。 在完成TCP协议三次握手之后，WebSocket 协议就在 TCP 协议之上，开始数据的传送。 WebSocket 协议需要服务器支持，目前比较流行的实现是基于 node.js 的 socket.io ，至于浏览器端，目前主流浏览器都支持 WebSocket 协议。 客户端 ：浏览器端对WebSocket协议的处理，主要归纳为三件事： 建立连接和断开连接 发送数据和接收数据 处理错误 建立连接和断开连接首先，客户端要检查浏览器是否支持 WebSocket ，使用的方法是查看 window 对象是否具有 WebSocket 属性。 if(window.WebSocket != undefined) { // WebSocket代码 } 然后，开始与服务器建立连接（这里假定服务器就是本机的1740端口，需要使用ws协议）。 if(window.WebSocket != undefined) { var connection = new WebSocket(&apos;ws://localhost:1740&apos;); } 建立连接以后的 WebSocket 实例对象（即上面代码中的connection），有一个readyState属性，表示目前的状态，可以取4个值： 0： 正在连接 1： 连接成功 2： 正在关闭 3： 连接关闭 握手协议成功以后，readyState 就从 0 变为 1 ，并触发 open 事件，这时就可以向服务器发送信息了。我们可以指定 open 事件的回调函数。 connection.onopen = wsOpen; function wsOpen (event) { console.log(&apos;Connected to: &apos; + event.currentTarget.URL); } 关闭 WebSocket 连接，会触发 close 事件。 connection.onclose = wsClose; function wsClose () { console.log(&quot;Closed&quot;); } connection.close(); 发送数据和接收数据连接建立后，客户端通过 send 方法向服务器端发送数据。 connection.send(message); 除了发送字符串，也可以使用 Blob 或 ArrayBuffer 对象发送二进制数据。 // 使用ArrayBuffer发送canvas图像数据 var img = canvas_context.getImageData(0, 0, 400, 320); var binary = new Uint8Array(img.data.length); for (var i = 0; i &lt; img.data.length; i++) { binary[i] = img.data[i]; } connection.send(binary.buffer); // 使用Blob发送文件 var file = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;).files[0]; connection.send(file); 客户端收到服务器发送的数据，会触发 message 事件。可以通过定义 message 事件的回调函数，来处理服务端返回的数据。 connection.onmessage = wsMessage; function wsMessage (event) { console.log(event.data); } 上面代码的回调函数 wsMessage 的参数为事件对象 event ，该对象的 data 属性包含了服务器返回的数据。 如果接收的是二进制数据，需要将连接对象的格式设为 blob 或 arraybuffer。 connection.binaryType = &apos;arraybuffer&apos;; connection.onmessage = function(e) { // ArrayBuffer对象有byteLength属性 console.log(e.data.byteLength); }; 处理错误如果出现错误，浏览器会触发WebSocket实例对象的error事件。 connection.onerror = wsError; function wsError(event) { console.log(&quot;Error: &quot; + event.data); } 服务端 ：服务器端需要单独部署处理 WebSocket 的代码。下面用 node.js 搭建一个服务器环境。 var http = require(&apos;http&apos;); var server = http.createServer(function(request, response) {}); 假设监听 1740 端口。 server.listen(1740, function() { console.log((new Date()) + &apos; Server is listening on port 1740&apos;); }); 接着启动 WebSocket 服务器。这需要加载websocket库，如果没有安装，可以先使用npm命令安装。 var WebSocketServer = require(&apos;websocket&apos;).server; var wsServer = new WebSocketServer({ httpServer: server }); WebSocket服务器建立 request 事件的回调函数。 var connection; wsServer.on(&apos;request&apos;, function(req){ connection = req.accept(&apos;echo-protocol&apos;, req.origin); }); 上面代码的回调函数接受一个参数 req ，表示 request 请求对象。然后，在回调函数内部，建立WebSocket连接connection。接着，就要对connection的message事件指定回调函数。 wsServer.on(&apos;request&apos;, function(r){ connection = req.accept(&apos;echo-protocol&apos;, req.origin); connection.on(&apos;message&apos;, function(message) { var msgString = message.utf8Data; connection.sendUTF(msgString); }); }); 最后，监听用户的 disconnect 事件。 connection.on(&apos;close&apos;, function(reasonCode, description) { console.log(connection.remoteAddress + &apos; disconnected.&apos;); }); 使用 ws 模块，部署一个简单的 WebSocket 服务器非常容易。 var WebSocketServer = require(&apos;ws&apos;).Server; var wss = new WebSocketServer({ port: 8080 }); wss.on(&apos;connection&apos;, function connection(ws) { ws.on(&apos;message&apos;, function incoming(message) { console.log(&apos;received: %s&apos;, message); }); ws.send(&apos;something&apos;); }); Socket.IO ：Socket.io 是目前最流行的 WebSocket 实现，包括服务器和客户端两个部分。它不仅简化了接口，使得操作更容易，而且对于那些不支持 WebSocket 的浏览器，会自动降为 Ajax 连接，最大限度地保证了兼容性。它的目标是统一通信机制，使得所有浏览器和移动设备都可以进行实时通信。 第一步，在服务器端的项目根目录下，安装 socket.io 模块。 $ npm install socket.io 第二步，在根目录下建立 app.js ，并写入以下代码（假定使用了Express框架）。 var app = require(&apos;express&apos;)(); var server = require(&apos;http&apos;).createServer(app); var io = require(&apos;socket.io&apos;).listen(server); server.listen(80); app.get(&apos;/&apos;, function (req, res) { res.sendfile(__dirname + &apos;/index.html&apos;); }); 上面代码表示，先建立并运行 HTTP 服务器。Socket.io 的运行建立在 HTTP 服务器之上。 第三步，将 Socket.io 插入客户端网页。 &lt;script src=&quot;/socket.io/socket.io.js&quot;&gt;&lt;/script&gt; 然后，在客户端脚本中，建立 WebSocket 连接。 var socket = io.connect(&apos;http://localhost&apos;); 由于本例假定 WebSocket 主机与客户端是同一台机器，所以 connect 方法的参数是 http://localhost。接着，指定 news 事件（即服务器端发送news）的回调函数。 socket.on(&apos;news&apos;, function (data){ console.log(data); }); 最后，用 emit 方法向服务器端发送信号，触发服务器端的 anotherNews 事件。 socket.emit(&apos;anotherNews&apos;); 请注意，emit 方法可以取代 Ajax 请求，而on方法指定的回调函数，也等同于 Ajax 的回调函数。 第四步，在服务器端的 app.js，加入以下代码。 //广播信息给除当前用户之外的用户 socket.broadcast.emit(&apos;user connected&apos;); //广播给全体客户端 io.sockets.on(&apos;connection&apos;, function (socket) { socket.emit(&apos;news&apos;, { hello: &apos;world&apos; }); socket.on(&apos;anotherNews&apos;, function (data) { console.log(data); }); }); 上面代码的 io.sockets.on 方法指定 connection 事件（ WebSocket 连接建立）的回调函数。在回调函数中，用 emit 方法向客户端发送数据，触发客户端的 news 事件。然后，再用 on 方法指定服务器端anotherNews 事件的回调函数。 不管是服务器还是客户端，socket.io 提供两个核心方法：emit方法用于发送消息，on方法用于监听对方发送的消息。 参考： JavaScript标准参考教程"},{"title":"NodeJS应用部署初探","permalink":"https://jiangink.github.io/2016/01/13/DeployNodeApp/","text":"尝试使用 MongoLab、Redis4You、Heroku 来部署一个 node 应用。 首先，使用 MongoLab ：MongoLab 与 MongoHQ 是当今比较流行的 MongoDB 云数据库提供商。MongoLab 的免费套餐提供了 500MB 的存储空间，MongoLab 的使用比较简单明了，这里简要概述一二。 注册mongolab.com/signup 创建数据库注册成功后，此时进入到自己的控制面板页，点击 Create new 创建一个数据库。在 Plan 选项选择 Single-node 的 Sandbox 免费套餐。在 Darebase name 除填写数据库的名字，点击 Create new MongoDB deployment 完成创建。 创建成功后跳转到了控制面板页，点击刚才创建的数据库进入该数据库配置页面。我们需要先为数据库创建一个拥有读写权限的用户，点击 Click here 添加一个用户。 打开 mongodb.js ，将： mongoose.connect(&apos;mongodb://localhost/drifter&apos;, {server: {poolSize: 10}}); 修改为： mongoose.connect(&apos;your_mongolab_url&apos;, {server: {poolSize: 10}}); 注意：将 &lt;dbuser&gt; 和 &lt;dbpassword&gt; 分别替换为刚才为数据库创建的用户名和密码。 然后，使用 Redis4You ：redis4you 是一家 Redis 云服务提供商，提供多个档次的 Redis 实例供选择，所有类型的服务包括免费的 5M 试用版，都提供诸如多 DB，持久化等功能。这里我们仅供测试使用。 注册redis4you.com/login_new 创建数据库验证邮箱完成注册。登录 Redis4You，点击右上角的 My Redis Instanses 进入 Redis 管理页面，点击 Provision new instance 创建一个新的数据库。点击 5MB 的免费套餐右边的 Select this instance 完成创建。 此时跳转到 Redis 管理页面，点击 Information 查看数据库详情。我们只会用到 Host、Port 和 Auth 这三项，其中 Auth 为随机生成的访问数据库的凭证。 注意：一定要点击左上角的 start 启动数据库。 打开 redis.js ，将： var client = redis.createClient(); 修改为： var client = redis.createClient(Port, Host, {auth_pass: Auth}); 注意：将 Host、Port 和 Auth 分别替换为给定的值。 最后，部署到 HeroKu ：Heroku 是一个主流的 PaaS 提供商，在开发人员中广受欢迎。这个服务围绕着基于 Git 的工作流设计，假如你熟悉 Git ，那部署就十分简单。这个服务原本是为托管 Ruby 应用程序而设计的，但 Heroku 之后加入了对 Node.js 、Clojure 、Scala 、Python 和 Java 等语言的支持。Heroku 的基础服务是免费的。 注册heroku.com 创建一个应用右上角 “＋” 中点击 Create a new app ，填写独一无二的应用名称后，点击 creat app 即创建成功，然后点击 Open app 即可打开应用 URL 链接 。 安装 Heroku ToolbeltHeroku 官方提供了 Heroku Toolbelt 工具更方便地部署和管理应用。它包含三个部分： Heroku client ：创建和管理 Heroku 应用的命令行工具 Foreman ：一个在本地运行你的 app 的不错的选择 Git ：分布式版本控制工具，用来把应用推送到 Heroku Heroku Toolbelt 下载地址：toolbelt.heroku.com 。 注意：假如你的电脑上已经安装了 Git ，那么在安装的时候选择 Custom Installation 并去掉安装 Git 的选项，否则选择 Full Installation 。 安装成功后，打开终端 shell ，输入 heroku login ，然后输入在 Heroku 注册的帐号和密码进行登录。Git 会检测是否有 SSH 密钥，如果有，则使用此密钥并上传，如果没有，则创建一个密钥并上传。 Tips：SSH 密钥通常用于授予用户访问服务器的权限。可将它们用于某些配置中，以便无需密码即可访问服务器。许多 PaaS 提供商都使用了此功能。 Procfile在工程的根目录下新建一个 Procfile 文件，添加如下内容： web: ./bin/www Procfile 文件告诉了服务器该使用什么命令启动一个 web 服务，这里我们通过 ./bin/www 执行 Node 脚本。为什么这里声明了一个 web 类型呢？官方解释为： The name “web” is important here. It declares that this process type will be attached to the HTTP routing stack of Heroku, and receive web traffic when deployed. 上传应用首先 clone 一份 git 仓库源码到你的本地机器中； $ heroku git:clone -a web_app_name $ cd web_app_name 使用 git 将源码变动的部分更新以及部署到 heroku 当中. $ git add . $ git commit -am &quot;make it better&quot; $ git push heroku master 在 push 到 heroku 服务器之前，我们还需要做一个工作。由于我国某些政策的原因，我们需到 ~/.ssh/ 目录下，新建一个 config 文件，内容如下： Host git.heroku.com User yourName Hostname 107.21.95.3 PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa port 22 然后回到 Git Bash ，输入： $ git push heroku master 稍等片刻即上传成功。 参考： nswbmw github"},{"title":"Redis学习笔记","permalink":"https://jiangink.github.io/2016/01/05/RedisNote/","text":"Redis是一个开源、支持网络、基于内存、键值对存储数据库，使用ANSI C编写。Redis是时下最流行的键值对存储数据库。 历史： 2015年6月至今：Redis 的开发由Redis Labs赞助。 2013年5月至2015年6月期间：其开发由Pivotal赞助。 2013年5月之前：其开发由VMware赞助。 如何安装 Redis ：在 Linux、Unix、Mac 中源码编译的安装方式： $ wget http://download.redis.io/releases/redis-3.0.6.tar.gz $ tar xzf redis-3.0.6.tar.gz $ cd redis-3.0.6 $ make 在 Mac 中可使用 Homebrew 管理工具进行安装： $ brew install redis 如何运行 Redis ：首先需要运行 redis 服务端： $ redis-server 然后切换到另一终端命令行，运行客户端可执行程序： $ redis-cli 注意，redis默认使用 6379 端口，如果想指定其它端口： $ redis-cli --port 1234 //指定端口为：1234 然后可输入 info 命令查看所有信息，确认 redis 是否已被正确安装； 同时，还可以使用 redis 提供的 ping 命令来测试客户端与 redis 服务是否连接正常： redis 127.0.0.1:6379&gt; PING PONG 关于 Redis 的数据类型：redis 存在五种数据类型，包括 string（字符串类型），hash（哈希类型），list（链表类型），set（无序集合类型）及 zset（有序集合类型）； 字符串类型 [string] 是 Redis 中最基本的数据类型，也是其他 4 种数据类型的基础，它能存储任何形式的字符串，包括二进制数据。 哈希类型 [hash] 是一个 string 类型的字段和字段值的映射表。哈希类型适合存储对象。相较于将对象的每个字段存成单个 string 类型，将一个对象存储在 hash 类型中会占用更少的内存，并且可以更方便的存取整个对象。 链表类型 [list] 可以存储一个有序的字符串列表，内部是使用双向链表实现的。所以我们通常用链表类型的 LPUSH 和 LPOP 或者 RPUSH 和 RPOP 实现栈的功能，用 LPUSH 和 RPOP 或者 RPUSH 和 LPOP 实现队列的功能。 集合类型 [set] 和数学中的集合概念相似，比如集合中的元素是唯一的、无序的，集合与集合之间可以进行交并差等操作。 有序集合类型 [zset] 与集合类型一样，只不过多了个 “有序” ，有序集合中每一个元素都关联了一个分数，虽然集合中每个元素都是不同的，但是它们的分数却可以相同。 关于 Redis 的基本使用：string［字符串类型］：字符串类型的操作中，SET、GET 以及 INCR 是三个常用的命令。 用法： SET key value : 设置 key 的值为 value，成功时返回 OK。 GET key : 获取 key 的值，成功时返回 key 的值。 INCR key : 给 key 的值加 1，成功时返回更新后的 key 值。 KEYS pattern : 查找当前所有符合给定模式 pattern 的 key（该命令适用于 redis 五种数据类型） hash［哈希类型］：哈希类型适合于存储对象。 用法： HSET key field value : 设置 key 的 field 字段值为 value，成功时返回 1，如果 key 中 field 字段已经存在且旧值已被新值覆盖，则返回 0 。 HGET key field : 获取 key 的 field 字段的值，成功时返回 field 字段的值。 HGETALL key : 获取 key 中所有的字段和其值，若 key 不存在，返回空。 list［链表类型］：链表类型适合存储如社交网站的新鲜事，假如有一个存储了几千万个新鲜事的链表，获取头部或尾部的 10 条记录也是极快的。以下是链表类型常用的几个命令： LPUSH key value [value …] : 往 key 链表左边添加元素，返回链表的长度。 RPUSH key value [value …] : 往 key 链表右边添加元素，返回链表的长度。 LPOP key : 移除 key 链表左边第一个元素，并返回被移除元素的值。 RPOP key : 移除 key 链表右边第一个元素，并返回被移除元素的值。 LRANGE key start stop : 获取链表中某一片段，但不会修改原链表的值。另外，与很多语言中截取数组片段的 slice 方法不同的是 LRANGE 返回的值包含最右边的元素。LRANGE 命令也支持负索引，即 -1 表示最后一个元素，所以 LRANGE key 0 -1 可以获取链表 key 中所有的元素。如果 start 的位置比 stop 的位置靠后，则返回空，如果 stop 大于实际链表范围，则会返回到链表最后的元素。 set［集合类型］：集合类型非常适合存储如文章的标签，这样我们在获取标签的时候就可以避免获取重复的标签了。以下是集合类型常用的几个命令： SADD key member [member …] : 向集合中添加一个或多个元素，若要添加的元素集合中已经存在，则忽略这个元素。返回成功加入的元素数量（不包含忽略的）。 SREM key member [member …] : 从集合中删除一个或多个元素，返回成功移除的元素的数量。 SMEMBERS key : 返回集合中所有元素。 集合类型还支持交集、差集和并集的运算，命令如下： SINTER key [key …] : 多个集合执行交集运算，并返回运算结果。 SDIFF key [key …] : 多个集合执行差集运算，并返回运算结果。 SUNION key [key …] : 多个集合执行并集运算，并返回运算结果。 zset［有序集合类型］：有序集合类型中每一个元素都关联了一个分数。这使得我们不仅可以进行大部分集合类型的操作，还可以通过分数获取指定分数范围内的元素等与分数有关的操作。有序集合类型适用于比如通过文章访问量排序的功能。常用命令如下： ZADD key score member [score member …] ： 向有序集合中加入一个或多个元素和该元素的值，如果该元素已存在则用新的分数替换原来的分数。返回新加入的元素个数（不包含已存在的元素）。 ZREM key member [member …] ： 删除集合中一个或多个元素，返回成功删除的元素的个数（不包含本来就不存在的元素）。 ZRANGE key start stop [WITHSCORES] ： 按元素分数从小到大顺序返回从 start 到 stop 之间所有的元素（同样包含两端的元素）。如果需要同时获得对应元素的分数的话，在尾部加上 WITHSCORES 参数。如果两个元素分数相同，则按照元素的字典序排列。 ZREVRANGE key start stop [WITHSCORES] ： 同上，只不过 ZREVRANGE 是按照元素分数从大到小顺序给出结果的。 参考： ZhiCun Github"},{"title":"关于Node版本切换","permalink":"https://jiangink.github.io/2015/12/22/ChangeNodeVersion/","text":"今天在安装yeoman时，警告提示node版本过低，那么好吧，咱们就来更新node版本。顺带把整个过程梳理一遍，做一个有节操的程序猿。 提到node的安装，就不得不说这个 NVM （ Node Version Manager ）Node版本管理器，一般情况下，不建议直接下载node安装程序进行安装，为了便于管理更新频繁的node，比较妥当的做法是通过nvm工具进行安装，而nvm工具本身的安装就不多加细究，在Mac上的方式是通过homebrew管理工具进行安装的，其他平台可Google寻找答案。 NVM的安装及简单使用：Mac上安装nvm的命令：1$ brew install nvm 我们可以查看nvm是否正常安装：1$ nvm —version 如果安装正常，我们可以通过如下命令，查看当前哪些版本可供安装：1$ nvm ls-remote 然后可以通过下面命令安装指定版本的node：1$ nvm install &lt;version&gt; 同样我们可以卸载指定版本的node：1$ nvm uninstall &lt;version&gt; 查看当前系统中可管理的所有版本node信息：1$ nvm ls Node安装及版本切换：我这里安装了v0.12.9版本：123$ nvm install v0.12.9#################################################### 100.0%Now using node v0.12.9 当前我使用v0.12.9版本即可，但如果此时我需要切换至其它版本，就可以使用如下命令：1$ nvm use &lt;version&gt; 但是使用 nvm use 命令有一个问题，即只对当前shell窗口生效，如果我们此时关闭该shell窗口，重新打开命令行就会发现node的版本又恢复到之前的版本状态；所以针对这种状况，我们需要设置一个默认node版本，命令如下：1$ nvm alias default &lt;version&gt; 此时打开shell窗口，就会使用默认版本的node执行； 噢，对了，更新了node版本过后，别忘记更新npm：1npm install -g npm"},{"title":"Node调试工具的使用","permalink":"https://jiangink.github.io/2015/05/11/NodeInspector/","text":"今天发现一个在浏览器上调试node代码的工具，分享一下.这个工具名叫：Node Inspector感兴趣的话可以点链接到Github看看详细文档说明，这里就简要介绍一二； 安装步骤安装命令：1npm install –g node-inspector 运行步骤1）首先，需要启动node.js应用命令如下：1node --debug-brk=[app port] [app name] 或者1node --debug=[app port] [app name] 例如：1node --debug-brk=3001 bin/www [debug与debug-brk两种配参见说明] 2）然后，启动node-inspector调试工具：1node-inspector --web-port [web port] --debug-port [app port] 例如：1node-inspector --web-port 8081 --debug-port 3001 [注：8081为web访问端口；3001为应用调试端口] 3)最后，访问chrome浏览器：具体访问的网址，其实在启动调试工具后会有输出提示，大致如下：1http://127.0.0.1:8081/debug?ws=127.0.0.1:8081&amp;port=3001 具体也可参考如下： 百度经验 用node-inspector调试Node.js"},{"title":"MEAN学习笔记","permalink":"https://jiangink.github.io/2015/04/08/MEANNote/","text":"(M)ongoDB——NoSQL的文档数据库，使用JSON风格来存储数据，甚至也是使用JS来进行sql查询；(E)xpress——基于Node的Web开发框架；(A)agular——JS的前端开发框架，提供了声明式的双向数据绑定；(N)ode——基于V8的运行时环境（JS语言开发），可以构建快速响应、可扩展的网络应用。 Express安装：1、如果需要安装express 3.x版本，可直接在待安装的包名后添加@版本数，命令行如下：1$ npm install -g express-generator@3 2、Express 4.x版本后，将express命令行工具分离了出来，我们使用全局方式安装时，其命令行如下：1$ npm install -g express-generator 3、在安装结束，我们想要创建一个应用app并进入工程目录：1$ express -e app &amp;&amp; cd app 4、初始化依赖环境：1$ npm install 5、运行这个应用：1$ npm start MongoDB安装：这里没有用源码进行安装，用的是HomeBrew，它是一个缺省包管理器，类似于apt-get、yum这样的工具。 1、我们首先需要更新一下homebrew的包数据库，这个过程可能需要几分钟，终端命令如下：1$ brew update 2、可以安装MongoDB了，命令行如下：1$ brew install mongodb 3、安装成功后，终端会提示如何启动，大概命令如下：1$ mongod --config /usr/local/etc/mongod.conf 4、若已启动，可以再启一个终端，命令行中键入“mongo”即可直接连接到MongoDB； 问题汇总：新工程创建标准：创建一个标准的package.json文件：1$ npm init 该命令将以文本互动的方式生成一份简易的 package.json 文件； 工程内模块的依赖生成；1$ npm install modules_name --save 通过 –save 参数，就能够在你安装模块的同时，自动将模块的依赖写入 package.json。安装完成后，查看 package.json，会多了一项 dependencies 字段，该字段内将罗列具体的依赖模块； package.json中的dependencies字段与devDependencies字段：node package存在两种依赖项，一种是dependencies，而另一种是devDependencies，其中前者的依赖项是正常运行该包时所需要的依赖项，而后者则是进行开发时所需要的依赖项，如一些单元测试的模块包。12\"dependencies\": &#123;&#125; //生产环境\"devDependencies\": &#123;&#125; //开发环境 在package.json所在目录执行npm install的时候，devDependencies里的模块同样会被安装。如果我们只想安装dependencies里面的包，可以执行1npm install –production 如果只安装devDependencies，可以执行1npm install –dev 同理自动更新dependencies字段值的依赖模块：1npm install node_module –save 自动更新devDependencies字段值的依赖模块：1npm install node_module –save-dev。 Most middleware (like session) is no longer bundled with Express and must be installed ……解决方法：大多数的中间件如session，在Express4.x版本之后不会再随着Express包本身一并被安装，而是被分离出来；原本3.x版本的写法，在这里将不再适用：123456789var express = require('express');var MongoStore = require('connect-mongo')(express);app.use(express.session(&#123; secret: settings.cookie_secret, store: new MongoStore(&#123; db: settings.db &#125;)&#125;)); 正确的做法应该首先安装中间件express-session，在package.json文件中添加中间件包名及版本，然后，命令行通过 npm install 来安装；最后修改代码：123456789var session = require('express-session');var MongoStore = require('connect-mongo')(session);app.use(session(&#123; secret: settings.cookie_secret, store: new MongoStore(&#123; db: settings.db &#125;)&#125;)); Redis的安装：到官网下载最新源码，或者通过命令下载：1$ wget http://download.redis.io/releases/redis-3.0.1.tar.gz 解压源码包并编译安装：123$ tar xzf redis-3.0.1.tar.gz$ cd redis-3.0.1$ make 这里比较有意思的是，redis的源码编译与安装只需“make”即可，不像之前编译其他源码要再进行一步“make install”才能安装；安装好后将会在/usr/local/bin/路径下看得到几个命令执行程序：123456redis-benchmark redis-cliredis-check-aof redis-sentinelredis-check-dump redis-server"},{"title":"不要活在假相之中","permalink":"https://jiangink.github.io/2015/04/06/DontLiveInIllusion/","text":"The Walking Dead 「season 5」I didn’t bring it in. It got inside on its own.They always will…They dead and the living, Because we’re in here.And the ones out there…They’ll hunt us. They’ll find us. They’ll try to use us. They’ll try to kill us.But we’ll kill them. We’ll survive. I’ll show you how.You know, I was thinking…I was thinking how many of you do I have to kill. to save your lives?But I’m not gonna do that. You’re gonna change. 那個你所不知的真相，或者那個你不願接受的真相，也許並沒有想像那樣糟。 真正的糟糕，是你誤陷於假相，卻認為那才是生活的原貌。 真正的糟糕，是你的固執己見，追求著這時空本不存在的絕對。 真正的糟糕，是你已被裹挾，再也沒有勇氣去面對這個亦真亦幻、或好或壞、非敵非友的真實世界。 我們的生活，我們所經歷的，每天都在變動著。 如果你還沒有做出絲毫改變，卻認為一切終會好轉。 那麼假相，它會尾隨你、找到你，然後試著利用你，最終殺掉你。 不要認為你的世界已築起高牆，不會有任何異端可以進入。 要知道它們終將會進來，因為這個世界你在其中。 你，必須做出改變！ 那些刺痛的真相，並不是要阻擋你繼續前行， 而是要告訴你，前行的道路上需要更大的決心。"},{"title":"Mac共享目录挂载","permalink":"https://jiangink.github.io/2015/04/05/MountMacSharedDir/","text":"最近，这个共享目录挂载的问题一直困扰着我，妈蛋！我的目标很简单，就是需要将一个Mac共享目录mount到Linux中去，但为什么这般折磨……「上帝啊！請寬恕這個狂躁的機智少年！」 共享目录：1、 Mac系统“系统偏好设置”中，有个“共享”功能；2、进入“共享”后，勾选“文件共享”服务，然后在右侧，我们可以添加需要共享的文件夹以及相关用户访问权限；3、如果你需要“SMB”和“AFP”，那就点击“选项…”，添加该服务以及账户； 创建用户：1、Mac系统“系统偏好设置”中，有个“用户与群组”功能，添加一个共享用户；2、在添加前，需要解锁才能操作，解锁后，点击“＋”号，新账户选择“仅限共享”，其他随意，然后“创建用户”； 其余操作：1、需要讲你新创建的“仅限共享”的用户，添加到共享文件夹的用户中去，设置“读与写”权限，这样，该用户才能够访问该文件夹；2、在Linux终端中，首先需要安装cifs： ``` bash yum install cifs-utils ``` 3、Linux创建挂载点： ``` bash sudo mkdir -p /mnt/MountPointName ``` 4、开始挂载： ``` bash mount -t cifs //MacHostOrMacIP/SharedDirectory /mnt/MountPointName/ -o username=UserName,password=Password,nounix,sec=ntlmssp ``` 但是，最终当我执行的时候，它不好使了～``` bash # mount -t cifs //192.168.1.101/Github /mnt/jiangink/ -o username=jiangink,password=123456,nounix,sec=ntlmssp mount error(13): Permission denied Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) ```"},{"title":"二零一五書單","permalink":"https://jiangink.github.io/2015/03/26/Booklist2015/","text":"Do not go gentle into that good night.不要溫和地踏入那個良夜。Rage, rage against the dying of the light.怒斥、怒斥將逝的時光。 專業相關： 《Node.JS開發指南》 BYVoid 「2015年03月30日 已讀完」 《Javascript語言精萃》 Douglas Crockford 「2015年04月25日 已讀完」 《Node.JS實戰》 CNode社區 《Essential C++》 Stanley B. Lippman 《Effective C++》 Scott Meyers 興趣相關： 《時間的形狀》 汪潔 《量子物理史話》 曹天元 沉積書角： 《生活在別處》 Milan Kundera 《Python基礎教程》 Magnus Lie Hetland 《Head First HTML&amp;CSS》 Elisabeth Robson/Eric Freeman 2015-03-26 執行情況：《Node.JS開發指南》 已讀95頁 第五章 Web開發《時間的形狀》 已讀135頁 第四章 狹義相對論 光速極限 2015-03-30 執行情況：《Node.JS開發指南》 已讀完《時間的形狀》 已讀163頁 第五章 廣義相對論的宇宙 等效原理 2015-04-25 執行情況：《Javascript語言精萃》 已讀完 2015-05-03 執行情況：《時間的形狀》 已讀182頁 第五章 廣義相對論的宇宙 時空彎曲《Node.JS實戰》 第一章 1.4"},{"title":"有关域名解析","permalink":"https://jiangink.github.io/2015/03/18/ResolveDomain/","text":"这里纪录一下服务端环境，单个域名在对应的多条IP信息情况下，配置与解析处理。 关于hosts允许主机对应多条IP的参数配置 我们知道 gethostbyname() 函数会使用到 hosts 文件进行域名解析，而在解析过程中将按照顺序读取文件内容； /etc/hosts 文件内容如下所示： 1234192.168.1.1 www.dns.com192.168.1.2 www.dns.com192.168.1.3 www.dns.com192.168.1.4 www.dns.com 一般情况下，若未做任何host参数配置的话，仅能够解析到第一条内容(即：192.168.1.1)，但如果在 /etc/host.conf 配置文件中添加 multi on (允许主机存在多条IP)，则能够解析到关于该域名的多条IP信息； 主机信息及IP地址列表 hostent(host entry)，该结构体记录主机的信息，包括主机名、别名、地址类型、地址长度和地址列表。之所以主机的地址是一个列表的形式，原因是当一个主机对应多个网络接口时，就存在多个地址。 123456789 struct hostent &#123; char* h_name; /* official name of host */ char* h_aliases; /* alias list */ short h_addrtype; /* host address type */ short h_length; /* length of address */ char** h_addr_list; /* list of addresses */#define h_addr h_addr_list[0] /* address, for backward compat */ &#125;; h_addr_list 为指针数组，数组中每个元素都是 in_addr 型指针。 具体域名IP解析的代码如下： void resolve_domain_name(const unsigned char* host_name) { struct hostent *host_info = NULL; char target_ip[32]; char **pptr; char *address = NULL; memset(target_ip, 0, sizeof(target_ip)); //获取主机信息 host_info = gethostbyname(host_name); if (NULL == host_info) { printf(\"DNS resolve error![%s]\\n\", host_name); return; } //获取IP地址列表 pptr = host_info-&gt;h_addr_list; while(NULL != *pptr) { //[二进制整数] 转换为 [点分十进制] address = inet_ntoa(*(struct in_addr *)*pptr); //address = inet_ntop(host_info-&gt;h_addrtype, *pptr, target_ip, sizeof(target_ip)); if (NULL == address) { printf(\"Addr convert error!\\n\"); return; } printf(\"IP Address = [%s]\\n\", address); pptr ++; } } 嗯，到这里，另外需要补充的是二进制整数转点分十进制时，“inet_ntoa函数陷阱”的问题，inet_ntoa() 函数返回值为一个指向字符的指针，且是一个由inet_ntoa()函数控制的静态固定指针，在每次调用 inet_ntoa() 后，它将会覆盖上次调用时所得的IP地址。所以如果上一次返回的结果仍然需要，则最好定义一个缓存来做保留。"},{"title":"SSL编程小札","permalink":"https://jiangink.github.io/2015/03/09/CodingWithSSL/","text":"SSL简介： 安全套接层(Secure Socket Layer，SSL)是一种在两台机器之间提供安全通道的协议。它具有保护传输数据以及识别通信机器的功能。安全通道是透明的，意思就是说它对传输的数据不加变更。客户端与服务器之间的数据是经过加密的，一端写入的数据完全是另一端读取的内容。透明性使得几乎所有基于TCP的协议稍加改动就可以在SSL上运行，非常方便。 SSL最初的版本是由网景(Netscape)设计的，但1996年由于市场份额下滑等因素，网景决定将SSL移交给因特网工程任务组（IETF）进行标准化制定，随后便产生了TLS，也被称作SSL3.1版本； SSL/TLS的特性确保了通信双方可以进行端点认证(ndpoint authentication)互验对方身份，并保证了数据的消息完整性(message integrity)以及私密性(confidentiality)。 SSL编程套路： SSL通信模型采用的是标准C/S结构，因此基于OpenSSL的程序就被分成两个部分：Client和Server。且程序遵循着以下几个重要步骤： OpenSSL初始化在使用OpenSSL之前，必须进行相应的初始化工作。完成初始化功能的函数原型如下：123int SSL_library_int(void); // 初始化SSL算法库函数，调用SSL函数之前必须调用此函数void OpenSSL_add_all_algorithms(void); //加载SSL所有算法void SSL_load_error_strings(void); //错误信息的初始化 在建立SSL连接之前，要为Client和Server分别指定本次连接采用的协议及其版本，目前能够使用的协议版本包括SSLv2、SSLv3、SSLv2/v3和TLSv1.0。SSL连接若要正常建立，则要求Client和Server必须使用相互兼容的协议。 创建CTX在OpenSSL中，CTX是指SSL会话环境。建立连接时使用不同的协议（ 详见 ），其CTX也不一样。创建CTX的过程中会依次用到以下OpenSSL函数：1234567891011//客户端、服务端都需要调用的SSL_CTX_new() //申请SSL会话环境//若有验证对方证书的需求，则需调用SSL_CTX_set_verify() //指定证书验证方式SSL_CTX_load_verify_location() //为SSL会话环境加载本应用所信任的CA证书列表//若有加载证书的需求，则需调用SSL_CTX_use_certificate_file() //为SSL会话加载本应用的证书SSL_CTX_use_certificate_chain_file() //为SSL会话加载本应用的证书所属的证书链SSL_CTX_set_default_passwd_cb_userdata() //设置私钥的密码(如果有的话);注：需要在加载私钥之前设置SSL_CTX_use_PrivateKey_file() //为SSL会话加载本应用的私钥SSL_CTX_check_private_key() //验证所加载的私钥和证书是否相匹配 对于SSL证书的双向认证，这里需要补充的是，可能需要client以及server配置如下的接口函数：12345678/* 设置验证证书链的最大长度 */void SSL_CTX_set_verify_depth(SSL_CTX *ctx,int depth);/* SSL_CTX_set_verify的回调函数，需要自己去写实现 */int verify_callback(int preverify_ok, X509_STORE_CTX *x509_ctx);/* 设置认证的模式及是否开启认证回调 */SSL_CTX_set_verify(SSL_CTX* ctx,int mode,int (*verify_callback)(int,X509_STORE_CTX*));/* 加载CA证书文件 */SSL_CTX_load_verify_locations(SSL_CTX* ctx,const char* CAfile,const char* CApath); 另外还需要注意认证的模式（ 详见 ），常用的模式如下： SSL_VERIFY_NONE: 不会进行证书验证 SSL_VERIFY_PEER: 要求对方提供证书进行验证的模式，该模式在服务端类似于一种自愿验证的模式，当遇到客户端不提供证书时，是否继续通信可自行决定；但该模式运用于客户端，若服务端无证书则会导致SSL握手失败。 SSL_VERIFY_FAIL_IF_NO_PEER_CERT: 服务端使用的一种的模式，此模式下，相当于强制验证客户端的证书，若客户端无证书则SSL握手失败。 建立SSL套接字在此之前要先创建普通的流套接字，完成TCP三次握手，建立普通的TCP连接。然后创建SSL套接字，并将其与流套接字绑定。这一过程中会使用以下几个函数：1234SSL *SSl_new(SSL_CTX *ctx); //申请一个SSL套接字int SSL_set_fd(SSL *ssl,int fd); //绑定读写套接字int SSL_set_rfd(SSL *ssl,int fd); //绑定只读套接字int SSL_set_wfd(SSL *ssl,int fd); //绑定只写套接字 完成SSL握手在这一步，我们需要在普通TCP连接的基础上，建立SSL连接。与普通流套接字建立连接的过程类似：Client使用函数SSL_connect()发起握手（ 类似于流套接字中用的connect() ），而Server使用函数SSL_accept()对握手进行响应（ 类似于流套接字中用的accept() ），从而完成握手过程。两函数原型如下：12int SSL_connect(SSL *ssl);int SSL_accept(SSL *ssl); 鉴别证书信息握手过程完成以后，Client以及Server通常能够获取到对方的证书信息，并对信息进行鉴别。具体实现中会用到以下函数:123X509 *SSL_get_peer_certificate(SSL *ssl); //从SSL套接字中获取对方的证书信息X509_NAME *X509_get_subject_name(X509 *a); //获取证书所有者名字X509_NAME *X509_get_issuer_name(X509 *a); //获取证书颁发者名字 进行数据传输经过前面的一系列过程后，就可以进行安全的数据传输了。在数据传输阶段，需要使用SSL_read( )和SSL_write( )来代替普通流套接字所使用的read( )和write( )函数，以此完成对SSL套接字的读写操作,两个新函数的原型分别如下：12int SSL_read(SSL *ssl,void *buf,int num); //从SSL套接字读取数据int SSL_write(SSL *ssl,const void *buf,int num); //向SSL套接字写入数据 会话结束当Client和Server之间的通信过程完成后，就使用以下函数来释放前面过程中申请的SSL资源：123int SSL_shutdown(SSL *ssl); //关闭SSL套接字void SSl_free(SSL *ssl); //释放SSL套接字void SSL_CTX_free(SSL_CTX *ctx); //释放SSL会话环境 具体源码示范： SSLSocketDemo 本文参考如下资料整理： 1. 《SSL与TLS》 2. 《使用OpenSSL API 建立SSL安全通信的一般流程》"},{"title":"遺忘的事","permalink":"https://jiangink.github.io/2015/02/04/ForgetThing/","text":"遺忘，是最好的事，有時，也卻是最壞的事……回到成都兩個月有餘，夜裏時常會醒來，每當望向窗外，看著遠處華陽河畔寂靜的街景，心中就會莫名的忐忑，略感缺失。或許是因為離開了北京，沒有了熟悉車流聲和地鐵的震動，讓我覺得反倒有些不適，又或許，是因為別的什麼⋯⋯這種感受若有若無反覆著，無法平息，無法清晰描述。就是在這樣的夜，在無法入眠時，我會常常想起過去的種種，關於已屬於曾經的，那些我以為遺忘的事。 選擇 想到三年之前，決定去北京，那個時候，應算第一次為了自己的想法去做的重要選擇，回顧這三年間的成長，苦悶不堪在所難免，但一路行走堅持到了現在，也有所獲得。時間開始漸漸讓我相信，專注於自己的想法不言放棄，定能獲得存在的真實價值。選擇，得失的權衡，不可避免的失去，有時是無可奈何，而有時是隱忍地堅持，即便會不捨會疼痛，但最終還是做了決斷。一次又一次，你說已找不到原來的樣子，他們卻說這才是生活的原樣。是否仍有意義去質問誰說了謊，可是選擇又從來都沒有對錯⋯⋯ 遠方 路經西安，來到北京，然後輾轉成都，穿梭於一座座城池，活著卻仍然像個孩子。迷惘了問，下一站會是哪兒？徬徨無果後卻隱約聽到『亡命之徒』，李宗盛在耳邊喃喃說詞：“對此我並無更高明的解釋”。說道遠方，是歲月靜好，現世安穩，是晨鐘暮鼓，安之若素⋯⋯但想想或許，又並不只是這些。為何，要如一葉浮萍隨處游離，大概是因為，無法接受妥協於平庸的現在。"},{"title":"離開，為了更好的前行","permalink":"https://jiangink.github.io/2015/01/27/MoveForward/","text":"在不經意的某個時刻，我所追逐的，或許已不再是你，而是我以為的你，再見，北京。 甲：“我說，還記得我們剛來這裏的時候嗎？天氣像今天一樣好。” 乙：“對啊，可是今天，卻是要離開。” 甲：“來的時候就會知道有離開的一天，這個心理準備我以為我們都有。” 乙：“你好像看的很透，但我覺得，你不會像你說的那樣捨得。” 甲：“這幾年間的記憶都在這裏了，而這裏，將要成為你我的舊時光，說捨得，怎麼可能。” 乙：“我這段時間有時在想，這裏的舊時光會不會只是場夢，只是夢醒了，你我也該走了。” 甲：“如果是場夢，那我倒覺得連夢本身都不屬於我，長達幾年的夢，最終醒來，不多不少，我還是我。” 乙：“那你覺得，這場夢是為了什麼？” 甲：“拜託，不要什麼事都要問為什麼，那你告訴我人的一生是為了什麼？” 沈默⋯⋯ 甲：“我不知道為了什麼，我也不記得來到這裏是為了什麼？” 乙：“我還記得，但我已不想在這裏提及，再也沒有這個必要了⋯⋯” 甲：“是因為要離開嗎？” 乙：“離開只是一個結果。” 甲：“那麼原因呢？” 乙：“因為我所追逐的，已不再是我以為的樣子。”"},{"title":"Hello, World!","permalink":"https://jiangink.github.io/2015/01/23/HelloWorld/","text":"歡迎來到我的博客，這是我的第一篇Hexo日誌，請多多指教！ 作為開篇日誌，依照慣例，不出所料即定為 Hello World 。正所謂一千隻程序猿，就會有一千種 Hello World ，而在我理解，「Hello World」它代表的是一種探索趣味世界的精神；所以敲出這幾個字母，即是希望這種精神能夠成為我接下來每一天的生活基調，將每一天都過的更有意義，也更有趣味。開設博客，就是這樣一個簡單的初衷，同時也是希望能夠結交更多志同道合的朋友。 進入編程這個行當也有些年月，或許你不會相信，只是在最近的這段時間，我才稍稍發覺，寫程序這個苦逼行當也存在著趣味可言。那麼過去一直以來，為什麼我就沒有過現在這麼高的覺悟呢？ 細細回想，覺得極有可能是被思維模式給“搞拐”了，過去的多數時間都耗費在碎小雜亂的結果上，為了短期功利的結果而去妥協做一件事，甚至於應付一件事，這樣的狀態是最糟糕的，這種背離初衷的做法最終也只會潛藏隱病，慶幸年末果斷的選擇，離開了這種畸形的思維模式，從而避免了捲入漩渦，寫這些只是希望今後自己能夠引以為戒，也希望給小夥伴們一個做事的考量。 好了，我們的主題仍然是關於生活、關於探索趣味世界，我想接下來，會有更多的經歷值得分享，所以我要記錄下去，拭目以待吧！ 「注」：“Hello, World!”最早出現於1972年由貝爾實驗室成員Brian Kernighan撰寫的內部技術資料《Introduction to the Language B》中。對於程序猿而言，它意味著美好的開始。"}]}