{"pages":[{"title":"關於","permalink":"https://erikjiang.github.io/about/index.html","text":"個人信息： 網絡暱稱：Erik Jiang . 目前職業：服務端程序猿 . 技能標籤：Node.JS、 Go、 MySQL、 MongoDB、 Redis、 Linux … 工作經歷： 2016.02～至今成都某醫藥行業互聯網公司從事於服務器後台醫藥業務功能開發 2012.05～2015.12北京某網絡信息安全公司從事於內網安全接入網關服務器後台研發 聯繫方式：&#106;&#105;&#x61;&#x6e;&#x67;&#105;&#x6e;&#107;&#64;&#102;&#x6f;&#x78;&#x6d;&#97;&#105;&#108;&#46;&#x63;&#111;&#109;"},{"title":"categories","permalink":"https://erikjiang.github.io/categories/index.html","text":""},{"title":"tags","permalink":"https://erikjiang.github.io/tags/index.html","text":""}],"posts":[{"title":"LVM 逻辑卷管理器的使用","permalink":"https://erikjiang.github.io/2020/05/15/LVMTutorials/","text":"逻辑卷管理器 LVM (Logical Volume Manager), 又称为逻辑卷宗管理器、逻辑磁盘管理器，是Linux核心所提供的逻辑卷管理功能。它在硬盘分区之上，又创建一个逻辑层，以便系统管理硬盘分割系统。 最早由IBM开发，在AIX系统上实现，OS/2 操作系统与 HP-UX也支持这个功能。在1998年，Heinz Mauelshagen 根据在 HP-UX 上的逻辑卷管理器，编写出第一个 Linux 版本的逻辑卷管理器。 LVM 的基本概念术语： PV(Physical volume): 物理卷, PV处于LVM系统最低层, 它可以是整个硬盘, 或者与磁盘分区具有相同功能的设备(如RAID) VG(Volume Group): 卷组, 创建在PV之上, 由一个或多个PV组成,可以在VG上创建一个或多个“LVM分区”(逻辑卷),功能类似非LVM系统的物理硬盘 LV(Logical Volume): 逻辑卷, 从VG中分割出的一块空间, 创建之后其大小可以伸缩，在LV上可以创建文件系统(如/var,/home) PE(Physical extent): 物理块, 每一个PV被划分为基本单元(也被称为PE), 具有唯一编号的PE是可以被LVM寻址的最小存储单元, 默认为4MB 1234567891011121314151617181920212223 Disk A Disk B+-------------------------------------------------------------+ +---------------------------------------+| | | || | | || PV1 PV2 PV3 | | PV1 PV2 |+------------------+ +------------------+ +------------------+ +------------------+ +------------------+|------------------| |------------------| |------------------| |------------------| |------------------||| PE || PE || PE || || PE || PE || PE || || PE || PE || PE || || PE || PE || PE || || PE || PE || PE |||------------------| |------------------| |------------------| |------------------| |------------------|+-------------------------------------------------------------+ +---------------------------------------+ | | | | | | | | +-----+ | | | | | | | +-----------+ | | +-----v-----+ | +--&gt;+ VG1 +&lt;-----+ +----------------------&gt;+ VG2 +&lt;-------------------+ +-----+-----+ +-----+-----+ | | | +-------------+------------+ | | | +----v-----+ +----v-----+ +----v-----+ | LV1 | | LV2 | | LV3 | | (/home) | | (/usr) | | (/data) | +----------+ +----------+ +----------+ 安装 LVM 工具CentOS 中通过 yum 包管理工具安装:1yum install lvm2* -y 物理磁盘分区首先对新挂载磁盘进行分区, 这里将整个磁盘作为一个分区配置:123456789101112131415161718192021222324252627282930[root@node01 ~]# fdisk /dev/vdd # 对 /dev/vdd 进行分区Welcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0xfb5eb0bf.Command (m for help): n # 新建分区Partition type: p primary (0 primary, 0 extended, 4 free) e extendedSelect (default p): p # 主分区Partition number (1-4, default 1): 1 # 分区号First sector (2048-209715199, default 2048): # 默认: 首扇区号Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-209715199, default 209715199): # 默认: 尾扇区号Using default value 209715199Partition 1 of type Linux and of size 100 GiB is set# 下面分区类型设置很重要!Command (m for help): t # 改变类型(分区系统ID)Selected partition 1Hex code (type L to list all codes): 8e # 8e 表示 LVM 的分区编号Changed type of partition 'Linux' to 'Linux LVM'Command (m for help): w # 保存配置The partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks. 注意, 如果是要对一个磁盘做多个分区, 则需要根据分区规划的大小, 多次对Partition number, First sector, Last sector进行调整划分; 磁盘分区设置完成, 查看分区表, 会有看到如下Linux LVM 分区信息:123# fdisk -l Device Boot Start End Blocks Id System/dev/vdd1 2048 209715199 104856576 8e Linux LVM 准备物理卷(PV)依赖于磁盘分区/dev/vdd1 创建物理卷:12[root@node01 ~]# pvcreate /dev/vdd1 Physical volume \"/dev/vdd1\" successfully created. 展示已创建的物理卷:123456789101112[root@node01 ~]# pvdisplay \"/dev/vdd1\" is a new physical volume of \"&lt;100.00 GiB\" --- NEW Physical volume --- PV Name /dev/vdd1 VG Name PV Size &lt;100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID n0tSls-bw30-tNpv-ogjB-ILOW-vvxq-xgp2Oq 删除物理卷命令:1# pvremove $&#123;pv_name&#125; 准备卷组(VG)创建新卷组, 使用物理卷/dev/vdd1创建卷组volume-group1:12[root@node01 ~]# vgcreate volume-group1 /dev/vdd1 Volume group \"volume-group1\" successfully created 展示已创建的卷组:123456789101112131415161718192021[root@node01 ~]# vgdisplay --- Volume group --- VG Name volume-group1 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size &lt;100.00 GiB PE Size 4.00 MiB Total PE 25599 Alloc PE / Size 0 / 0 Free PE / Size 25599 / &lt;100.00 GiB VG UUID F3T1Ix-5t4S-tcfI-OZEE-wrAE-wyNc-qDIeNX 删除卷组命令:1# vgremove $&#123;vg_name&#125; 若卷组容量已满, 需要扩展卷组大小:1234567891011121314151617181920212223[root@node01 ~]# vgextend volume-group1 /dev/vdc1 Volume group \"volume-group1\" successfully extended[root@node01 ~]# vgdisplay --- Volume group --- VG Name volume-group1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 199.99 GiB PE Size 4.00 MiB Total PE 51198 Alloc PE / Size 25575 / 99.90 GiB Free PE / Size 25623 / &lt;100.09 GiB VG UUID F3T1Ix-5t4S-tcfI-OZEE-wrAE-wyNc-qDIeNX 准备逻辑卷(LV)使用卷组volume-group1来创建逻辑卷(名字:lv1, 大小:99.9G):123[root@node01 ~]# lvcreate -L 99.9G -n lv1 volume-group1 Rounding up size to full physical extent 99.90 GiB Logical volume \"lv1\" created. 展示已创建的逻辑卷:1234567891011121314151617[root@node01 ~]# lvdisplay --- Logical volume --- LV Path /dev/volume-group1/lv1 LV Name lv1 VG Name volume-group1 LV UUID iRlrj0-PvcQ-wKFk-IR1M-LysP-q2CK-Yy7xUi LV Write Access read/write LV Creation host, time node01, 2020-05-14 15:37:17 +0800 LV Status available # open 0 LV Size 99.90 GiB Current LE 25575 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 252:0 将创建好的逻辑卷格式化为 ext4 分区:12345678910111213141516171819202122[root@node01 ~]# mkfs.ext4 /dev/volume-group1/lv1mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks6553600 inodes, 26188800 blocks1309440 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2174746624800 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872Allocating group tables: doneWriting inode tables: doneCreating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done 注: mkfs.ext4 格式化逻辑卷为不可逆操作, 若逻辑卷中有重要数据, 则请备份后再格式化, 否则无法复原; 挂载逻辑卷, 比如挂载到/data:123456[root@node01 ~]# mount /dev/volume-group1/lv1 /data/# 查看挂载情况[root@node01 ~]# df -h -t ext4Filesystem Size Used Avail Use% Mounted on/dev/mapper/volume--group1-lv1 99G 61M 94G 1% /data 开机时自动挂载逻辑卷, 需要在 /etc/fstab 文件中配置:1234567891011## /etc/fstab# Created by anaconda on Mon Aug 13 11:29:03 2018## Accessible filesystems, by reference, are maintained under &apos;/dev/disk&apos;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=db6c825d-c763-42ae-8ccb-e8f5eaead61a / ext4 defaults 1 1LABEL=YUNIFYSWAP none swap sw 0 0/dev/volume-group1/lv2 /var/lib/docker ext4 defaults 1 1/dev/volume-group1/lv1 /data ext4 defaults 1 1 这里将逻辑卷lv1 与 lv2, 分别与 /data 和 /var/lib/docker 目录进行关联挂载; 检测 /etc/fstab 挂载配置是否可用:1# mount -a 注: 在重启系统前, 一定要做 mount -a 检测, 否则可能会有系统无法正常启动的风险!!! 关于磁盘数据的备份与还原 数据备份前, 先对 k8s 节点进行安全移除, 以便当前 pods 可以转移到其他节点主机: 1# kubectl drain &lt;node name&gt; --ignore-daemonsets 若涉及到 docker 数据备份, 先停用 docker 服务: 12# systemctl stop docker# systemctl stop kubelet 考虑到磁盘数据备份还原迁移耗时, 可使用 screen 建立会话, 防止数据迁移任务中断: 123456789# 备份 /data/ 数据$ screen -S backup-data$ cp /data/* /backup/ -avr$ &lt; ctrl + a + d &gt;# 还原 /data/ 数据$ screen -S restore-data$ cp /backup/* /data/ -avr$ &lt; ctrl + a + d &gt; 数据还原后, 启用 docker 服务: 12# systemctl start docker# systemctl start kubelet 还原集群节点 1# kubectl uncordon &lt;node name&gt; 若通过 uncordon 还原后, node 状态为 NotReady可查看 kubelet log:1# journalctl -u kubelet 若错误信息如下, 表明 swap 被开启, 影响到 kubelet 启动, 需要关闭:1failed to run Kubelet: Running with swap on is not supported, please disable swap! 关闭 swap 处理方法:1# swapoff -a LVM简明教程"},{"title":"govendor 升级小记","permalink":"https://erikjiang.github.io/2019/06/11/GoVendorUpgrade/","text":"老项目要升级 k8s 的 client-go 包，而这个项目由于历史原因， 使用的是 govendor 这样一个包管理工具，于是就捣鼓了一下 … 首先，我看了一下 vendor/vendor.json, 发现涉及到 client-go 的包有很多：1234567891011121314151617181920212223242526272829303132...&#123; \"checksumSHA1\": \"Ovxe5OrkHu7tTYVE/XbTD5+DvF4=\", \"path\": \"k8s.io/client-go/discovery\", ...&#125;,&#123; \"checksumSHA1\": \"mgVdewcg4jTq0piouybeQoXd/yQ=\", \"path\": \"k8s.io/client-go/kubernetes\", ...&#125;,&#123; \"checksumSHA1\": \"Jmg4wTN/9ztjnzQgADNhFALddv8=\", \"path\": \"k8s.io/client-go/kubernetes/scheme\", ...&#125;,&#123; \"checksumSHA1\": \"pmmsHaBIeDlD1LSw/KM2wVdYh2I=\", \"path\": \"k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1\", ...&#125;,&#123; \"checksumSHA1\": \"t7UdbYz8ir1FewYR53Izec0Sivw=\", \"path\": \"k8s.io/client-go/kubernetes/typed/apps/v1\", ...&#125;,&#123; \"checksumSHA1\": \"JU//A2Vhtt2lmEJvjPSzotPbax4=\", \"path\": \"k8s.io/client-go/kubernetes/typed/apps/v1beta1\", ...&#125;,... 如果我逐个对每个 package 进行 govendor update 也没什么问题， 但是效率会很低，且升级下来估计要吐血~ 于是，这里用到了通配符 /..., 将通配符追加到 k8s.io/client-go 包之后， 表示的是，将与 k8s.io/client-go 包相关的所有依赖一并操作处理； 这样就一劳永逸了。 如下具体操作，将 client-go 升级到 v11.0.0 版本：注：@= 表示指定固定的 tag 版本 1. 更新 k8s.io/client-go 相关包12$ govendor remove k8s.io/client-go/...$ govendor fetch -v k8s.io/client-go/...@=v11.0.0 2. 更新 k8s.io/apimachinery 相关包12$ govendor remove k8s.io/apimachinery/...$ govendor fetch -v k8s.io/apimachinery/...@=kubernetes-1.14.3 3. 更新 k8s.io/api 相关包12$ govendor remove k8s.io/api/...$ govendor fetch -v k8s.io/api/...@=kubernetes-1.14.3 当然，也可以尝试使用直接更新的方式来处理升级，比如：注：待验证 123$ govendor update -v k8s.io/client-go/...@=v11.0.0$ govendor update -v k8s.io/apimachinery/...@=kubernetes-1.14.3$ govendor update -v k8s.io/api/...@=kubernetes-1.14.3"},{"title":"kubectl config 管理多集群","permalink":"https://erikjiang.github.io/2019/05/02/KubeConfMultiCluster/","text":"我们日常开发中会不可避免的使用到多个 Kubernetes 集群，一般比较传统的做法，我们会使用 ssh 远程到集群节点主机上，然后再进行 kubectl 相关命令的操作，这样做也没什么问题，但是当集群数增多以后，就会发现切换集群的操作略显麻烦，且无法管理，于是就有了一个需求，怎样能够更加优雅的组织管理这些不同的集群接入场景？ 在了解 kubectl config 提供的功能时发现，它本身就能够清晰的组织管理不同集群的上下文，且能够无缝切换集群环境。 在这篇文章中，我将介绍如何使用 kubectl config 组织管理日常工作中的四个不同的集群环境。 我们先列出需要组织管理的集群列表： docker for desktop: 本地 Docker Desktop 提供的 k8s 单节点环境 work-test: k8s 多节点测试环境 work-dev: k8s 多节点开发环境 work-prod: k8s 多节点生产环境 首先，当你本地成功安装了 kubectl，会存在一个 ~/.kube/ 的目录，该目录用于存放所有集群的配置文件； 由于我本地安装了 Docker Desktop Kubernetes 环境，所以现在可以通过 vim ~/.kube/config 查看一下本地 k8s 单节点配置信息： 12345678910111213141516171819apiVersion: v1clusters:- cluster: insecure-skip-tls-verify: true server: https://localhost:6443 name: docker-for-desktop-clustercontexts:- context: cluster: docker-for-desktop-cluster user: docker-for-desktop name: docker-for-desktopcurrent-context: docker-for-desktopkind: Configpreferences: &#123;&#125;users:- name: docker-for-desktop user: client-certificate-data: ... client-key-data: ... 从该配置能够看出，一个完整的集群配置至少要包含三个要素： clusters 集群 contexts 上下文 users 用户 接下来，我们需要分别将 work-test、work-dev、work-prod 三个环境中的配置添加到本地； 通常 Kubernetes 集群在安装时会生成一个管理配置文件，该配置文件位于每个集群的主节点上，具体目录： 1/etc/kubernetes/admin.conf 了解到了这一点，我们就可以使用 scp 命令将各个集群主节点中的配置拷贝到本地 .kube 目录中了： 12345678# copy the Kubernetes admin config for work testscp root@SERV_MASTER_TEST:/etc/kubernetes/admin.conf ~/.kube/config-work-test# copy the Kubernetes admin config for work devscp root@SERV_MASTER_DEV:/etc/kubernetes/admin.conf ~/.kube/config-work-dev# copy the Kubernetes admin config for work prodscp root@SERV_MASTER_PROD:/etc/kubernetes/admin.conf ~/.kube/config-work-prod 将如上命令中 SERV_MASTER_TEST、SERV_MASTER_DEV、SERV_MASTER_PROD 分别替换为对应集群主节点 IP; 注：配置文件中包含敏感信息，请妥善保管！ 现在，我们已经完成了配置的拷贝，但是为了便于管理，我们需要对这些配置中的命名进行修改规范； 我们要修改这三个配置文件，并着重关注三大要素：集群、上下文、用户； 以 work-dev 为例： 1. 集群 clusters1234# 修改集群名称- cluster: server: https://xx.xx.xx.xx:6443 name: work-dev-cluster 2. 用户 users123# 修改用户名称users:- name: work-dev-admin 3. 上下文 contexts123456# 更新上下文名称，关联对应用户及集群contexts:- context: cluster: work-dev-cluster user: work-dev-admin name: work-dev 三个配置文件都修改完成后，我们需要将这些配置全部加载到 kube config 中； 具体这里将会使用到名为 KUBECONFIG 的环境变量，该环境变量用于保存以冒号分隔的配置文件路径列表； kubectl config 将通过 KUBECONFIG 环境变量加载并组合所有的集群配置信息； 我们可以将 KUBECONFIG 环境变量配置添加到 .bash_profile 或 .zshrc 当中： 1export KUBECONFIG=$HOME/.kube/config-work-test:$HOME/.kube/config-work-dev:$HOME/.kube/config-work-prod:$HOME/.kube/config 设置完后，执行命令以使配置生效：123$ source ~/.bash_profile# 或者$ source ~/.zshrc 也可以打开一个新终端，验证环境变量是否生效：1$ echo $KUBECONFIG 至此，kubectl config 集群配置已经完成; 让我们验证一下！ 1. 获取所有的集群上下文列表：123456$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* docker-for-desktop docker-for-desktop-cluster docker-for-desktop work-test work-test-cluster work-test-admin work-dev work-dev-cluster work-dev-admin work-prod work-prod-cluster work-prod-admin 2. 获取展示当前的上下文：12$ kubectl config current-contextdocker-for-desktop 3. 将当前上下文设置为开发环境：12$ kubectl config use-context work-devSwitched to context &quot;work-dev&quot;. 此时，即可使用 kubectl get nodes 等命令验证一下是否已切换到对应集群环境。 4. 更多的 kubectl config 命令见帮助信息：12345678910111213current-context 显示 current_contextdelete-cluster 删除 kubeconfig 文件中指定的集群delete-context 删除 kubeconfig 文件中指定的 contextget-clusters 显示 kubeconfig 文件中定义的集群get-contexts 描述一个或多个 contextsrename-context Renames a context from the kubeconfig file.set 设置 kubeconfig 文件中的一个单个值set-cluster 设置 kubeconfig 文件中的一个集群条目set-context 设置 kubeconfig 文件中的一个 context 条目set-credentials 设置 kubeconfig 文件中的一个用户条目unset 取消设置 kubeconfig 文件中的一个单个值use-context 设置 kubeconfig 文件中的当前上下文view 显示合并的 kubeconfig 配置或一个指定的 kubeconfig 文件"},{"title":"Kubenetes 常用命令","permalink":"https://erikjiang.github.io/2019/03/10/KubeCommand/","text":"查看 Node1$ kubectl get node -o wide -o wide：该参数表示输出更加详细的信息 创建 Deployments这里创建了一个名为 hello-test 的 Deployment1$kubectl run hello-test --image=alpine --replicas=2 sleep 360000 查看 Deployments查看所有的 Namespaces:1kubectl get deployments --all-namespaces -o wide 查看 Deployments 详情1$ kubectl describe deployment hello-test 删除 Deployments1$ kubectl delete deployment hello-test -n default 查看 Pod1$ kubectl get pod -o wide 查看具体某一个 Pod1$ kubectl get pod &lt;pod-name&gt; -o yaml -o yaml: 以 yaml 格式显示详细信息-o json: 以 json 格式显示详细信息 查看 Pod 的详细信息kubectl describe 不支持 -o 输出参数；1$ kubectl describe pod &lt;pod-name&gt; 查看 Pod 容器内标准输出的日志信息12# -f 随日志信息增长而输出$ kubectl logs &lt;pod-name&gt; -f 命令式配置文件操作基于文件创建集群资源1$ kubectl create -f xxx.yaml 基于文件对资源进行更新、替换操作比如修改了 xxx.yaml 中的副本数、镜像版本、端口等, 此时需要更新资源：1$ kubectl replace -f xxx.yaml 删除配置文件对应的资源1$ kubectl delete -f xxx.yaml 声明式API操作创建及更新资源创建以及修改更新对应资源都可以执行同样的命令：1$ kubectl apply -f xxx.yaml kubectl replace 操作相当于新建一个 API 对象来替换原有 API 对象；kubectl apply 操作则是对原有的 API 对象进行了 Patch 操作；"},{"title":"Minikube MacOS 安装搭建","permalink":"https://erikjiang.github.io/2019/03/09/MinikubeInstall/","text":"要完成 minikube 环境的整体安装搭建，我们需要： docker hyperkit minikube kubectl 安装 dockerdocker 有针对 mac 系统的安装包，只需要下载并安装即可，Get started with Docker Desktop for Mac ; 当然安装运行好后，需要查看一下运行状态是否良好，这个官方文档上也写的很明了； 安装 hyperkitMac 系统下支持 VirtualBox、 VMware Fusion、 Hyperkit 三种虚拟机管理程序，这里虚拟机的作用是为 kubenetes 提供基本环境，由于官方推荐了 Hyperkit，于是选择 Hyperkit 的驱动程序来进行安装： 12345$ brew install docker-machine-driver-hyperkit# docker-machine-driver-hyperkit need root owner and uid $ sudo chown root:wheel /usr/local/opt/docker-machine-driver-hyperkit/bin/docker-machine-driver-hyperkit$ sudo chmod u+s /usr/local/opt/docker-machine-driver-hyperkit/bin/docker-machine-driver-hyperkit 安装 kubectl使用 homebrew 直接安装即可：1$ brew install kubectl 安装 minikube采用手动方式安装 minikube：1$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 &amp;&amp; sudo install minikube-darwin-amd64 /usr/local/bin/minikube 以上执行完成后，就到了较为关键的部分，即使用 minikube start 开启并运行一个集群； 首先，如果直接运行 minikube start 会因为执行过程中，在下载相应依赖时报错； 原因是这些相关的下载项被 GFW 封阻，如果要做到正常下载访问，就需要使用http代理； 具体操作命令如下：123$ minikube start --vm-driver=hyperkit \\ --docker-env http_proxy=http://$YOURPROXY:PORT \\ --docker-env https_proxy=https://$YOURPROXY:PORT 那么问题来了，如何才能拥有一个可访问的 http 代理服务呢？考虑到有 shadowsocks，但是基于 socks 又不能直接使用，于是问题又变成如何将 socks 转为 http proxy 呢？OK，找到了一个转换工具，喔嘈，这配置工作能不能简单点？MMP … 在经过一番挣扎之后，终于发现，原来 shadowsocks 客户端提供了 HTTP 代理功能，晕~ 在这个客户端的 偏好设置 -&gt; HTTP 里面可以直接配置HTTP代理监听地址及端口，爽~ 这里需要注意的是，由于上述 minikube start 是在虚拟机上进行依赖项下载的，故在命令中的代理监听地址不能使用127.0.0.1，而应该是宿主机的IP地址，否则无论如何虚拟机上都无法访问到宿主机的代理服务的。 比方说，宿主机IP是：192.168.1.101，shadowsocks 上设置的 HTTP 代理监听端口设置为1087，那么 minikube start 的实际执行命令应该是：123$ minikube start --vm-driver=hyperkit \\ --docker-env HTTP_PROXY=http://192.168.1.101:1087 \\ --docker-env HTTPS_PROXY=http://192.168.1.101:1087 此外，在 shadowsocks 上设置 HTTP 代理的监听地址时，由于我们上述命令使用的是宿主机的 IP 地址，所以如果这里 HTTP 代理监听地址还是使用 127.0.0.1 这个默认的回环地址的话，上述命令的宿主机IP地址将无法访问到代理服务，于是需要将 shadowsocks 上的代理地址由 127.0.0.1 改为 0.0.0.0，这样代理服务就可以接收任何IP地址的请求了； HTTP 代理设置完成并执行 minikube start 之后，OK，集群启动成功； 此时，配置 kubectl 使用 minikube 上下文，这样使得 bubectl 能够与 minikube 集群进行交互：1$ kubectl config use-context minikube 验证查看 bubectl 已配置的集群信息：1$ kubectl cluster-info 此时，可以使用 kubectl 与 k8s 集群交互，并启动一个服务：1$ kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080 公开该服务为 NodePort：1$ kubectl expose deployment hello-minikube --type=NodePort minikube 可以轻松的使用你的浏览器打开这个公开的端点（endpoint）：1$ minikube service hello-minikube 开启第二个本地集群：1$ minikube start -p cluster2 停止你的本地集群：1$ minikube stop 删除你的本地集群：1$ minikube delete 重启系统后的问题minikube 正常安装完了之后，以为一切归于平静，没想到系统重启后，又出了幺蛾子：12$ kubectl get nodeThe connection to the server xx.xx.xx.xx:8443 was refused - did you specify the right host or port? 关于这个问题，minikube 的 github issue 中有提及，但作者并没有很好的解决，最终给出的意见是建议使用 docker for mac 中提供的 K8S 功能； 感觉是绕了一圈，最终又回到了原点，人生有时就是如此之骚~ 好吧，于是我放弃了minikube，转向 docker for mac 提供的 kubenetes 功能； 由于这其中下载安装仍然困难重重，这里提供一个安装方法，见：k8s-for-docker-desktop 参考： http://blog.samemoment.com/articles/kubernetes/ https://k8smeetup.github.io/docs/tutorials/stateless-application/hello-minikube/"},{"title":"容器笔记：容器大事记","permalink":"https://erikjiang.github.io/2019/01/03/DockerPreview/","text":"容器的历史进程：第一阶段： AWS、 OpenStack 等虚拟机的 PaaS 用法，需要使用脚本或者手动命令操作的方式，将应用从本地部署到云端； 第一阶段产生的问题：云端虚拟机与本地环境不一致，需要在云端做繁琐且无章可循的操作，比如运行依赖环境的安装、配置的调整等，使得云端能够与本地环境一致，从而运行应用； 第二阶段：Cloud Foundry 的 PaaS 项目兴起，它提供了一套核心组件能够实现应用的打包和分发；由于需要在一个虚拟机上启动很多个来自不同用户的应用，Cloud Foundry 调用了操作系统的 Cgroups 和 Namespace 机制，为每个应用创建了一个单独的沙盒隔离环境，然后应用在沙盒中启动，这使得虚拟机中的应用互不干扰、且能够批量、自动运行； 而这个过程中，调用系统的 Cgroups 和 Namespace 机制，所创建的沙盒隔离环境，就是所谓的“容器”； 第二阶段产生的问题：虽然比第一阶段有所改善，但它需为每种环境、框架、应用维护一个包，且部署应用时可能还会有问题，需要做大量修改及调整配置； 第三阶段：Docker 容器和镜像的兴起，当时的 Docker 项目实际和 Cloud Foundry 类似，同样采用了系统的 Cgroups 和 Namespace 机制实现沙盒；但不同的是，Docker 创新的引入了镜像的概念，并根本性的解决的部署打包的问题； Docker 镜像提供了一种非常便利的打包机制。这种机制直接打包了应用运行所需要的整个操作系统，从而保证了本地环境和云端环境的高度一致，避免了用户通过“试错”来匹配两种不同运行环境之间差异的痛苦过程。 第四阶段: PaaS 领域开源基础设施平台 Kubernetes 的兴起；它主要关注于“容器编排” 当程序被执行时，它就会从磁盘上的一个二进制文件，变成计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合；像这样一个程序运起来后的计算机执行环境的总和，即被称为“进程”； 程序的静态表现，即磁盘上的二进制文件； 程序的动态表现，即在运行时计算机中数据和状态的总和； 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造一个“边界”； 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而Namespace 技术则是用来修改进程视图的主要方法。 虚拟机技术与容器技术虚拟机技术-优点：虚拟机与宿主机隔离彻底，环境高度独立，安全性高；虚拟机技术-缺点：本身占用内存资源，与宿主机调用时会被虚拟化软件拦截并处理，此过程会对计算、网络、磁盘I/O产生性能消耗； 容器技术-优点：轻量化、节省资源、与宿主机调用无性能消耗；容器技术-缺点：隔离不彻底，攻击面比虚拟机大，由于容器只是运行在宿主机上的特殊进程，故容器间使用的是同一个宿主机操作系统的内核，意味着低版本宿主机不能运行高版本容器，windows上不能运行linux容器（除非借助Linux虚拟机）；此外，linux内核的很多资源及对象不能 Namespace 化，比如时间； Linux Namespace 可以对操作系统中的 PID、Mount、UTS、IPC、Network、User 等的各种不同的进程上下文进行“障眼法”操作； 操作 Namespace 的方法是调用 linux 创建进程的函数 clone(); Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能，它主要的作用是可以限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等； 操作 Cgroups 的方法既是在 /sys/fs/cgroup/ 目录下创建一个子系统目录，然后修改对应的资源限制文件； 容器是一个“单进程”模型。 容器与应用是同生命周期的，不会存在容器正常运行，但内部应用早已挂掉的情况； Mount Namespace 用来隔离文件系统的挂载点，使得进程只能看到自己的 mount namespace 中的文件系统挂载点；它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效，若单单仅开启 Mount Namespace，则新创建的容器会直接继承宿主机的各个挂载点； ROOTFS:rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。由于rootfs里打包的并不只是应用，而是整个操作系统的文件和目录，这就意味着，应用及它运行所需的所有依赖都被封装在了一起； 对一个应用来说，操作系统本身才是它运行所有需要的完整“依赖库”；Dockerfile Dockerfile 中的每个原语执行后，都会生成一个对应的的镜像层； https://time.geekbang.org/column/article/14653"},{"title":"Docker 构建小记","permalink":"https://erikjiang.github.io/2018/12/14/MonitorDockerBuild/","text":"关于在项目 market_monitor 尝试进行 Docker 构建的过程中的一些记录整理； 构建应用服务 docker 镜像 本项目见：market_monitor docker 构建操作步骤编写镜像构建文件：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162############################# STEP 1 构建可执行文件############################# 指定 GO 版本号ARG GO_VERSION=1.11.1# 指定构建环境FROM golang:$&#123;GO_VERSION&#125;-alpine AS builder# ca-certificates is required to call HTTPS endpoints.# tzdata is required to time zone info.RUN apk update &amp;&amp; apk upgrade &amp;&amp; apk add --no-cache ca-certificates tzdata &amp;&amp; update-ca-certificates# 创建用户 appuserRUN adduser -D -g '' appuser# 复制源码并指定工作目录RUN mkdir -p /src/myappCOPY ./src/ /src/myappWORKDIR /src/myapp# 为 go build 设置环境变量:# * CGO_ENABLED=0 表示构建一个静态链接的可执行程序# * GOOS=linux GOARCH=amd64 表示指定linux 64位的运行环境# * GOPROXY=https://goproxy.io 指定代理地址# * GOFLAGS=-mod=vendor 在执行 `go build` 强制查看 `/vendor` 目录ENV CGO_ENABLED=0 GOOS=linux GOARCH=amd64 GOFLAGS=-mod=vendor# 构建可执行文件RUN go build -a -installsuffix cgo -ldflags=\"-w -s\" -o /src/myapp/monitor############################# STEP 2 构建镜像############################# 指定最小镜像源FROM scratch AS final# 设置系统语言ENV LANG en_US.UTF-8# 从 builder 中导入时区信息COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo# 从 builder 中导入证书COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/# 从 builder 中导入用户及组相关文件COPY --from=builder /etc/passwd /etc/passwd# 将构建的可执行文件复制到新镜像中COPY --from=builder /src/myapp/config /configCOPY --from=builder /src/myapp/monitor /monitor# 端口申明EXPOSE 8000# 运行ENTRYPOINT [ \"/monitor\" ] 在构建镜像前，为了保证效率，我们最好配置一个国内的镜像地址，可以采用阿里云的镜像加速器地址； 执行构建命令：12# 指定镜像名为 monitordocker build -t monitor . 构建成功，查看镜像列表： 发现多了一个monitor镜像1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmonitor latest 3cce1a5a8514 12 minutes ago 34.6MBgolang 1.11.1-alpine 95ec94706ff6 3 months ago 310MB 运行容器服务：1docker run -p 8000:8000 monitor -d monitor 镜像的构建流程这里使用到 Docker 的多阶段构建(multi-stage builds)方式; 按步骤主要两个阶段： 源码编译阶段，镜像使用：golang:1.11.1-alpine 环境构建阶段，镜像使用：scratch 大致过程，我们会在第一个镜像中安装运行所需的依赖、创建用户、将源码编译为可执行文件等，然后再将运行所需的所有文件拷贝至第二个镜像中，第二个镜像最终会作为容器的运行镜像； 为什么不直接用一个镜像完成所有操作？1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmonitor latest 3cce1a5a8514 12 minutes ago 34.6MBgolang 1.11.1-alpine 95ec94706ff6 3 months ago 310MB 如上就可以说明问题，monitor是采用scratch最终生成的镜像，而golang-1.11.1-alpine是用于编译源码的基础镜像，如果采用golang-alpine作为基础镜像，再加上源码及依赖包文件，总大小可能会在500MB左右，这对于运维部署是不能接受的；而scratch能够很好的控制镜像大小，它本身即是一个精简的最小镜像，再加上golang编译过程中的优化，最终镜像尺寸会很小； 处理构建过程的依赖包问题本来一开始是打算采用go mod download的方式，在镜像环境中下载依赖包以满足构建编译，但是没走通，故转而尝试vendor的方式；也就是说我们会首先将所有依赖包存入项目根目录的vendor/下，然后连同项目源码一起拷贝至源码编译镜像环境，再进行编译操作； 首先，要保证你曾成功执行过go build或go run main.go操作，只有这样，才能迅速将依赖包的缓存文件汇总到vendor/目录下； 当然，你要去执行go mod vendor，它才会生成vendor/目录； 然后，需要在dockerfile的golang编译命令前添加环境变量，指定依赖从vendor/目录读取；1ENV GOFLAGS=-mod=vendor 如此，依赖问题就解决了； HTTPS X509 问题如果项目中有包含对第三方HTTPS接口的请求，那么多少都会遇到如下报错：1x509: certificate signed by unknown authority 这个问题，是没有安装导入CA证书的原因； 解决方法分两步： 编译源码阶段，需要安装ca-certificates 1RUN apk update &amp;&amp; apk upgrade &amp;&amp; apk add --no-cache ca-certificates &amp;&amp; update-ca-certificates 构建最小环境阶段，需要将证书拷入镜像环境 1COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ 关于编排ok，经过如上步骤的操作尝试，会发现虽然服务已运行，但是 mysql、redis 无法正常连接，噢，mysql、redis 容器服务还没有跑起来 … 是不是有些烦躁，因为 mysql、redis 容器服务的构建还会涉及到一些初始化操作、账号权限配置、密码更改等很多自定义设置，对于一介开发，有些凌乱； 但是，别忘了 docker compose 容器编排，它将为你解决余下的事情，请见： Compose 容器编排部署 create the smallest and secured golang docker image based on scratch"},{"title":"CentOS7 安装 Docker 及 Docker Compose","permalink":"https://erikjiang.github.io/2018/11/13/InstallDockerCentOS/","text":"记录 CentOS 环境下 Docker 相关的安装过程，备忘~ Step 1 — 安装 Docker安装需要的依赖包：1$ sudo yum install -y yum-utils device-mapper-persistent-data lvm2 配置 docker-ce repo：1$ sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 安装 docker-ce：1$ sudo yum install docker-ce 将当前用户添加至 Docker 用户组当中：1$ sudo usermod -aG docker $(whoami) 设置 Docker 在引导时自启动：1$ sudo systemctl enable docker.service 最后运行 Docker 服务：1$ sudo systemctl start docker.service Step 2 — 安装 Docker Compose为企业版linux安装额外依赖包：1$ sudo yum install epel-release 安装 python-pip：1$ sudo yum install -y python-pip 然后安装 Docker Compose：1$ sudo pip install docker-compose 同时需要为你在 CentOS 7 下的 Python 进行升级，以便能够成功运行 docker-compose：1$ sudo yum upgrade python* 验证 Docker Compose 是否成功安装完成：1$ docker-compose version"},{"title":"JS 与 GO 差异比较","permalink":"https://erikjiang.github.io/2018/10/25/JsGoCompare/","text":"Javascript is an event driven, dynamically typed and interpreted language, while Go is a statically typed and compiled language. 语言特性js事件驱动、动态类型、解释型语言 go静态类型、编译型语言 编译时安全Go is compiled. Javascript is not, though some Javascript runtimes use JIT compilation. From the developer experience perspective, the biggest effect of compiled languages is compile-time safety. You get compile-time safety with Go, while in Javascript you can use external code linters to ease the missing of this feature. 顺序执行 与 并发执行js 顺序执行 12345678async function fetchSequential() &#123; const a = await fetchA(); console.log(a); const b = await fetchB(); console.log(b); const c = await fetchC(); console.log(c);&#125; 并发执行 1234async function fetchConcurrent() &#123; const values = await Promise.all([fetchA(), fetchB(), fetchC()]) console.log(values);&#125; go 顺序执行 12345678func fetchSequential() &#123; a := fetchA() fmt.Println(a) b := fetchB() fmt.Println(b) c := fetchC() fmt.Println(c)&#125; 并发执行 123456789101112131415161718192021222324252627func fetchConcurrent() &#123; aChan := make(chan fetchResult, 0) bChan := make(chan fetchResult, 0) cChan := make(chan fetchResult, 0) go func(c chan fetchResult) &#123; c &lt;- fetchA() &#125;(aChan) go func(c chan fetchResult) &#123; c &lt;- fetchB() &#125;(bChan) go func(c chan fetchResult) &#123; c &lt;- fetchC() &#125;(cChan) for i := 0; i &lt; 3; i++ &#123; select &#123; case a := &lt;-aChan: fmt.Println(a) case b := &lt;-bChan: fmt.Println(b) case c := &lt;-cChan: fmt.Println(c) &#125; &#125;&#125; this 关键字jsthis 是对象Object的引用，指向对象内部属性或内部方法 gojs 的 this 概念，近似于 go 中的接收器 receiver ，可以将接收器变量命名为 this;(但一般建议使用短变量，单个字母命名)1234567type Bar struct &#123; foo string&#125;func (this *Bar) Foo() string &#123; return this.foo&#125; new 关键字jsnew Foo() 将从 Foo 返回一个实例化对象，而 Foo 一般被认为是构造函数或者类; gonew(T) 会为类型T的新条目，分配零值存储，并返回指针*T; 在JS等其他语言，new被用于初始化对象，而go中则只是将对象设置为零值； 在 go 中，以New为前缀命名的方法，表示将会返回方法名New后面名字为类型名的指针；1timer := time.NewTimer(d) // timer is a *time.Timer setTimeout / timer 定时器延时执行js1setTimeout(somefunction, 3*1000) go1time.AfterFunc(3*time.Second, somefunction) setInterval / ticker 定时器周期执行js1setInterval(somefunction, 3*1000) go123456ticker := time.NewTicker(3 * time.Second)go func() &#123; for t := range ticker.C &#123; somefunction() &#125;&#125;() String 字面量不同在于 go 使用双引号&quot;&quot;表示字符串, 而 js 既可以双引号&quot;&quot;也可以单引号&#39;&#39; 值类型Values, 指针类型Pointers, 引用类型Referencesjs既有值类型也有引用类型，值类型如string、number，引用类型像object、array、function123456789101112131415var a = &#123; message: 'hello'&#125;var b = a;// mutateb.message = 'goodbye';console.log(a.message === b.message); // prints 'true'// reassignb = &#123; message: 'galaxy'&#125;console.log(a.message === b.message); // prints 'false' go既有值类型也有引用类型同时还有指针类型，引用类型：切片slices、字典maps、通道channels，其余的都是值类型，但是可以通过指针的方式获得被引用的能力；指针和引用的虽然都可以对所被指向或被引用的底值进行修改，但是指针还可以做到重新分配底值（underlaying value）；12345678910111213141516a := struct &#123; message string&#125;&#123;\"hello\"&#125;b := &amp;a// mutate// note b.message is short for (*b).messageb.message = \"goodbye\"fmt.Println(a.message == b.message) // prints \"true\"// reassign*b = struct &#123; message string&#125;&#123;\"galaxy\"&#125;fmt.Println(a.message == b.message) // prints \"true\" Switch 语句jsjs 中 switch 的 case 子句后面默认带有 fallthrough，即在匹配到对应 case 并执行后，会默认执行之后的 case，除非在 case 后添加 break 跳出整个 switch;1234567891011switch (favorite) &#123; case \"yellow\": console.log(\"yellow\"); break; case \"red\": console.log(\"red\"); case \"pruple\": console.log(\"(and) purple\"); default: console.log(\"white\");&#125; gogo 则恰恰相反，case 子句后面默认带有 break，即在匹配到对应 case 并执行后会默认直接跳出整个 switch，除非在 case 后面添加 fallthrough，才能接着执行之后的 case 语句；1234567891011switch favorite &#123;case \"yellow\": fmt.Println(\"yellow\")case \"red\": fmt.Println(\"red\") fallthroughcase \"pruple\": fmt.Println(\"(and) purple\")default: fmt.Println(\"white\")&#125; 函数嵌套函数 function 在 js 和 go 中都是一等公民，都被允许作为参数传递、作为值返回，可进行函数嵌套，可闭包； 但请注意，关于函数嵌套，js 能够使用命名函数或匿名函数完成，但 go 仅能使用匿名函数完成嵌套； 函数多值返回jsjs 原生并不支持函数多值返回，但是 ES6 中提供了解构赋值语法，可以实现类似多值返回的功能；123456function hello() &#123; return [\"hello\", \"world\"];&#125;var [a, b] = hello();console.log(a,b); gogo 原生支持函数的多值返回；12345678func hello() (string, string) &#123; return \"hello\", \"world\"&#125;func main() &#123; a, b := hello() fmt.Println(a, b)&#125; 错误处理流程控制Javascript使用throw catch finally块，Go使用panic recover defer 参考：go for javascript developers"},{"title":"Go笔记之基于共享变量的并发","permalink":"https://erikjiang.github.io/2018/09/02/GoConcurrent/","text":"这篇我们了解下并发机制；尤其是多goroutine之间的共享变量，并发问题的分析手段，以及解决这些问题的基本模式； 竞争条件竞争条件指的是程序在多个goroutine交叉执行操作时，没有给出正确的结果。竞争条件带来的问题非常难以复现而且难以分析诊断。 并发安全 如果一个程序或函数或类型在线性程序中能够正常操作运行，却当在并发情况下依然能够正常操作运行，那么我们认为该程序或函数或类型是并发安全的； 导出包级别的函数一般情况都是并发安全的；而包级变量必须采用互斥条件； 数据竞争 数据竞争会在两个以上的goroutine并发访问相同的变量且至少其中一个为写操作时发生。避免数据竞争的三种方式：123&gt; 不要对变量进行写操作；&gt; 避免从多个goroutine访问变量，使用channel；&gt; 多goroutine访问变量，采用互斥方式，同一时刻最多仅有一个goroutine访问； sync.Mutex 互斥锁 通过容量为1的缓存通道实现互斥 一个只能为1和0的信号量叫做二元信号量；123456789var ( sema = make(chan struct&#123;&#125;, 1) // a binary semaphore guarding balance balance int)func Deposit(amount int) &#123; sema &lt;- struct&#123;&#125;&#123;&#125; // acquire token balance = balance + amount &lt;-sema // release token&#125; 通过sync.Mutex进行互斥操作 12345678910var ( mu sync.Mutex // guards balance balance int)func Deposit(amount int) &#123; mu.Lock() balance = balance + amount // 临界区代码段 mu.Unlock()&#125; 共享变量与临界区 mutex 所保护的共享变量一般在mutex变量声明之后立刻声明；在Lock和Unlock之间包含的代码段可以进行正常读写操作，这之间的代码段称为临界区； Lock与Unlock必须成对调用 临界区逻辑复杂时，使用defer Unlock() 在临界区代码段逻辑复杂情况下，可使用defer Unlock()以简化代码逻辑分支中重复的Unlock()调用12345func Balance() int &#123; mu.Lock() defer mu.Unlock() return balance&#125; 互斥所无法重入 go中互斥锁无法重入，即无法对已经上锁的mutex再次上锁，这将会导致程序死锁，没法继续执行，从而使得程序一直保持阻塞状态； 遇到重入情况，可以将代码拆分封装为多个函数，消除锁重入情况； sync.RWMutex 读写锁有时会存在一个接口要求写操作完全互斥，而读操作希望是并行执行的，这种时候的锁称为“多读单写”锁，go提供sync.RWMutex支持该锁行为；1234567var mu sync.RWMutexvar balance intfunc Balance() int &#123; mu.RLock() // readers lock defer mu.RUnlock() return balance&#125; RLock只能在临界区共享变量没有任何写入操作时可用。一般来说，我们不应该假设逻辑上的只读函数/方法也不会去更新某一些变量。如果对这类函数/方法的读写行为不确定，请使用互斥锁； 内存同步由于现代计算机常见为多处理器，每个处理器都会有其本地缓存，出于效率，对内存的写入操作一般会在每个处理器中缓存，并在必要时一并flush到主存。而在多任务并行时，多个goroutine会在不同的CPU上运行，每个CPU存在独立的本地缓存，各goroutine的写入操作首先仅能作用于CPU本地缓存，在没有同步到主存之前，各CPU的写入操作均为不可见的； 所以在设计到内存同步问题时，建议如下： 将变量限定于goroutine内部； 若为多goroutine都需要访问的变量，采用互斥条件来访问； sync.Once 初始化在多goroutine并发情况下进行初始化操作，可能会存在goroutine重复初始化操作的情况，这时，可以使用sync.Once进行初始化操作，它将锁定mutex,并检测是否已初始化的布尔变量，若为初始化，则执行初始化函数并设置布尔变量为true；当其他goroutine执行时，会检测该布尔值，为false代表未初始化，为true表示已执行完成初始化操作；其中Do方法传参为初始化函数；1234567891011121314func loadIcons() &#123; icons = make(map[string]image.Image) icons[\"spades.png\"] = loadIcon(\"spades.png\") icons[\"hearts.png\"] = loadIcon(\"hearts.png\") icons[\"diamonds.png\"] = loadIcon(\"diamonds.png\") icons[\"clubs.png\"] = loadIcon(\"clubs.png\")&#125;var loadIconsOnce sync.Oncevar icons map[string]image.Image// Concurrency-safe.func Icon(name string) image.Image &#123; loadIconsOnce.Do(loadIcons) return icons[name]&#125; 竞争条件检测go的运行时环境和工具链提供了一套动态分析工具，专用于并发程序的竞态检测（the race detector）。 go命令-race选项只要在go build，go run或者go test命令后面加上-race的flag，就会使编译器创建一个你的应用的“修改”版或者一个附带了能够记录所有运行期对共享变量访问工具的test，并且会记录下每一个读或者写共享变量的goroutine的身份信息。 race仅能检测到运行时的竞争条件确保并发测试能够尽可能的覆盖； 添加race的程序运行会慢一些由于需要额外的记录，因此构建时加了竞争检测的程序跑起来会慢一些，且需要更大的内存，即使是这样，这些代价对于很多生产环境的工作来说还是可以接受的； Goroutines与线程之间比较 栈： goroutine: 栈大小动态伸缩（2KB~1GB）thread: 栈大小固定（一般2MB） 调度方式： goroutine: 语言本身特性实现调度协程的切换步骤： 休眠当前goroutine 唤醒执行下一个goroutine整个过程不需要进入内核上下文，开销成本低； thread: 操作系统内核实现调度线程的切换步骤： 挂起当前线程 保存挂起线程的寄存器信息 检查线程列表选出下一个执行线程 恢复执行线程的寄存器信息 恢复执行线程环境并执行整个步骤需要完整的上下文切换，切换效率很慢； 操作系统CPU多核利用 goroutine:可以使用GOMAXPROCS变量来决定同时执行Go代码的操作系统CPU核心数；天然支持多核利用； thread:各语言线程利用多核方式不同；多进程方式较多； 程序单元身份ID goroutine:goroutine设计上取消掉了身份ID,简化了多任务并行处理复杂度； thread:线程存在身份ID,可以使用线程本地存储（thread-local storage）进行管理,但是线程本地存储总被滥用，是的函数行为别的不可预知，过于复杂化；"},{"title":"Go笔记之协程及通道","permalink":"https://erikjiang.github.io/2018/09/01/GoGoroutines/","text":"协程与通道是实现go多任务并发程序的方式；goroutine和channel，其支持“顺序通信进程”(communicating sequential processes)或被简称为CSP。CSP是一种现代的并发编程模型，在这种编程模型中值会在不同的运行实例(goroutine)中传递，尽管大多数情况下仍然是被限制在单一实例中。 goroutines 协程在Go语言中，每一个并发的执行单元叫作一个goroutine。当程序启动时，main函数即在一个单独的goroutine中运行，我们叫它main goroutine; 在一个普通的函数或方法调用前添加关键字go，即为goroutine函数：go f(); channel 通道goroutine是go语言程序的并发体，而channels是并发体之间的通信机制；channel是引用类型，其零值为nil； 通过make函数创建channelchannel使用make函数创建将对应一个底层的数据结构引用； 1234// 创建一个无缓存int类型的通道ch := make(chan int)// 创建一个缓存容量为10的int类型的通道ch1 := make(chan int, 10) channel的运算符比较两个相同类型的channel可以使用==运算符比较；一个channel也可以和nil进行比较。 channel主要的两种通信行为channel主要有发送和接收两种通信操作行为;通过&lt;-运算符表示发送与接收操作，当该运算符位于channel标识符右侧表示发送，当该运算符位于channel标识符左侧表示接收 123ch := make(chan in)ch &lt;- x // 发送x到通道ch&lt;- ch // 接收通道中的信息 channel的关闭Close操作当对一个已经关闭的channel进行接收操作依然可以收到之前已经成功发送的数据；当对一个已经关闭的channel进行接收操作且该channel已经没有数据时将会产生一个零值数据；当试图重复关闭或者关闭一个nil值的channel都会导致panic异常；内置close函数即可关闭channel:close(ch);不管channel是否被关闭，当它没有被引用时将会被Go语言的垃圾自动回收器回收。 无缓存channels无缓存Channels也被称为同步Channels。 一个基于无缓存Channels的发送或接收操作都会导致当前操作的goroutine阻塞，直到另一个goroutine在相同的channels上执行接收或发送操作； 串联的channels即pipelineChannels也可以用于将多个goroutine连接在一起，一个Channel的输出作为下一个Channel的输入。这种串联的Channels就是所谓的管道（pipeline） 判断channel是否关闭且无值可接收通道的接收操作可返回两个值，其中ok可以判断当前channel是否已关闭且无值可接收； 1234x, ok := &lt;- chif !ok &#123; // todo ...&#125; 使用 range channel代替&lt;-接收使用range循环依次从channel接收数据，当channel被关闭并且没有值可接收时将跳出循环。 单向的通道通道有时的运用场景是典型单一的；可能会被专门用于只发送或者只接收； 为了表明这种意图并防止被滥用，Go语言的类型系统提供了单方向的channel类型，分别用于只发送或只接收的channel。 类型chan&lt;- int表示一个只发送int的channel，只能发送不能接收。相反，类型&lt;-chan int表示一个只接收int的channel，只能接收不能发送。（箭头&lt;-和关键字chan的相对位置表明了channel的方向。）这种限制将在编译期检测;123456func squarer(out chan&lt;- int, in &lt;-chan int) &#123; for v := range in &#123; out &lt;- v * v &#125; close(out)&#125; 只有在发送者所在的goroutine才会调用close函数因为关闭操作只用于断言不再向channel发送新的数据，所以只有在发送者所在的goroutine才会调用close函数，因此对一个只接收的channel调用close将是一个编译错误。 缓存通道make函数创建channel时，第二个参数可指定缓存容量； 向缓存Channel的发送操作就是向内部缓存队列的尾部插入元素，接收操作则是从队列的头部删除元素。 如果内部缓存队列是满的，那么发送操作将阻塞直到因另一个goroutine执行接收操作而释放了新的队列空间。相反，如果channel是空的，接收操作将阻塞直到有另一个goroutine执行发送操作而向队列插入元素； cap()及len()在通道channel中的使用cap()函数用于获取通道的缓存容量；len()函数用于获取通道缓存队列中有效元素个数； 并发的循环基于select多路复用当我们希望能够从channel中发送或者接收值，并避免因为发送或者接收导致的阻塞，尤其是当channel没有准备好写或者读时。select语句就可以实现这样的功能。select会有一个default来设置当其它的操作都不能够马上被处理时程序需要执行的逻辑。12345678910select &#123;case &lt;-ch1: // ...case x := &lt;-ch2: // ...use x...case ch3 &lt;- y: // ...default: // ...&#125; Tick()函数time.Tick函数的表现就像创建了一个在循环中调用time.Sleep的goroutine，每次被唤醒时发送一个事件。当select多路复用结束时，它会停止从tick中接收事件，但是ticker这个goroutine还依然存活，继续徒劳地尝试向channel中发送值，然而这时候已经没有其它的goroutine会从该channel中接收值了，这种情况被称为goroutine泄露； Tick()函数只有当程序整个生命周期都需要这个时间时我们使用它才比较合适；否则应该采用如下方式：123ticker := time.NewTicker(1 * time.Second)&lt;-ticker.C // receive from the ticker's channelticker.Stop() // cause the ticker's goroutine to terminate select 多路复用中case激活禁用方式由于当channel为零值nil时，其发送和接收都会被阻塞，而在多路复用中channel如果为nil,将不会被select到；因此在select中可以通过为channel赋nil值，作为开关激活或禁用通道； 并发的退出通过关闭一个channel来进行广播，然后每个goroutine通过select轮询检测channel的关闭状态，一旦检测到就自动退出当前goroutine函数；"},{"title":"Go笔记之反射","permalink":"https://erikjiang.github.io/2018/08/26/GoReflect/","text":"Reflection（反射）在计算机中表示程序能够检查自身结构的能力，尤其是类型；它是元编程的一种形式； interface与反射 变量由 (type,value) 类型与值两部分组成; 类型Type包括static type静态类型与concrete type具体类型; 12静态类型即指go语言编码中指定的类型，如int、string具体类型指运行时runtime系统看见的类型 类型断言取决于具体类型而不是静态类型; 反射永远指的是interface类型，而非go语言指定的静态类型； 每个interface变量实现中都有一个对应的pair，即记录了实际变量的值与类型的键值对：(value, type)； interface{}包含两个指针，一个指向值的类型concrete type，一个指向实际的值value； 反射就是用来检测存储在接口变量内部pair对(value, concrete type)的一种机制； 反射reflect基本功能TypeOf和ValueOf reflect.ValueOf()和reflect.TypeOf()用于访问接口变量pair中的value及type； 12345// ValueOf用来获取输入参数接口中的数据的值，如果接口为空则返回0func ValueOf(i interface&#123;&#125;) Value &#123;...&#125;// TypeOf用来动态获取输入参数接口中的值的类型，如果接口为空则返回nilfunc TypeOf(i interface&#123;&#125;) Type &#123;...&#125; 反射类型变量reflect.Type与reflect.Value 12通过执行reflect.ValueOf(interface)，将会返回一个类型为reflect.Value的变量通过执行reflect.TypeOf(interface)，将会返回一个类型为reflect.Type的变量 反射类型转换为真实类型有两种情况 12345678910111213141516171819202122&gt; 情况1. 已知原有类型（使用类型断言强制装换）`reflect.Value`变量可以通过其本身`interface()`方法获取接口变量真实内容，然后再通过类型断言进行转换，转换为原来的真实类型；大致过程即: 反射类型 -&gt; 接口类型 -&gt; 真实类型realValue := reflectValue.Interface().(已知的类型)注意：类型断言中的预转换类型应该与已知类型一致，否则会触发Panic；&gt; 情况2. 未知原有类型（遍历探测其Filed及Method）&gt; 2.1. 获取未知类型的interface的具体变量及其类型的步骤为：1) 先获取interface的reflect.Type，然后通过NumField进行遍历2) 再通过reflect.Type的Field获取其Field3) 最后通过Field的Interface()得到对应的value&gt; 2.2. 获取未知类型的interface的所属方法（函数）的步骤为：1) 先获取interface的reflect.Type，然后通过NumMethod进行遍历2) 再分别通过reflect.Type的Method获取对应的真实的方法（函数）3) 最后对结果取其Name和Type得知具体的方法名 通过reflec.Value设置实际变量的值reflect.Value是通过reflect.ValueOf(X)获得的，只有当X是指针的时候，才可以通过reflec.Value修改实际变量X的值，即：要修改反射类型的对象就一定要保证其值是可寻址addressable的;123456var pi float64 = 3.1415poiner := reflect.ValueOf(&amp;pi)newVal := poiner.Elem()newVal.Type() // float64newVal.CanSet() // truenewVal.SetFloat(1.618) 当修改实际变量值时，reflect.ValueOf的传参必须为指针； relect.Value通过Elem()方法获取原始值对应的反射对象, 如果传入的非指针调用Elem()会Panic; 通过CanSet()方法判断是否可修改设置，如果传入的非指针则返回false; 通过reflect.ValueOf来进行方法的调用 要通过反射来调用起对应的方法，必须先通过reflect.ValueOf(interface)获取reflect.Value，得到“反射类型对象”后才能做下一步处理； 通过reflect.Value.MethodByName(funcName)方法注册并获取调用方法，需要注意的是，必须传入准确真实的方法名称，如果错误将直接panic，MethodByName返回一个函数值对应的reflect.Value方法的名字。 []reflect.Value作为最终需要调用的方法的参数，可以没有或者一个或者多个，根据实际参数来定。 reflect.Value的Call()方法，通过反射方式最终调用真实的方法，参数务必保持一致； 反射reflect性能golang的反射性能不理想，原因： reflect视线中存在大量的枚举，即for循环，如类型等； reflect涉及到内存分配及后续的垃圾回收GC; 小结 反射提高的程序的灵活性，使得interface{}发挥更大作用反射必须结合interface类型，也即是说变量的类型Type必须是具体类型concrete type; 反射可以将接口类型变量转换为反射类型变量通过反射的TypeOf及ValueOf方法将接口变量转为反射变量； 反射也可将反射类型变量转换为接口类型变量在已知类型情况下通过reflect.value.Interface().(已知的类型)方式转换接口类型变量；在未知类型情况下通过遍历reflect.Type的Field然后通过Field.Interface()获取接口类型变量； 反射可以修改反射类型变量，但其值必须是可寻址的addressable 通过反射可以动态的调用方法 参考阅读： Golang的反射reflect深入理解和示例 反射三定律"},{"title":"Go笔记之接口","permalink":"https://erikjiang.github.io/2018/08/24/GoInterface/","text":"接口类型是对其它类型行为的抽象和概括；因为接口类型不会和特定的实现细节绑定在一起，通过这种抽象的方式可以让我们的函数更加灵活和更具有适应能力； 接口约定接口类型是一种抽象的类型(不同于数组、映射、切片等具体类型)。它不会暴露出它所代表的对象的内部值的结构和这个对象支持的基础操作的集合；它们只会表现出它们自己的方法。也就是说当你有看到一个接口类型的值时，你不知道它是什么，唯一知道的就是可以通过它的方法来做什么。 接口类型接口类型具体描述了一系列方法的集合，一个实现了这些方法的具体类型是这个接口类型的实例。 接口类型可以通过组合已有的接口来定义123456789101112131415type Reader interface &#123; Read(p []byte) (n int, err error)&#125;type Closer interface &#123; Close() error&#125;type ReadWriter interface &#123; Reader Writer&#125;type ReadWriteCloser interface &#123; Reader Writer Closer&#125; 以上接口类型的组合类似于结构内嵌，可以称为接口内嵌 实现接口的条件 接口指定规则表达一个类型属于某个接口只要这个类型实现这个接口 即使具体类型有其它的方法，也只有接口类型暴露出来的方法能被调用到 空接口类型空接口类型interface{}空接口类型对实现它的类型没有要求，所以我们可以将任意一个值赋给空接口类型;interface{}代表空接口，任何类型都是它的实现类型；将非接口类型标识符x转换为接口类型：interface{}(x); 空花括号{}: 一对不包裹任何东西的花括号，除了可以代表空的代码块之外，还可以用于表示不包含任何内容的数据结构（或者说数据类型） 12345struct&#123;&#125; // 代表了不包含任何字段和方法的、空的结构体类型interface&#123;&#125; // 代表了不包含任何方法定义的、空的接口类型[]string&#123;&#125; // 代表空切片，值不包含任何元素map[int]string&#123;&#125; // 空字典... 类型转换语法形式是T(x)，其中的x可以是一个变量，也可以是一个代表值的字面量，还可以是一个表达式。注意，如果是表达式，那么该表达式的结果只能是一个值，而不能是多个值。在这个上下文中，x可以被叫做源值，它的类型就是源类型，而那个T代表的类型就是目标类型。 并非仅指针类型才能实现接口类型接口类型也能被其他引用类型实现，如slice、map、function等，甚至基本类型也可以实现一些接口； flag.Value接口接口值接口值，由两个部分组成，一个具体的类型和那个类型的值。它们被称为接口的动态类型和动态值; 空接口值即其类型type和值value都为nil 调用一个空接口值上的任意方法都会产生panic 接口值可以使用==和!＝来进行比较两个接口值相等仅当它们都是nil值，或者它们的动态类型相同并且动态值也根据这个动态类型的==操作相等。因为接口值是可比较的，所以它们可以用在map的键或者作为switch语句的操作数。 通过%T获取接口值类型在fmt包内部，使用反射来获取接口动态类型的名称fmt.Printf(&quot;%T\\n&quot;, w) 一个包含nil指针的接口不是nil接口一个不包含任何值的nil接口值和一个刚好包含nil指针的接口值是不同的 sort.Interface接口内置的排序算法需要知道三个东西： 序列的长度 表示两个元素比较的结果 一种交换两个元素的方式 1234567package sorttype Interface interface &#123; Len() int Less(i, j int) bool // i, j are indices of sequence elements Swap(i, j int)&#125; 为了对序列进行排序，我们需要定义一个实现了这三个方法的类型，然后对这个类型的一个实例应用sort.Sort函数。 http.Handler接口1234567package httptype Handler interface &#123; ServeHTTP(w ResponseWriter, r *Request)&#125;func ListenAndServe(address string, h Handler) error error 接口error类型实际上是interface类型123type error interface &#123; Error() string&#125; 创建error方式 123456// 三种import \"fmt\"import \"errors\"errors.New() // 创建errorfmt.Errorf() // 创建error并格式化，是errors.New()的封装syscall.Errno(1)// 创建err的数字类型，满足标准错误接口 类型断言类型断言是一个使用在接口值上的操作(非接口类型需要通过interface{}进行转换后再进行断言操作)。语法上它看起来像x.(T)被称为断言类型，这里x表示一个接口的类型而T表示一个类型。一个类型断言检查它操作对象的动态类型是否和断言的类型匹配。 断言的两种可能 12&gt; 1. 断言的类型T为具体类型&gt; 2. 断言的类型T为接口类型 断言时返回两个参数可以避免发生panic 123var w io.Writer = os.Stdoutf, ok := w.(*os.File) // success: ok, f == os.Stdoutb, ok := w.(*bytes.Buffer) // failure: !ok, b == nil 基于类型断言识别错误类型12345678910111213141516171819import ( \"errors\" \"syscall\")var ErrNotExist = errors.New(\"file does not exist\")// IsNotExist returns a boolean indicating whether the error is known to// report that a file or directory does not exist. It is satisfied by// ErrNotExist as well as some syscall errors.func IsNotExist(err error) bool &#123; // 错误类型断言 if pe, ok := err.(*PathError); ok &#123; err = pe.Err &#125; return err == syscall.ENOENT || err == ErrNotExist&#125;_, err := os.Open(\"/no/such/file\")fmt.Println(os.IsNotExist(err)) // \"true\" 通过类型断言查询接口我们不能对任意io.Writer类型的变量，假设它也拥有WriteString方法。但是我们可以定义一个只有这个方法的新接口并且使用类型断言来检测是否变量的动态类型满足这个新接口。123456789101112131415161718192021func writeString(w io.Writer, s string) (n int, err error) &#123; // 定义新接口 type stringWriter interface &#123; WriteString(string) (n int, err error) &#125; // 类型断言检测接口方法是否存在 if sw, ok := w.(stringWriter); ok &#123; return sw.WriteString(s) // avoid a copy &#125; return w.Write([]byte(s)) // allocate temporary copy&#125;func writeHeader(w io.Writer, contentType string) error &#123; if _, err := writeString(w, \"Content-Type: \"); err != nil &#123; return err &#125; if _, err := writeString(w, contentType); err != nil &#123; return err &#125; // ...&#125; 类型分支1234567891011121314151617func sqlQuote(x interface&#123;&#125;) string &#123; switch x := x.(type) &#123; case nil: return &quot;NULL&quot; case int, uint: return fmt.Sprintf(&quot;%d&quot;, x) // x has type interface&#123;&#125; here. case bool: if x &#123; return &quot;TRUE&quot; &#125; return &quot;FALSE&quot; case string: return sqlQuoteString(x) // (not shown) default: panic(fmt.Sprintf(&quot;unexpected type %T: %v&quot;, x, x)) &#125;&#125; 虽然x的类型是interface{}，但是我们把它认为是一个int，uint，bool，string，和nil值的discriminated union（可识别联合） 补充 设计新包时，并不一定要用到接口只有当有两个或两个以上的具体类型，且都必须以相同的方式进行处理时，才需要使用接口;"},{"title":"Go笔记之函数","permalink":"https://erikjiang.github.io/2018/08/19/GoFunction/","text":"这篇关于Go的函数特性相关内容，如递归函数、匿名函数、延迟函数、错误机制等 函数声明 函数声明的构成包括函数名、形式参数列表、返回值列表（可省略）以及函数体。 123func name(parameter-list) (result-list) &#123; body&#125; 函数声明时若包含返回值列表，则该函数必须以return语句结尾 函数标识符函数的类型被称为函数的标识符。如果两个函数形式参数列表和返回值列表中的变量类型一一对应，那么这两个函数被认为有相同的类型或标识符。 go函数没有默认参数及命名关键字参数 函数的形参是实参的拷贝实参通过值的方式传递，因此函数的形参是实参的拷贝，对形参进行修改不会影响实参；但是，如果实参包括引用类型，如指针，slice(切片)、map、function、channel等类型，实参可能会由于函数的间接引用被修改。 没有函数体的函数声明,表示该函数不是以Go实现,仅仅是定义了函数标识符 递归 函数可变调用栈大部分编程语言使用固定大小的函数调用栈，常见的大小从64KB到2MB不等；固定大小栈会限制递归的深度，当你用递归处理大量数据时，需要避免栈溢出；Go语言使用可变栈，栈的大小按需增加(初始时很小)。这使得我们使用递归时不必考虑溢出和安全问题； 多返回值 Go的垃圾回收机制垃圾回收机制会回收不被使用的内存，但是这不包括操作系统层面的资源，比如打开的文件、网络连接。所以操作系统层面的资源需要通过代码实现回收； 多返回之裸返回如果一个函数所有的返回值都有显式的变量名，那么该函数的return语句可以省略操作数。这称之为bare return。当一个函数有多处return语句以及许多返回值时，bare return 可以减少代码的重复，但是使得代码难以被理解。所以不宜过度使用； 1234567891011121314func CountWordsAndImages(url string) (words, images int, err error) &#123; resp, err := http.Get(url) if err != nil &#123; return &#125; doc, err := html.Parse(resp.Body) resp.Body.Close() if err != nil &#123; err = fmt.Errorf(\"parsing HTML: %s\", err) return &#125; words, images = countWordsAndImages(doc) return&#125; 错误 一个良好的程序永远不应该发生panic异常 函数的错误返回参数对于那些将运行失败看作是预期结果的函数，它们会返回一个额外的返回值，通常是最后一个，来传递错误信息。如果导致失败的原因只有一个，额外的返回值可以是一个布尔值，通常被命名为ok。 1value, ok := cache.Lookup(key) 错误处理策略 123456789101112&gt; 1. 传播错误函数中出现的失败，传播给父级函数，传播前编写错误信息时，要确保错误信息对问题细节的描述是详尽的；&gt; 2. 重新尝试失败的操作限制重试的时间间隔或重试的次数，防止无限制的重试;&gt; 3. 输出错误信息并结束程序这种策略只应在main中执行。对库函数而言，应仅向上传播错误，除非该错误意味着程序内部包含不一致性，即遇到了bug，才能在库函数中结束程序;&gt; 4. 仅输出错误信息，无需中断程序运行&gt; 5. 可以直接忽略掉错误 文件结尾错误（EOF） 函数值在Go中，函数被看作第一类值（first-class values）：函数像其他值一样，拥有类型，可以被赋值给其他变量，传递给函数，从函数返回。对函数值（function value）的调用类似函数调用; 函数类型的零值是nil。调用值为nil的函数值会引起panic错误 函数间不可比较，函数与nil可以比较，不能用函数值作为map的key; 匿名函数拥有函数名的函数只能在包级语法块中被声明，通过函数字面量（function literal），我们可绕过这一限制，在任何表达式中表示一个函数值。函数字面量的语法和函数声明相似，区别在于func关键字后没有函数名。函数值字面量是一种表达式，它的值被称为匿名函数（anonymous function）。1strings.Map(func(r rune) rune &#123; return r + 1 &#125;, \"hello world\") Go使用闭包（closures）技术实现函数值，Go程序员也把函数值叫做闭包。 可变参数参数数量可变的函数称为可变参数函数。 声明可变参数函数在声明可变参数函数时，需要在参数列表的最后一个参数类型之前加上省略符号“…”，这表示该函数会接收任意数量的该类型参数。 12345678// 可变参数vals被视为slice切片func sum(vals...int) int &#123; total := 0 for _, val := range vals &#123; total += val &#125; return total&#125; interface{}表示可以接收任意类型 Deferred延迟函数当执行到defer语句时，函数和参数表达式得到计算，但直到包含该defer语句的函数执行完毕时，defer后的函数才会被执行，不论包含defer语句的函数是通过return正常结束，还是由于panic导致的异常结束。你可以在一个函数中执行多条defer语句，它们的执行顺序与声明顺序相反; defer语句经常被用于处理成对的操作，如打开、关闭、连接、断开连接、加锁、释放锁。通过defer机制，不论函数逻辑多复杂，都能保证在任何执行路径下，资源被释放。释放资源的defer应该直接跟在请求资源的语句后; 调试复杂程序时，defer机制也常被用于记录何时进入和退出函数。 defer控制函数的出入口我们可以只通过一条defer语句控制函数的入口和所有的出口，甚至可以记录函数的运行时间，需要注意一点：不要忘记defer语句后的圆括号，否则本该在进入时执行的操作会在退出时执行，而本该在退出时执行的，永远不会被执行。 123456789101112func bigSlowOperation() &#123; defer trace(\"bigSlowOperation\")() // don't forget the extra parentheses // ...lots of work… time.Sleep(10 * time.Second) // simulate slow operation by sleeping&#125;func trace(msg string) func() &#123; start := time.Now() log.Printf(\"enter %s\", msg) return func() &#123; log.Printf(\"exit %s (%s)\", msg,time.Since(start)) &#125;&#125; 循环体中的defer在循环体中的defer语句需要特别注意，因为只有在函数执行完毕后，这些被延迟的函数才会执行。解决方法是将循环体中的defer语句移至另外一个函数。在每次循环时，调用这个函数; 12345678910111213for _, filename := range filenames &#123; if err := doFile(filename); err != nil &#123; return err &#125; // defer f.Close()&#125;func doFile(filename string) error &#123; f, err := os.Open(filename) if err != nil &#123; return err &#125; defer f.Close()&#125; Panic异常Go的类型系统会在编译时捕获很多错误，但有些错误只能在运行时检查，如数组访问越界、空指针引用等。这些运行时错误会引起painc异常。一般而言，当panic异常发生时，程序会中断运行，并立即执行在该goroutine中被延迟(defer)的函数。随后，程序崩溃并输出日志信息。 panic函数不是所有的panic异常都来自运行时，直接调用内置的panic函数也会引发panic异常；panic函数接受任何值作为参数。 panic用于严重错误,一般性问题应采用错误机制虽然Go的panic机制类似于其他语言的异常，但panic的适用场景有一些不同。由于panic会引起程序的崩溃，因此panic一般用于严重错误，如程序内部的逻辑不一致； 为方便诊断问题，runtime包允许输出堆栈信息 123456789func main() &#123; defer printStack() f(3)&#125;func printStack() &#123; var buf [4096]byte n := runtime.Stack(buf[:], false) os.Stdout.Write(buf[:n])&#125; Recover捕获异常通常来说，不应该对panic异常做任何处理，但有时，也许我们可以从异常中恢复，至少我们可以在程序崩溃前，做一些操作。 如果在deferred函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运行，但能正常返回。在未发生panic时调用recover，recover会返回nil。12345678func Parse(input string) (s *Syntax, err error) &#123; defer func() &#123; if p := recover(); p != nil &#123; err = fmt.Errorf(\"internal error: %v\", p) &#125; &#125;() // ...parser...&#125; 不加区分的恢复所有的panic异常，不是可取的做法； 不应该试图去恢复其他包引起的panic; 公有的API应该将函数的运行失败作为error返回，而不是panic。 安全的做法是有选择性的recover只恢复应该被恢复的panic异常，此外，这些异常所占的比例应该尽可能的低。为了标识某个panic是否应该被恢复，我们可以将panic value设置成特殊类型。在recover时对panic value进行检查，如果发现panic value是特殊类型，就将这个panic作为errror处理，如果不是，则按照正常的panic进行处理;1234567891011121314151617181920212223242526func soleTitle(doc *html.Node) (title string, err error) &#123; type bailout struct&#123;&#125; defer func() &#123; switch p := recover(); p &#123; case nil: // no panic case bailout&#123;&#125;: // \"expected\" panic err = fmt.Errorf(\"multiple title elements\") default: panic(p) // unexpected panic; carry on panicking &#125; &#125;() // Bail out of recursion if we find more than one nonempty title. forEachNode(doc, func(n *html.Node) &#123; if n.Type == html.ElementNode &amp;&amp; n.Data == \"title\" &amp;&amp; n.FirstChild != nil &#123; if title != \"\" &#123; panic(bailout&#123;&#125;) // multiple titleelements &#125; title = n.FirstChild.Data &#125; &#125;, nil) if title == \"\" &#123; return \"\", fmt.Errorf(\"no title element\") &#125; return title, nil&#125;"},{"title":"Go笔记之复合数据类型","permalink":"https://erikjiang.github.io/2018/08/11/GoCompoundTypes/","text":"这篇主要记录Go的复合数据类型，包含数组、切片、映射以及结构体等相关记录； Array 数组数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。因为数组的长度是固定的，因此在Go语言中很少直接使用数组。和数组对应的类型是Slice（切片），它是可以增长和收缩的动态序列，slice功能也更灵活； 数组的长度是数组类型的组成部分数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定； 123// [3]int 与 [4]int 为不同的两种数组类型q := [3]int&#123;1, 2, 3&#125;q = [4]int&#123;1, 2, 3, 4&#125; // compile error: cannot assign [4]int to [3]int 指定索引和对应值的数组初始化 123456789101112131415type Currency intconst ( USD Currency = iota // 美元 EUR // 欧元 GBP // 英镑 RMB // 人民币)symbol := [...]string&#123;USD: &quot;$&quot;, EUR: &quot;€&quot;, GBP: &quot;￡&quot;, RMB: &quot;￥&quot;&#125;fmt.Println(RMB, symbol[RMB]) // &quot;3 ￥&quot;// 含有100个元素的数组r,除索引99为-1外其余元素为0r := [...]int&#123;99: -1&#125; 数组作为函数传参的用法当调用函数时，函数的每个入参将作为函数内部的参数变量，所以函数参数变量接收的是一个复制的副本，数组作为参数传递时也是如此，这样的函数传参机制在遇到大的数组参数时往往是低效的，在其它编程语言中可能会隐式地将数组作为引用或指针对象传入被调用的函数，而在go中则是显式的传入数组指针； 虽然通过指针来传递数组参数是高效的，而且也允许在函数内部修改数组的值，但是数组依然是僵化的类型，因为数组的类型包含了僵化的长度信息。 除了像需要处理特定大小数组的特例外，数组依然很少用作函数参数；相反，我们一般使用slice来替代数组; Slice 切片Slice（切片）代表变长的序列，序列中每个元素都有相同的类型。一个slice类型一般写作[]T，其中T代表slice中元素的类型；slice的语法和数组很像，只是没有固定长度而已。一个slice是一个轻量级的数据结构，提供了访问数组子序列（或者全部）元素的功能，而且slice的底层确实引用一个数组对象。 slice的构成 12345678910111213141516171819# slice由 指针、长度、容量三个部分构成；&gt; 指针: 指向第一个slice元素对应的底层数组元素的地址（注：slice的第一个元素并不一定就是数组的第一个元素）&gt; 长度:对应slice中元素的数目;长度不能超过容量;内建函数len()测量长度；&gt; 容量:一般是从slice的开始位置到底层数据的结尾位置;内建函数cap()测量容量;---如果切片操作超出cap(s)的上限将导致一个panic异常，但是超出len(s)则是意味着扩展了slice； slice作为函数传参的使用因为slice值包含指向第一个slice元素的指针，因此向函数传递slice将允许在函数内部修改底层数组的元素。 slice 不支持比较唯一合法的比较操作是与nil比较，零值的slice等于nil; 1if summer == nil &#123;/* ... */&#125; 使用make创建指定元素类型、长度、容量的切片slice 12make([]T, len) // len num == cap nummake([]T, len, cap) // same as make([]T, cap)[:len] append 函数内置的append函数用于向slice追加元素； slice内存操作技巧一个slice可以用来模拟一个stack。最初给定的空slice对应一个空的stack，然后可以使用append函数将新的值压入stack： 1stack = append(stack, v) // push v stack的顶部位置对应slice的最后一个元素：1top := stack[len(stack)-1] // top of stack 通过收缩stack可以弹出栈顶的元素1stack = stack[:len(stack)-1] // pop Map 映射哈希表（映射）是一种巧妙并且实用的数据结构。它是一个无序的key/value对的集合，其中所有的key都是不同的，然后通过给定的key可以在常数时间复杂度内检索、更新或删除对应的value。 map的创建、修改、删除操作： 123456789101112131415161718192021// 通过make函数创建mapages := make(map[string]int)// 通过map字面量语法创建ages := map[string]int&#123; \"Sam\": 25, \"Lisa\": 32&#125;// 创建空mapages := map[string]int&#123;&#125;// 修改mapages[\"Sam\"] = 20// 通过delete()函数删除元素delete(ages, \"Sam\")// 累加：简短赋值语法ages[\"Sam\"] += 1ages[\"Sam\"]++ map 遍历的顺序是随机的Map的迭代顺序是不确定的，并且不同的哈希函数实现可能导致不同的遍历顺序。在实践中，遍历的顺序是随机的，每一次遍历的顺序都不相同。 向一个nil值的map存入元素将导致一个panic异常所以需要在map存数据前通过map字面量或者make()创建map 1ages[\"carol\"] = 21 // panic: assignment to entry in nil map 判断元素是否存在于map 12// 返回的ok用于判断元素是否存在if age, ok := ages[&quot;bob&quot;]; !ok &#123; /* ... */ &#125; map可作为集合set使用Go语言中并没有提供一个set类型，但是map中的key也是不相同的，可以用map实现类似set的功能。 Struct 结构体结构体是一种聚合的数据类型，是由零个或多个任意类型的值聚合成的实体。每个值称为结构体的成员。 通常一行对应一个结构体成员，成员的名字在前，类型在后，如果相邻的成员的类型相同，则可以合并成一行； 12345678type Employee struct &#123; ID int Name, Address string DoB time.Time Position string Salary int ManagerID int&#125; struct 结构体成员输入顺序是有意义的成员顺序改变即会将原结构体改变成为新的结构体； 结构体成员名字是以大写字母开头，则该成员可以导出；这是Go语言导出规则决定的。一个结构体可能同时包含导出和未导出的成员。 聚合类型struct及array的成员类型不能是其自身；一个命名为S的结构体类型将不能再包含S类型的成员，但是S类型的结构体可以包含*S指针类型的成员，这可以让我们创建递归的数据结构，比如链表和树结构等。 结构体类型的零值是每个成员都为零值； 如果结构体没有任何成员的话就是空结构体，写作struct{}。它的大小为0； 结构体字面值结构体有两种字面值语法 123456789// 1. 要求以结构体成员定义的顺序为每个结构体成员指定一个字面值；// 缺点：结构体成员有细微的调整就可能导致不能编译// 适用范围：只在定义结构体的包内部使用，或者是在较小的结构体中使用且这些结构体的成员排列比较规则；type Point struct&#123; X, Y int &#125;p := Point&#123;1, 2&#125;// 2. 以成员名字和相应的值来初始化，可以包含部分或全部的成员，成员被忽略将默认用零值；type Gif struct&#123; X, Y, W, H int &#125;anim := Gif&#123; X: 1, W: 2 &#125; 结构体作为函数传参较大的结构体建议使用结构体指针作为入参，考虑到效率；如果要在函数内部修改结构体成员的话，用指针传入是必须的； 结构体嵌入与匿名成员结构体嵌入机制让一个命名的结构体包含另一个结构体类型的成员; 123456789101112131415161718type Point struct &#123; X, Y int&#125;type Circle struct &#123; Center Point Radius int&#125;type Wheel struct &#123; Circle Circle Spokes int&#125;var w Wheelw.Circle.Center.X = 8w.Circle.Center.Y = 8w.Circle.Radius = 5w.Spokes = 20 但由于修改成员字面量需要逐层访问子成员太过于繁琐，故go语法提供了匿名成员嵌套的方式，这样之后再修改时，就可以通过简短形式访问匿名成员下的嵌套成员；1234567891011121314type Circle struct &#123; Point Radius int&#125;type Wheel struct &#123; Circle Spokes int&#125;var w Wheelw.X = 8 // equivalent to w.Circle.Point.X = 8w.Y = 8 // equivalent to w.Circle.Point.Y = 8w.Radius = 5 // equivalent to w.Circle.Radius = 5w.Spokes = 20 在声明了潜入匿名成员的结构体后，如何进行结构体成员字面量赋值？ 结构体字面值必须遵循形状类型声明时的结构 123456789w = Wheel&#123;Circle&#123;Point&#123;8, 8&#125;, 5&#125;, 20&#125;w = Wheel&#123; Circle: Circle&#123; Point: Point&#123;X: 8, Y: 8&#125;, Radius: 5, &#125;, Spokes: 20, // NOTE: trailing comma necessary here (and at Radius)&#125; 匿名方法集匿名成员并不要求是结构体类型；其实任何命名的类型都可以作为结构体的匿名成员，比如匿名类型的方法集，这个机制可以用于将一些有简单行为的对象组合成有复杂行为的对象。组合是Go语言中面向对象编程的核心； JSON 结构体slice转为JSON的过程叫编组，编组通过调用json.Marshal函数完成 1234567891011121314151617181920type Movie struct &#123; Title string Year int `json:&quot;released&quot;` Color bool `json:&quot;color,omitempty&quot;` Actors []string&#125;var movies = []Movie&#123; &#123;Title: &quot;Casablanca&quot;, Year: 1942, Color: false, Actors: []string&#123;&quot;Humphrey Bogart&quot;, &quot;Ingrid Bergman&quot;&#125;&#125;, &#123;Title: &quot;Cool Hand Luke&quot;, Year: 1967, Color: true, Actors: []string&#123;&quot;Paul Newman&quot;&#125;&#125;, &#123;Title: &quot;Bullitt&quot;, Year: 1968, Color: true, Actors: []string&#123;&quot;Steve McQueen&quot;, &quot;Jacqueline Bisset&quot;&#125;&#125;, // ...&#125;// 将结构体编码为jsondata, err := json.Marshal(movies)// 将结构体编码为json,并进行格式化整理data, err := json.MarshalIndent(movies, &quot;&quot;, &quot; &quot;) 只有导出的结构体成员才会被编码，所以编码的结构体应选择用大写字母开头的成员名称 结构体成员Tag结构体成员Tag是和在编译阶段关联到该成员的元信息字符串；结构体的成员Tag可以是任意的字符串面值，但是通常是一系列用空格分隔的key:”value”键值对序列；因为值中含有双引号字符，因此成员Tag一般用原生字符串面值的形式书写。 12Year int `json:&quot;released&quot;`Color bool `json:&quot;color,omitempty&quot;` json解码为结构体编码的逆操作是解码，对应将JSON数据解码为Go语言的数据结构，Go语言中一般叫unmarshaling，通过json.Unmarshal函数完成； 文本与HTML模板一个模板是一个字符串或一个文件，里面包含了一个或多个由双花括号包含的对象。 文本模板text/template HTML模板html/template"},{"title":"Go笔记之基础数据类型","permalink":"https://erikjiang.github.io/2018/08/05/GoBasicTypes/","text":"这篇主要是Go的基础数据类型，包含数值类型、布尔类型、字符串类型以及常量的相关记录； 数值类型：整数 整数类型符： 1234567891011121314151617// 1. 有符号整数类型int8、int16、int32、int64// 2. 无符号整数类型uint8、uint16、uint32、uint64// 3. 与CPU平台位数相关的整数类型 （32bit or 64bit）int、uint // 等同于int32、uint32 或者 int64、uint64// 4. unicode字符类型，用于表示Unicode码点rune // 等价于int32类型// 5. 原始数据数值类型byte // 等价于uint8类型// 6. 无符号整数类型指针uintptr 算术运算、逻辑运算和比较运算的二元运算符按照优先级递减的顺序排列 (从左往右从上到下递减)： 12345* / % &lt;&lt; &gt;&gt; &amp; &amp;^+ - | ^== != &lt; &lt;= &gt; &gt;=&amp;&amp;|| 二元比较运算符 123456== 等于!= 不等于&lt; 小于&lt;= 小于等于&gt; 大于&gt;= 大于等于 bit位操作运算符 123456&amp; 位运算 AND| 位运算 OR^ 位运算 XOR&amp;^ 位清空 (AND NOT)&lt;&lt; 左移&gt;&gt; 右移 数值类型：浮点数 两种精度浮点数 1float32 float64 用于浮点数的格式化输出符 123%g/G: 大指数（指数 &gt;= 6）使用 %e/%E，其它情况使用 %f/%F%e/E: 科学计数法（以 10 为底，小写 e/大写 E）%f/F: 普通小数格式（两者无区别） 无穷大与非数 123+Inf: 正无穷大-Inf: 负无穷大NaN: 非数 数值类型：复数 两种精度的复数类型： 1complex64 complex128 通过complex()函数构建复数 使用内建real()及imag()获取复数的实部及虚部； 布尔型布尔型值即true和false; 运算操作短路行为布尔值可以和&amp;&amp;（AND）和||（OR）操作符结合，并且有短路行为：如果运算符左边值已经可以确定整个布尔表达式的值，那么运算符右边的值将不再被求值； 字符串字符串是一个不可改变的字节序列,文本字符串通常被解释为采用UTF8编码的Unicode码点（rune）序列; 不变性因为字符串是不可修改的，因此尝试修改字符串内部数据的操作也是被禁止的；不变性意味着如果两个字符串共享相同的底层数据的话也是安全的，这使得复制任何长度的字符串代价是低廉的； 转义字符 123456789101112\\a 响铃\\b 退格\\f 换页\\n 换行\\r 回车\\t 制表符\\v 垂直制表符\\&apos; 单引号 (只用在 &apos;\\&apos;&apos; 形式的rune符号面值中)\\&quot; 双引号 (只用在 &quot;...&quot; 形式的字符串面值中)\\\\ 反斜杠\\xhh 一个十六进制的转义形式, 其中两个h表示十六进制数字（大写或小写都可以）\\ooo 一个八进制转义形式, 包含三个八进制的o数字（0到7）,但是不能超过\\377（译注：对应一个字节的范围，十进制为255） 原生字符串面值普通字符串面值使用&quot;&quot;来包含内容，而原生字符串面值反引号``代替双引号;在原生的字符串面值中，没有转义操作；全部的内容都是字面的意思，包含退格和换行，因此一个程序中的原生字符串面值可能跨越多行;唯一的特殊处理是会删除回车以保证在所有平台上的值都是一样的;原生字符串面值应用场景：正则表达式、HTML模板、JSON面值、命令行提示信息以及那些需要扩展到多行的场景。 UnicodeUnicode 收集了这个世界上所有的符号系统，包括重音符号和其它变音符号，制表符和回车符，还有很多神秘的符号，每个符号都分配一个唯一的Unicode码点，Unicode码点对应Go语言中的rune整数类型（译注：rune是int32等价类型）优点：涵盖语言符号广；缺点：浪费很多存储空间；unicode码点使用rune符文类型，且等价于int32类型，这种编码方式叫UTF-32或UCS-4, 每个码点占用32bit位，而计算机中大多数位ASCII码，每个ASCII字符仅需要8bit位或1字节大小，所以全部使用unicode码点会比较占用存储； UTF-8UTF-8是一个基于Unicode码点编码的可变长编码；是Unicode标准；UTF-8使用1（8bit）到4(32bit)个字节来表示Unicode码点;ASCII部分字符只使用1个字节，常用字符部分使用2或3个字节表示。每个符号编码后第一个字节的高端bit位用于表示编码总共有多少个字节。优点：按需使用存储，紧凑，完全兼容ASCII码 四种用于处理字符串的标准库包 1234567891011&gt; strings包：提供了诸如字符串的查询、替换、比较、截断、拆分和合并等功能&gt; bytes包：与strings包类似，但针对于 `[]byte` 类型, 使用bytes.Buffer处理更加灵活&gt; strconv包：提供了布尔型、整型数、浮点数和对应字符串的相互转换，还提供了双引号转义相关的转换&gt; unicode包：提供了IsDigit、IsLetter、IsUpper和IsLower等类似功能，它们用于给字符分类 常量常量表达式的值在编译期计算，而不是在运行期。每种常量的潜在类型都是基础类型：boolean、string或数字；常量值不可改变； 因为它们的值是在编译期就确定的，因此常量可以是构成类型的一部分，例如用于指定数组类型的长度； 批量生成常量如果是批量声明的常量，除了第一个外其它的常量右边的初始化表达式都可以省略，如果省略初始化表达式则表示使用前面常量的初始化表达式写法，对应的常量类型也一样的 12345678const ( a = 1 b c = 2 d)fmt.Println(a, b, c, d) // &quot;1 1 2 2&quot; iota常量生成器 常量声明可以使用iota常量生成器初始化，它用于生成一组以相似规则初始化的常量，但是不用每行都写一遍初始化表达式。在一个const声明语句中，在第一个声明的常量所在的行，iota将会被置为0，然后在每一个有常量声明的行加一；在其它编程语言中，这种类型一般被称为枚举类型。1234567891011type Weekday intconst ( Sunday Weekday = iota Monday Tuesday Wednesday Thursday Friday Saturday) 无类型常量许多常量并没有一个明确的基础类型。编译器为这些没有明确基础类型的数字常量提供比基础类型更高精度的算术运算；可以认为至少有256bit的运算精度。有六种未明确类型的常量类型，分别是无类型的布尔型、无类型的整数、无类型的字符、无类型的浮点数、无类型的复数、无类型的字符串;"},{"title":"Go笔记之程序结构","permalink":"https://erikjiang.github.io/2018/08/01/GoProgramStruct/","text":"这篇的内容关于Go的程序结构，包含命名、声明、变量、赋值、类型、作用域等 命名 25个关键字： 12345break default func interface selectcase defer go map structchan else goto package switchconst fallthrough if range typecontinue for import return var 30余个预定义名： 12345678910内建常量: true false iota nil内建类型: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error内建函数: make len cap new append copy close delete complex real imag panic recover 命名的作用域(程序实体的访问权限) 123模块级私有 -&gt; 命名于函数内：函数作用域，仅函数内部可以访问；包级私有 -&gt; 命名于函数外：包级作用域，可在当前包的所有文件中访问；公开 -&gt; 命名于函数外且首字母大写：可被导出，允许被外部包所访问； 命名要求简短且遵循驼峰命名原则 程序实体程序实体即：变量、常量、函数、结构体、接口的统称 标识符标识符即：声明或定义程序实体的变量名，或者可以说是程序实体的名字（即变量的命名） 限定符限定符即：代码包被导入使用时，调用包内方法的前缀名，格式[限定符].[包内方法名]限定符即导入代码包的包名称，并非包所在路径目录名称； 声明 四种声明关键字1234var // 变量const // 常量type // 类型func // 函数 变量 变量声明的一般语法： 123456// 常见的几种变量声明方式var 变量名 类型 = 表达式var 变量名 = 表达式 // 若没有类型，则通过表达式推导变量类型var 变量名 类型 // 若没有表达式，则将对应类型零值初始化给该变量var 变量1, 变量2, 变量3 类型 // 没有表达式的多变量声明var 变量1, 变量2, 变量3 = 表达式1, 表达式2, 表达式3 // 没有表达式的多变量声明 零值初始化机制： 12345671. 数值类变量零值为02. 布尔类变量零值为false3. 字符串类变量零值为\"\"（空字符串）4. 接口或引用类型（slice、指针、map、chan、func）变量对应零值为nil5. 数组或结构体等聚合类型变量的零值是该类型下元素具体类型的对应零值由于零值初始化机制使得Go语言不存在未初始化变量； 简短变量声明简短变量声明的类型确定是在编译期完成的，因此不会对程序的运行效率产生影响；简短变量声明语句用于声明和初始化局部变量，并且仅允许在函数内部使用;变量名 := 表达式，注：:=是一个变量声明语句Q: 关于变量声明，什么时候用var？什么时候使用简短变量声明？A: var往往用于需要显示指定变量类型的地方及变量稍后会重新赋值而初始值无关紧要的地方，而简短变量声明则不要求严格指定类型可接收根据表达式推导且声明当即要能表达式赋值的地方； 12345i := 100 // an intvar boiling float64 = 100 // a float64var names []stringvar err errorvar p Point 简短变量声明中的变量名必须保证在之前没有被声明过(单一变量时)，当简短变量声明多个变量时，变量中至少要保证有一个在之前未声明过，其他已声明的变量仅做赋值操作；简短变量声明语句只有对已经在同级词法域声明过的变量才和赋值操作语句等价，如果变量是在外部词法域声明的，那么简短变量声明语句将会在当前词法域重新声明一个新的变量。 变量的重声明 1234567&gt; 1. 由于变量的类型在其初始化时就已经确定了，所以对它再次声明时赋予的类型必须与其原本的类型相同，否则会产生编译错误;&gt; 2. 变量的重声明只可能发生在某一个代码块中。如果与当前的变量重名的是外层代码块中的变量，那么就会在当前代码块中声明一个新的变量，而非外层变量的重声明；&gt; 3. 变量的重声明只有在使用短变量声明时才会发生，否则也无法通过编译。如果要在此处声明全新的变量，那么就应该使用包含关键字var的声明语句，但是这时就不能与同一个代码块中的任何变量有重名了\u0010\u0010。&gt; 4. 被“声明并赋值”的变量必须是多个，并且其中至少有一个是新的变量。这时我们才可以说对其中的旧变量进行了重声明。 指针如果用 var x int 声明语句声明一个x变量，那么&amp;x表达式（取x变量的内存地址）将产生一个指向该整数变量的指针，指针对应的数据类型是int，指针被称之为“指向int类型的指针”。如果指针名字为p，那么可以说“p指针指向变量x”，或者说“p指针保存了x变量的内存地址”。同时p表达式对应p指针指向的变量的值。一般p表达式读取指针指向的变量的值，这里为int类型的值，同时因为p对应一个变量，所以该表达式也可以出现在赋值语句的左边，表示更新指针所指向的变量的值； 12345x := 1p := &amp;x // p, of type *int, points to xfmt.Println(*p) // \"1\"*p = 2 // equivalent to x = 2fmt.Println(x) // \"2\" new函数调用内建函数new也可以创建变量；表达式new(T)将创建一个T类型的匿名变量，初始化为T类型的零值，然后返回变量地址，返回的指针类型为*T。new并不是关键字，而是自定义函数，所以要注意new内建函数名称被重新定义为其他类型的情况； 1234p := new(int) // p, *int 类型, 指向匿名的 int 变量fmt.Println(*p) // \"0\"*p = 2 // 设置 int 匿名变量的值为 2fmt.Println(*p) // \"2\" 变量的生命周期变量的生命周期指的是在程序运行期间变量有效存在的时间段。对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。而相比之下，局部变量的生命周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。函数的参数变量和返回值变量都是局部变量。它们在函数每次被调用的时候创建。因为一个变量的有效周期只取决于是否可达，因此一个循环迭代内部的局部变量的生命周期可能超出其局部作用域。同时，局部变量可能在函数返回之后依然存在。 赋值 元组赋值当出现以下三种类型赋值语句时，右边的变量都会出现ok布尔变量值，用以判断赋值是否成功；123v, ok = m[key] // map lookup映射查询v, ok = x.(T) // type assertion类型断言v, ok = &lt;-ch // channel receive通道接收 和变量声明一样，我们可以用下划线空白标识符_来丢弃不需要的值； 可赋值性不管是隐式还是显式地赋值，在赋值语句左边的变量和右边最终的求到的值必须有相同的数据类型。更直白地说，只有右边的值对于左边的变量是可赋值的，赋值语句才是允许的。 类型 类型声明类型声明语句 type 类型名字 底层类型 一般出现在包级作用域，若类型名首字母大写则可被包外使用；例如：12345678910111213141516package tempconvimport \"fmt\"type Celsius float64 // 摄氏温度type Fahrenheit float64 // 华氏温度const ( AbsoluteZeroC Celsius = -273.15 // 绝对零度 FreezingC Celsius = 0 // 结冰点温度 BoilingC Celsius = 100 // 沸水温度)func CToF(c Celsius) Fahrenheit &#123; return Fahrenheit(c*9/5 + 32) &#125;func FToC(f Fahrenheit) Celsius &#123; return Celsius((f - 32) * 5 / 9) &#125; 其中以类型名(x)方式可以将传参x转换成与类型名一致的类型，但是前提是x参数的类型要与类型名的底层类型保持一致； 比较运算符==和&lt;也可以用来比较一个命名类型的变量和另一个有相同类型的变量，或有着相同底层类型的未命名类型的值之间做比较。但是如果两个值有着不同的类型，则不能直接进行比较：123456var c Celsiusvar f Fahrenheitfmt.Println(c == 0) // \"true\"fmt.Println(f &gt;= 0) // \"true\"fmt.Println(c == f) // compile error: type mismatchfmt.Println(c == Celsius(f)) // \"true\"! 包与文件 Go语言中的包和其他语言的库或模块的概念类似，目的都是为了支持模块化、封装、单独编译和代码重用。 每个包都对应一个独立的名字空间。 如果导入了一个包，但是又没有使用该包将被当作一个编译错误处理 包的初始化1func init() &#123; /* ... */ &#125; 作用域声明语句的作用域是指源代码中可以有效使用这个名字的范围; 作用域与生命周期声明语句的作用域对应的是一个源代码的文本区域；它是一个编译时的属性。变量的生命周期是指程序运行时变量存在的有效时间段，在此时间区域内它可以被程序的其他部分引用；是一个运行时的概念。 句法块句法块是由花括弧所包含的一系列语句，就像函数体或循环体花括弧包裹的内容一样;句法块内部声明的名字是无法被外部块访问。即块决定块内变量的作用域； 词法块词法块即是没有显式地使用花括号包裹的申明代码；全局词法块是对于全局源代码来说存在的一个整体词法块；其他的词法块如：包级词法块、for语句词法块、if语句词法块、switch语句词法块、switch分支词法块、select分支词法块、显式词法块（即花括弧内的句法块）；"},{"title":"长城下的Go Get, 被安排的明明白白","permalink":"https://erikjiang.github.io/2018/07/20/GoGetDownload/","text":"在nodejs开发中，包的安装，习惯了npm install的流畅，再不济也可以用cnpm代替； 但是，当进行golang开发时，由于golang.org及其他相关开发站点被GFW屏蔽，就不再那么好运了，而这里主要记录了一些方法，希望能让go get安装时显得不那么痛苦； 人肉搬运大法比如 golang.org/x/xxx 这种包，其实已被golang团队托管到了github; 需要做的就是在 github.com/golang 找到对应的源码并手动下载安装，下载的方式有多种，可以用git clone或者直接下载源码压缩文件，下载完的源码，需要放入GOPATH的src对应路径中； 比方说golang.org/x/crypto这个包，当源码下载完成后，需要创建目录$GOPATH/src/golang.org/x/，并将该源码包放入即完成； 当然有些网站的源码包需要翻墙才能下载，如果有代理或VPN则直接访问下载即可，但当没有这些工具时，该怎么办呢？其实还是有一些办法的，比如，使用第三方网站提供的下载服务，这里列出两个： gopm.io package golangtc package 代理安装大法首先，需要用到shadowsocks翻墙，但是由于shadowsocks基于socks5协议，而我们的go get基于http协议，故即使开启shadowsocks也依然无法安装下载golang包，所以还需要一个可以将socks5转为http的代理工具； 这里我们使用的是COW，即通过它完成socks5转为http； 安装方法见该github中REAMD.md说明； 安装完成后，需要在其配置$HOME/.cow/rc中修改如下：12listen = http://127.0.0.1:7777 #默认已存在proxy = socks5://127.0.0.1:1080 然后在~/.profile中添加环境变量：12export http_proxy=http://127.0.0.1:7777export https_proxy=http://127.0.0.1:7777 再然后启用环境变量：12$ source ~/.profile # 启用环境变量$ echo $http_proxy # 查看是否生效 最后，后台启动cow：1$ cow &amp; 一切配置完毕，就可以愉快的使用go get安装下载需要的依赖包了； 总结最后说了这么多，其实还是希望不要这么麻烦，但愿谷歌早日重返中国吧，至于李彦宏能不能赢就不知道了…."},{"title":"Go Micro 入门指南","permalink":"https://erikjiang.github.io/2018/07/05/GoMicroGuide/","text":"go-micro是一种可插拔的RPC分布式系统微服务开发框架，这一章主要介绍一下入门的相关内容。 服务发现依赖项首先 micro 这个框架需要且依赖于服务发现工具(service discovery)，框架默认的服务发现工具是 Consul ，同时框架的插拔机制也可确保能够切换到其他的服务发现工具上，如Etcd、NATS等，详见 micro plugins； 关于consul的安装与运行：12$ brew install consul # 安装$ consul agent -dev # 运行 gRPC的安装由于 micro 框架基于 gRPC，故需要安装相关依赖项，同时安装时确保 go version 版本在1.6以上； 安装gRPC1$ go get -u google.golang.org/grpc 安装Protocol Buffers v3安装用于生成gRPC服务代码的protoc编译器工具；最简单的方式是通过在protobuf工具文件列表中，以查看protoc-&lt;version&gt;-&lt;platform&gt;.zip文件名格式的方式找出符合你当前版本及系统平台的安装包，找到后下载解压放置于bin目录、修改环境变量PATH等； 同时安装protoc go插件1$ go get -u github.com/golang/protobuf/protoc-gen-go Micro相关安装框架包的安装1$ go get github.com/micro/go-micro protoc micro插件安装用于通过.proto文件生成.micro.go代码文件1$ go get github.com/micro/protoc-gen-micro micro工具包的安装1$ go get github.com/micro/micro micro 命令行工具可以提供诸如服务列表查看、服务详情查看、调用服务接口等功能； 实践一下首先，创建一个user.proto结构文件12345678910111213syntax = &quot;proto3&quot;;service User &#123; rpc Hello(Request) returns (Response) &#123;&#125;&#125;message Request &#123; string name = 1;&#125;message Response &#123; string msg = 1;&#125; 其次，通过proto文件编译生成代码1protoc --proto_path=$GOPATH/src:. --micro_out=. --go_out=. user.proto 此时，通过protoc工具对user.proto文件的编译，生成了两个go文件: user.micro.go user.pb.go 然后，分别编写服务端及客户端代码服务端 service.go123456789101112131415161718192021222324252627282930package mainimport ( \"context\" \"fmt\" proto \"github.com/ErikJiang/coin_exchange/ms-user/proto\" micro \"github.com/micro/go-micro\")type User struct&#123;&#125;func (u *User) Hello(ctx context.Context, req *proto.Request, res *proto.Response) error &#123; res.Msg = \"Hello \" + req.Name return nil&#125;func main() &#123; service := micro.NewService( micro.Name(\"user\"), ) service.Init() proto.RegisterUserHandler(service.Server(), new(User)) if err := service.Run(); err != nil &#123; fmt.Println(err) &#125;&#125; 客户端 client.go12345678910111213141516171819202122package mainimport ( \"context\" \"fmt\" proto \"github.com/ErikJiang/coin_exchange/ms-user/proto\" micro \"github.com/micro/go-micro\")func main() &#123; service := micro.NewService(micro.Name(\"user.client\")) service.Init() user := proto.NewUserService(\"user\", service.Client()) res, err := user.Hello(context.TODO(), &amp;proto.Request&#123;Name: \"World ^_^\"&#125;) if err != nil &#123; fmt.Println(err) &#125; fmt.Println(res.Msg)&#125; 最后，运行起来首先启动服务发现1$ consul agent -dev 可通过访问：http://127.0.0.1:8500/ui/ 查看服务； 启动服务端程序1$ go run service.go 此时刷新：http://127.0.0.1:8500/ui/ 页面会发现新服务user； 执行客户端程序，调用接口1$ go run client.go GRPC Quick Start micro install guide"},{"title":"NodeJS定时器与事件循环","permalink":"https://erikjiang.github.io/2018/06/09/TimerEventLoop/","text":"一直以为事件循环是一个独立于NodeJS主线程的单独工作线程，其实并非如此；事件循环是在NodeJS主线程中完成的； NodeJS提供四种定时器为了协调异步任务，使任务按时间顺序执行，NodeJS提供了四种定时器； setTimeout() setInterval() setImmediate() process.nextTick() 其中setTimeout、setInterval存在于JS语言标准，而setImmediate、process.nextTick则为NodeJS独有； 任务的执行顺序 同步任务先于异步任务执行 本轮循环先于次轮循环执行 本轮事件循环本轮循环中存在两个队列： nextTick队列(nextTickQueue) 微任务队列(microTaskQueue) 在本轮循环中process.nextTick是优先执行的，所有的nextTick定时器执行完成后，才轮到微任务队列定时器执行，常见的Promise就属于微任务定时器； 次轮事件循环 事件循环的六个阶段： timers阶段: 处理setTimeout、setInterval的回调函数 i/o callback阶段: 处理除setTimeout、setInterval、setImmediate、close callback以外的回调函数 idle、prepare阶段: 供LibUV内部使用 poll阶段: 等待未返回的IO事件，直到返回结果 check阶段: 处理setImmediate的回调函数 close callback阶段: 处理关闭事件的回调，如socket.on(‘close’) 参考：定时器详解 op0=>operation: Timer op1=>operation: I/O callbacks op2=>operation: Idle, Prepare op3=>operation: Poll op4=>operation: Check op5=>operation: Close callbacks op0->op1->op1->op2->op3->op4->op5{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);"},{"title":"不算可怜","permalink":"https://erikjiang.github.io/2018/05/31/YouAreNotPitiful/","text":"日出東海落西山，愁也一天，喜也一天；遇事不鑽牛角尖，人也舒坦，心也舒坦； 樂觀點，哥們，至少站在我旁邊，你還不算可憐 心量放開，接受自己的不足，認可他人的長處 你又不差，不要學我這樣，對消極上癮 日子那麼長，別總待在舒適區，活在自己的世界 拜託，向著未知的方向，探出頭，邁一步 最後希望你，還能像個孩子一樣，開心點"},{"title":"Go问题集萃","permalink":"https://erikjiang.github.io/2018/04/25/GoQuestion/","text":"语言相关谈谈 进程、线程、协程，以及它们出现所要解决的问题，及相关技术的演进历程？ C10K问题 c10k主要指单机并发连接在1万的情况下，硬件性能足够，但仍然无法正常提供服务的问题c10k是计算机从PC时代到互联网时代过程中由于并发连接量剧增做产生的问题；c10k解决的关键在于，降低cpu等核心资源的消耗 IO多路复用的种类： select: 存在句柄上线、重复初始化、轮询低效问题 poll：解决了句柄上线、重复初始化问题，但依然需要轮询IO句柄 epoll: 采用仅排查当前状态变化的句柄，无需逐个轮询句柄的方式，效率提升, 但不能跨平台，不同平台的库不同 epoll\\kqueque\\IOCP； nginx, libevent, libev, nodejs底层的libuv都是基于epoll; Epoll两种触发方式： 水平触发：当被监控的文件描述符（IO文件句柄）有可读写事件发生，epoll_wait()会通知处理程序去读写，若本次未读写完，则在下次epoll_wait()时会接着上次没读写完的位置继续读写；与select\\poll类似；（可能会有部分不需要读写的文件描述符被epoll_wait(),效率要低些） 边缘触发：当被监控的文件描述符（IO文件句柄）有可读写事件发生，epoll_wait()会通知处理程序去读写，若本次未读写完，则在下次epoll_wait(),并不会通知继续处理，只会在文件描述符（IO文件句柄）下次再有可读写事件时才会通知执行后续读写操作； 发展三阶段：1). 一个进程/线程处理一个连接2). 一个进程/线程处理多条连接（IO多路复用）3). 用户态异步协程处理连接 进程：是系统进行资源分配和调度的独立单元，拥有独立的内存空间，不同进程通过IPC进行通信，进程较重，上下文进程间切换（栈、寄存器、虚拟存储、文件句柄）开销大，但稳定性好； 线程：是进程内部的实体，是CPU调度和分派的基本执行单元，它与同属一个进程内的其他线程共享进程所拥有的资源，线程间通信主要通过共享内存，相比进程资源开销小，上下文切换快，但较不稳定； 协程：是一种用户态的轻量级线程，调度完全由用户程序控制，没有内核开销，效率快； 进程与线程： 地址空间：线程是进程内独立的执行单元，一个进程至少有一个线程，进程有独立的地址空间， 且进程内的线程共享进程的地址空间； 资源共享：进程内的资源与其内部的线程共享； 调度处理：线程是处理器CPU调度的基本单位，进程不是； 都可以并发执行； 线程与协程： 一个线程或进程可以拥有多个协程； 线程进程是同步机制，协程是异步机制； https://www.cnblogs.com/lxmhhy/p/6041001.html channel 要记得 close，但 close 时候需要注意些什么？ channel 不能被重复close,否则会panic; 已经关闭的 channel 不能向其发送数据，否则painc; 从已关闭的 channel 中读取数据，如果是无缓冲的，读出的是零值，如果有缓冲且有数据，可以继续读； 通过i, ok := &lt;- ch的 ok 获取channel是否已关闭； 应该只在唯一或最后唯一剩下的生产者发送端协程中关闭channel； channel 是通过注册相关 goroutine id 实现消息通知，具体阐述？// todo slice 相关问题， slice 与数组的区别，slice的底层结构？array 属于值类型，长度固定；slice 属于引用类型，长度不定，slice的底层引用了一个数组对象； 声明时，array要指明长度，slice则不需要；作为函数参数时，数组传递的时其副本，而slice传递的是指针； 谈谈 互斥锁，读写锁，死锁，及其数据竞争的概念？互斥锁: 是并发程序对共享资源进行唯一性访问控制的主要手段;读写锁: 即是并发时针对于读写操作的互斥锁；多个写操作之间是互斥的；写操作与读操作之间是互斥的；多个读操作之间不存在互斥；死锁: 并发时所有协程都彼此等待的状态（goroutine间存在无缓冲channel只读不写或只写不读的情况、加锁但没解锁的情况）数据竞争：并发情况下存在的多个协程读写相同数据的情况，至少有一个协程是写操作；若多个协程都是读操作，则不存在数据竞争； 什么是channel，为什么它可以做到线程安全？是golang协程间通信的方式，channel本质是通过内存队列，一次只处理一个数据，从而实现了访问的序列化，避免了数据竞争，从而并发安全； 如何用channel实现一个令牌桶// todo 如何写单元测试和基准测试 使用 gotest 包，文件名格式 *_test.go 导入 import testing，单元测试函数命名格式：func Test*(t *testing.T){} 导入 import testing，基准测试函数命名格式：func Benchmark*(b *testing.B){} go test执行单元测试，go test -test.bench=.*执行所有基准测试； go 及 goroutine 的调度是怎样的（为什么并发性能好），抢占式goroutine调用什么意思？1). 线程池的缺陷线程池中的woker线程获取任务队列中的任务并执行，但若任务中发生系统调用，该worker进程将处于阻塞状态，从而任务队列任务堆积，解决方法是增加线程数,但会使得多线程争抢CPU,从而使得处理能力下降； 2). G-P-M调度模型 G: Goroutine Go协程 M: Machine 系统工作线程 P: Processor 上下文调度器调度策列： 队列轮询：P维护着一个G的队列，P周期性将G队列中的G调度到M中执行，执行一会儿，保存上下文放入队尾，再执行队列中下一个G; 系统调用: 当M在执行G1的过程中遇到系统调用时，M会释放P以及P所维护的G队列，此时空闲的M1获取到P及剩余的G队列继续执行，M1接替M的剩余工作 工作量窃取：通常当两个P中的G队列不均衡时，空闲的P会从忙碌的P的G队列中窃取部分G来执行； 抢占式goroutine调用：如果一个goroutine运行时间过长或则长时间阻塞于系统调用，它就会被剥夺运行权； https://segmentfault.com/a/1190000015352983https://my.oschina.net/renhc/blog/2221426 golang 的内存回收是如何做到的？go 的 GC 采用了mark and sweep 标记清除模式，从根变量扫描遍历所有被引用的对象并进行标记，将没有标记的对象回收；缺点：每次执行垃圾回收时会暂停所有正常运行的代码，系统响应能力降低，但后期加入了三色标记法与写屏障，又使得性能缓解； 三色标记法： 起初所有对象都是白色。 从根出发扫描所有可达对象，标记为灰色，放入待处理队列。 从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。 重复上一步，直到灰色对象队列为空。此时白色对象即为垃圾，进行回收。 http://legendtkl.com/2017/04/28/golang-gc/ cap和len分别获取的是什么, cap函数适用的类型有哪些？len:array: 指数组成员的数量slice: 指切片成员的数量channel: 指通道缓冲区未读的成员数量 cap:array: 指数组成员的数量slice: 指切片当前最大容量channel: 指通道缓冲区的容量 https://studygolang.com/articles/8519 go build时，tag 中 netgo，netcgo有什么区别？netgo 使用 /opt/go/pkg/tool/linux_amd64/compile 进行编译；netcgo 使用 gcc 编译； 什么是interface？interface是一种抽象类型，是一组方法(method)的集合，是鸭式（duck-type）编程的一种体现; 但凡不相关的对象实现了interface中的所有方法，那这些不相关的对象都可以给interface变量赋值，从而完成一些共性的行为操作； 使用go语言，编写并行计算的快速排序算法// todo 数据竞争（Data Race）问题怎么解决？能不能不加锁解决这个问题？数据竞争指并发多个协程对同一共享资源进行读写操作时，产生的数据错乱问题；解决的关键在于同一时间仅允许一个协程对共享资源进行读写操作； 通过 go build -race 命令，生成可执行文件并运行，即可检测资源竞争并输出检测报告； 加锁的方式处理：sync/atomic、sync.Mutex https://software.intel.com/en-us/blogs/2013/01/06/benign-data-races-what-could-possibly-go-wrong 常用的 golang 库有哪些？用到其中哪些方法？ viper 配置管理 zerolog 可配置日志管理 redigo redis客户端操作 gin web服务搭建 cron 定时任务设置 jwt-go 请求认证相关 swaggo restful api 文档化 crypto 常用的加解密算法库 fmt 信息格式化及打印信息 errors 设置自定义错误信息 net/http 服务相关 strings|strconv 字符串处理 time 时间相关 go for range 的坑？for range 遍历的出的是slice/array中每个元素的副本，并非引用而将这些副本值传给新变量时，我们最终结果的每一项都存的时新变量的指针，那么所有新变量的指针都同时指向遍历到的最后一个值，故多个结果值一样； https://studygolang.com/articles/9701 Golang 的 GC 触发时机是什么 内存阈值触发：当前分配内存是否大于上次GC后内存的2倍 每两分钟定时主动触发：没2min触发一次 golang中的引用类型与值类型分别有哪些？值类型：int、float、bool、string； 引用类型：slice、map、channel； 两者区别：拷贝操作和函数传参 go vet 的用途用于检查golang源码中的静态错误； select 的用途select是一种通信开关，用来监听协程通过channel发送的信息 每次执行select，都会只执行其中1个case或者执行default语句。 当没有case或者default可以执行时，select则阻塞，等待直到有1个case可以执行。 当有多个case可以执行时，则随机选择1个case执行。 case后面跟的必须是读或者写通道的操作，否则编译出错。 go 的 new 与 make 的区别？new 和 make 都是用于分配内存；make 仅适用于slice、map、channel引用类型的非零值初始化，且会返回类型本身；new 用于类型的零值初始化，且会返回类型的指针； 怎么实现协程完美退出使用 sync.WaitGroup; 创建一个 waitgroup 实例wg 在每个协程启动时调用 wg.Add(1)，或者创建n个协程前调用wg.Add(n) 当协程任务完成后，调用wg.Done() 在等待所有协程的地方调用wg.Wait() gRPC是什么？RPC的优点？gRPC是谷歌开源的远程过程调用（RPC）框架，它采用 protocol buffers 作为接口描述语言（Interface Description Language）,protobuf 也可以看做 gRPC 底层消息交换的格式； 通过编写 .proto 扩展名的protobuf文件，再使用 protoc 及相关插件、选择指定语言、编译该文件，就可以生成的对应的客户端、服务端代码，我们只需要实现C/S两端代码中定义的方法，即可实现通信； 相较于REST基于http1.1，RPC基于 tcp 或 http2 较多，头部信息较少，传输效率更高，适合后台服务间通信；但较于REST学习开发成本略大； 设计模式： 单例模式：定义了一个单一的类，并且该类负责创建自己的对象，同时保证只有一个对象被创建 工厂模式:定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行 观察者模式:定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新 模板方法模式:定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤 桥接模式:将抽象部分与实现部分分离，使它们都可以独立的变化 容器相关问题如何给容器添加映射端口?提交一个运行中的容器为镜像:1docker commit containerid foo/live 运行镜像并添加端口:1docker run -d -p 8000:80 foo/live /bin/bash 如何查看 docker 镜像的环境变量?1docker run &lt;image id&gt; env Linux 操作相关如何后台启动一个服务？ 命令：conmmand &amp;，通过增加一个（&amp;）符号，将应用程序在后台启动，但此操作在关闭终端时服务会停止； 命令：nohup conmmand &amp;， 该命令可以在你退出帐户之后继续运行相应的进程，nohup( no hang up)； bg: 将一个在后台暂停的命令，变成继续执行; fg: 将后台中的命令调至前台继续运行; jobs: 查看当前有多少在后台运行的命令; ctrl+z: 将一个正在前台执行的命令放到后台，并且暂停; 说说孤儿进程、僵尸进程孤儿进程与僵尸进程都是指子进程的状态； 孤儿进程：当父进程退出时，并未通知子进程结束，使得子进程仍然在运行，这样的子进程就是孤儿进程，它将被系统init进程接管，当孤儿进程生命周期结束，init进程将调用wait()处理孤儿进程； 僵尸进程：当子进程退出时，父进程并未调用wait()释放子进程的进程描述符(进程号、退出状态、运行时间等)，此时的子进程状态为僵尸状态，进程即僵尸进程； 任何子进程在exit()后，并非立即释放所有该子进程的资源信息，而是会留下一个僵尸状态的数据结构，等待父进程处理；由于系统所能使用的进程号是有限的，如果大量产生僵尸进程占用所有进程号资源，将使得新进程无法创建，危害极大；故孤儿进程无危害，僵尸进程存在危害； 解决僵尸进程的方法：即 kill 掉僵尸子进程的父进程，使得僵尸进程被系统init进程接管，这样init进程会使用wait()释放掉进程号资源； https://blog.csdn.net/morgerton/article/details/69388694 数据库相关如何做 SQL 优化使用 explain select ... 分析：id执行顺序、key用到的索引、extra额外改进信息 优化建议： 尽量不要在复杂的查询中使用模糊匹配 LIKE %param% 避免在索引字段上进行计算操作、使用not、&lt;&gt;、!=、IS NULL、NOT NULL判断、进行数据类型转换、使用函数、使用空值，这些都会使得索引失效； 避免SELECT * FROM t 使用 JOIN 代替子查询 适当添加冗余字段来减少连表查询 在 WHERE 中，尽量避免对索引字段进行计算操作 数据库事务相关ACID 特性 原子性 automatic：事务是原子工作单位，要么全部执行、要么全部不执行 一致性 consistency：数据库通过事务完成状态转变 隔离性 isolation：事务提交前，对数据的影响是不可见的 持久性 duration：事务完成后，对数据的影响是持久的 4 种隔离级别隔离级别由低到高： read uncommitted(读未提交)，本事务可读取其他事务未提交的数据，隐患：脏读：即可以随意读取其他事务未提交的数据； read committed(读已提交)，本事务可读取其他事务已提交的数据，隐患：非重复读：即在事务中两次相同读取操作,结果不同； repeatable read(可重读)，本事务查询多次相同查询，结果必须相同，mysql默认级别，隐患：幻读：即A事务提交的新增数据，在B事务中查不到，此时B事务新增同样数据会报数据已存在； serializeable(串行化)，事务间按串行方式执行，隐患：写挂起：即事务执行过程，若有其他写操作访问相同数据，则会挂起等待； https://www.jianshu.com/p/4e3edbedb9a8 3 种实现隔离的锁 共享锁 S 锁：只读 SELECT 操作，锁定共享资源，阻止其他用户写数据 更新锁 U 锁：阻止其他用户更新数据 独占锁 X 锁：一次只能有一个独占锁占用一个资源，阻止添加其他所有锁。有效防止脏读 SQL查询成绩表的各个科目中成绩最高与最低的学生首先查询成绩表中各个科目最高与最低分12&gt; select subject, max(score) from grade group by subject;&gt; select subject, min(score) from grade group by subject; 然后查询最高及最低分对应学生12&gt; select b.* from (select subject, max(score) m from grade group by subject) t, grade b where t.subject=b.subject and t.m=b.score;&gt; select b.* from (select subject, min(score) m from grade group by subject) t, grade b where t.subject=b.subject and t.m=b.score; 最后使用 UNION 合并结果集12&gt; select b.*, \"最高分\" from (select subject, max(score) m from grade group by subject) t, grade b where t.subject=b.subject and t.m=b.score unionselect b.*, \"最低分\" from (select subject, min(score) m from grade group by subject) t, grade b where t.subject=b.subject and t.m=b.score; WEB 通信相关当打开浏览器输入url到打开网页，这当中发生了什么？ URL输入 DNS解析：浏览器缓存-&gt;操作系统缓存-&gt;本地host文件-&gt;路由器缓存-&gt;ISP DNS缓存-&gt;顶级DNS服务器/根DNS服务器，获取到IP地址 TCP连接：通过ip与服务器进行三次握手建立连接 发送HTTP请求：请求包括请求方法、请求头、请求体 服务器处理请求：服务器通过请求路径找到对应路由，获取请求中的参数 服务器响应请求：根据请求参数进行业务逻辑处理，可能包含对数据库、第三方服务接口、缓存数据的访问，处理完后返回响应数据； 浏览器解析渲染页面：返回的数据经过前端HTML CSS 经行可视化渲染； 连接结束 https://juejin.im/post/5b148a2ce51d4506965908d2 什么是 restful api, 有哪些方法，分别代表什么？REST 是一种接口定义的规范，它要求服务提供标准可见的接口；1). 通过 URI 标识资源2). 统一使用 POST GET DELTE PATCH PUT 来定义操作资源的方式；3). 数据组织形式采用标准通用格式 JSON 或 XML4). 使用 HTTP 作为组件间交流协议 TCP 和 UDP 有什么区别? 描述一下 TCP 三次握手、四次挥手的过程?OSI七层模型： 应用层：FTP\\HTTP\\SMTP\\DNS\\Telnet … 表现层: 数据格式、代码转换、数据加密 会话层：建立和解除与别的节点的连接 传输层：提供端到端接口 TCP\\UDP 网络层: 为数据包选择路由 IP\\ICMP … 数据链路层: 传输有地址的帧及错误检测 物理层：以二进制形式在物理媒体上传输数据 TCP传输控制协议，UDP用户数据报协议 基于连接与无连接； 对系统资源的要求（TCP较多，UDP少）； UDP程序结构较简单； 字节流模式与数据报模式 ； TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证。 TCP仅支持点对点通信，UDP可以支持一对一、一对多、多对一、多对多； seq: 字节流序列号（Sequence Number），表示报文中的第一个数据字节在发送的数据流中的序号，防止网络报乱序； ack: 确认序列号（Acknowledgment Number），通常是上一次收到的字节流序列号加一（seqNum + 1），防止网络丢包； 三次握手： 第一次，客户端发送SYN连接请求报文，设置SYN为1，seqNum为x，此时客户端处于 SYN_SEND（同步发送） 状态； 第二次，服务端收到SYN报文，需要对其确认， 设置SYN为1，ackNum为x+1、seqNum为y 的 SYN-ACK 报文给客户端，此时服务端处于 SYN_RECV（同步接收） 状态 第三次，客户端接收到 SYN-ACK 报文，向服务端发送 ackNum为y+1 的ACK报文，此时客户端/服务端都进入 ESTABLISHED（确认）状态； 为什么握手需要三次？在信道不可靠的情况下要保证数据传输可靠，理论上需要三次通信； 第一次和第二次确保了 Server 能收到 Client 信息且 Server 能做出正确应答； 第二次和第三次确保了 Client 能收到 Server 信息且 Client 能做出正确应答； 四次挥手： 第一次，客户端发送 FIN 报文，设置 seqNum 为 x ，ackNum 为 y ， 此时客户端进入 FIN_WAIT_1 (终止等待1) 状态； 第二次, 服务端收到 FIN 报文，向客户端发送 ackNum 为 x+1 的 ACK 报文，此时服务端进入 CLOSE_WAIT（关闭等待）状态，客户端收到ACK报文将进入FIN_WAIT_2状态； 第三次，服务端判断是否还有数据需要发送，若有，则先将剩余数据发送完毕，然后向客户端发送 FIN 报文（seqNum为y），请求关闭连接，同时服务端进入 LAST_ACK(最终确认)状态； 第四次，客户端收到 FIN 报文，向服务端发送 ackNum 为 y+1 的 ACK 报文，然后客户端进入 TIME_WAIT 状态，服务端此时收到FIN报文直接关闭连接，客户端等待一会儿依然没有收到服务端回复，则客户端也关闭连接； https://juejin.im/post/5b189ca0f265da6e1e1adcbf HTTPS 和 HTTP 有什么区别? 状态码有哪些？301和302有什么区别? 401和403有什么区别? 502和504有什么区别?HTTP 传输过程是明文，HTTPS 在 HTTP 基础上增加了 SSL 安全层，使得传输的数据被加密成密文，保证传输数据安全； SSL 握手协议： 第一, 客户端向服务端发送协议版本号、一个客户端生成的随机数A、客户端支持的加密方式三个信息； 第二, 服务端收到客户端信息后，将确认的双方使用的加密方式、数字证书、一个服务端生成的随机数B发送给客户端； 第三, 客户端确认数字证书有效，然后生成新的随机数C，并使用数字证书中公钥加密该随机数C，发送给服务器； 第四, 服务端使用私钥对随机数C密文进行解密，至此客户端/服务端都拥有了三个随机数（clientRandom、serverRandom、premasterSecret）； 第五, 客户端根据约定的加密方式，使用之前三个随机数生成 session 密钥，用于加密之后的通信数据； 状态码： 1XX 信息提示 2XX 请求成功 3XX 重定向相关 4XX 客户端请求错误 5XX/6XX 服务端错误 301：是永久重定向（表示旧地址资源被永久移除），302：是临时重定向（表示旧地址仍在，只是临时跳转）； 401：是在认证过程中，认证信息缺失或认证有误而导致的未被授权，403：则是在认证完成以后，在访问某个特定资源时，由于没有权限而被禁止； 502：通过网关或代理服务执行请求时，应用服务返回了一个无效的响应；504：通过网关或代理服务执行请求时，应用服务响应超时； http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html CSRF 跨站请求伪造：攻击者知道所有参数、构造合法请求伪造成其他用户，发起请求：如拼接恶意链接给其他用户点击。防范： 关键操作限制 POST 合适的使用验证码 添加 token：发起请求时添加附加的 token 参数，值随机。服务端做验证 header refer：外部来源可拒绝请求 XSS 跨站脚本攻击：在客户端不知情的情况下运行 js 脚本。防范： 过滤和转义用户的输入内容：htmlspecialchars() 限制参数类型 文件上传做大小、类型限制 如何实现一个URL短链接服务？将长短链接映射关系存入数据库，访问短链时，查询获取到对应数据库中的长链，然后转发请求； https://www.zhihu.com/question/29270034/answer/46446911 简述 cookie/session 验证过程 以及 token 验证过程，token机制能够代替session机制吗？由于http协议是无状态的，服务器需要记录用户的状态，所以cookie和session都是用来保持状态的方案； cookie/session 验证过程：1). 用户在登录页面向服务器发起用户名/密码认证请求；2). 服务端认证成功，将该认证用户相关信息保存session中(存储于服务端的文件、数据库、redis中任其一)，并为该session生成一个唯一的sessionID；3). 在认证请求的响应头中，会将该sessionID及有效期保存于set-cookie键值中；4). 客户端收到响应，从set-cookie中获取cookie并保存浏览器本地，而在接下来的每次请求中，都会将cookie带入请求头一并发出;5). 服务端从请求头的cookie中获取到sessionID，并查找是否存在相关session，若存在则通过验证； 如果浏览器禁用cookie，则可以在URL添加querystring参数?sid=&lt;seesion id&gt; token 验证过程：1). 用户在登录页面向服务器发起用户名/密码认证请求；2). 服务端认证成功，将用户的少量非重要信息生成token（服务端不存储），并在响应数据中带入token返回给客户端；4). 客户端收到响应，获取token并保存在浏览器本地，之后在每次请求中，都会将token设置到请求头的Authorization中;5). 服务端从请求头的Authorization中获取到token，通过jwt验证token是否有效，若有效则通过验证； json web token机制不能取代session机制:由于jwt的Token只能被动等待过期，不能直接设置失效，不能延续有效期；且jwt在服务端无状态存储的特性，使得在像如下场景无法替代session机制： 比如多端登录情况下，只允许最后一个token有效 修改密码后，希望当前token立即失效 页面访问刷新时，token有效期延长 https://www.jianshu.com/p/bd1be47a16c1https://segmentfault.com/a/1190000016236765https://stayhungrystayfoolish.github.io/Token/ 使用golang net/http 实现一个简易的http服务12345678910111213141516171819202122232425package mainimport ( \"fmt\" \"log\" \"net/http\")// w表示response对象，返回给客户端的内容都在对象里处理// r表示客户端请求对象，包含了请求头，请求参数等等func index(w http.ResponseWriter, r *http.Request) &#123; // 往w里写入内容，就会在浏览器里输出 fmt.Fprintf(w, \"Hello golang http!\")&#125;func main() &#123; // 设置路由，如果访问/，则调用index方法 http.HandleFunc(\"/\", index) // 启动web服务，监听9090端口 err := http.ListenAndServe(\":9090\", nil) if err != nil &#123; log.Fatal(\"ListenAndServe: \", err) &#125;&#125; 使用 golang request/response 常用的方法？// todo 编程题题目1有四个线程1、2、3、4。线程1的功能就是输出1，线程2的功能就是输出2，以此类推…现在有四个文件ABCD。初始都为空。现要让四个文件呈如下格式：A：1 2 3 4 1 2….B：2 3 4 1 2 3….C：3 4 1 2 3 4….D：4 1 2 3 4 1…. 12345678910111213141516171819202122232425262728package mainimport ( \"bytes\" \"fmt\")func main() &#123; chs := make([]chan int, 4) for i := 0; i &lt; len(chs); i++ &#123; chs[i] = make(chan int) go func(i int) &#123; for &#123; chs[i] &lt;- i + 1 &#125; &#125;(i) &#125; f := make([]bytes.Buffer, len(chs)) for i := 0; i &lt; 10; i++ &#123; for j := 0; j &lt; len(f); j++ &#123; fmt.Fprintf(&amp;f[j], \"%d \", &lt;-chs[(i+j)%len(chs)]) &#125; &#125; for i := 0; i &lt; len(f); i++ &#123; fmt.Printf(\"%d: %s\\n\", i, f[i].String()) &#125;&#125; 题目2实现strconv库中的atoi与itoa方法 1). golang版本 atoi 实现1234567891011121314151617181920212223func MyAtoi(s string) int &#123; s0 := s if s[0] == '-' || s[0] == '+' &#123; s = s[1:] fmt.Printf(\"string: %s\\n\", s) if len(s) &lt; 1 &#123; fmt.Printf(\"err: param isn't number format\") &#125; &#125; n := 0 for _, ch := range []byte(s) &#123; ch -= '0' if ch &gt; 9 &#123; fmt.Printf(\"err: param isn't number format\") &#125; fmt.Printf(\"ch item: %d\\n\", ch) n = n*10 + int(ch) &#125; if s0[0] == '-' &#123; n = -n &#125; return n&#125; 2). c 版本 atoi 实现12345678910111213141516171819202122232425262728#include &lt;stdio.h&gt;#include &lt;ctype.h&gt; /* atoi : convert s to integer; version 2*/int atoi(char s[])&#123; int i, n, sign; for (i = 0; isspace(s[i]); i ++) &#123; // do nothing &#125; sign = (s[i] == '-') ? -1 : 1; if (s[i] == '-' || s[i] == '+') &#123; i ++; &#125; for ( n = 0; isdigit(s[i]); i ++) &#123; n = 10 * n + (s[i] - '0'); &#125; return sign * n;&#125;int main()&#123; char a[] = \"1132\"; printf(\"%d\\n\", atoi(a)); return 0;&#125; 3). golang 版本 atoi 实现123456789101112131415161718func MyItoa(n int) string &#123; b := []byte&#123;&#125; for &#123; b = append(b, uint8(n%10)+'0') n = int(n / 10) if n == 0 &#123; break &#125; &#125; reverse(b) return string(b)&#125;func reverse(b []byte) &#123; for i, j := 0, len(b)-1; i&lt;j; i, j = i+1, j-1 &#123; b[i], b[j] = b[j], b[i] &#125;&#125; 4). c 语言版本 itoa 实现12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;stdio.h&gt;#include &lt;ctype.h&gt;#include &lt;string.h&gt; void reverse(char s[]);/*itoa : convert n to characters in s*/void itoa(int n, char s[])&#123; int i, sign; if((sign = n) &lt; 0)&#123;//记录符号 n = -n; //使n变为正数 &#125; i = 0; do &#123;//生成的字符是反序的 s[i ++] = n % 10 + '0'; &#125; while((n /= 10) &gt; 0); if (sign &lt; 0) &#123; s[i ++] = '-'; &#125; s[i] = '\\0'; reverse(s);&#125; void reverse(char s[])&#123; int c, i, j; for (i = 0, j = strlen(s) - 1; i &lt; j ;i ++, j --) &#123; c = s[i]; s[i] = s[j]; s[j] = c; &#125;&#125; int main()&#123; int n = 235; char s[] = \"000\"; itoa(n, s); printf(\"%s\\n\", s); return 0;&#125; 题目3: 两数之和算法题目一 题目二 题目4: 无重复字符的最长子串https://blog.csdn.net/heart66_A/article/details/83714168 题目5：如何用channel实现一个令牌桶题目6：编写并行计算的快速排序算法https://studygolang.com/articles/5618 题目5SQL: 查出用户表中每个省区钱最多的用户 说说对分布式系统原理的理解、容器技术的原理、网络相关知识、kubernetes用途、CI/CD相关工具 effective go 常规性Golang面试题解析 Golang面试题解析（二） Golang面试题总结"},{"title":"NodeJS问题集萃","permalink":"https://erikjiang.github.io/2018/02/15/NodeQuestion/","text":"这篇放上一些关于Node.JS的问题总结，有助于对NodeJS加深理解。 node modules 中，exports 及 module.exports 的区别？需要注意，模块导出返回的永远是module.exports (1) 两者关系： exports 是 module.exports 的引用 (2) 何时使用 exports 何时使用 module.exports: module.exports 一般用于导出指定的对象类型:如module.exports = new Gift(); exports 一般导出的是模块实例:如exports.PI = 3.1415; 描述一下 require 一个 module 的过程？ 检查 module._cacahe 是否已经缓存了模块. 如果没有缓存，则创建一个新的模块实例并且存入缓存. 根据所给的模块名调用 module.load() 方法，load 完毕后会调用 module.compile() 方法进行编译. 如果装载/编译过程中出现了错误，则会抛出错误并且将错误的模块从缓存中移除. 最后返回所依赖的模块: module.exports 哪些后缀扩展名的文件会被自动识别并require ？require 默认会加载.js、.json、.node 格式的文件 解释一下 同步和异步 以及 阻塞与非阻塞？ 同步异步关注的是: 消息通信机制 (发生调用是否立即返回)同步即在发生调用时，在没有得到结果时，该调用不会返回，待得到结果后，调用才会返回；异步即在发生调用时，在没有得到结果时，该调用直接返回，待得到结果后，通过状态、消息通知调用者，或通过回调函数处理该调用； 阻塞与非阻塞关注的是: 程序等待调用结果时的状态 （当前线程是否被挂起）阻塞即指调用结果返回前，当前线程会被挂起，直到得到结果返回；非阻塞即指调用结果返回前，该调用不会阻塞挂起当前线程； 怎样理解阻塞非阻塞与同步异步的区别 什么是C10K问题，聊聊它的技术变革及演进方向？C10K即单机1万个并发连接情况下，硬件性能足够，但依然无法提供正常服务的问题；C10K问题是计算机从PC时代变迁到互联网时代的过程中因并发量剧增而逐渐出现问题；C10K问题特点: 并发性能与硬件性能成非线性解决C10K问题的关键: 尽可能减少CPU等核心计算资源消耗，防止榨干单台服务器的性能； (1) 技术变革与演进方向1. 每个进程/线程处理一个连接2. 每个进程/线程处理多个连接（即：IO多路复用）2.1 Select 方案同时监控多个文件句柄，逐个排查IO句柄状态，若准备好了就处理缺点：有句柄上限、重复初始化、逐个排查所有文件句柄效率低； 2.2 Poll 方案Poll 方案在 Select 方案基础上解决了句柄上线和重复初始化的问题，效率有所提高；缺点：仍然存在逐个排查所有文件句柄的问题，在句柄变多后，处理效率会变差； 2.3 Epoll 方案Epoll 方案采用仅排查当前发生状态变化的文件句柄的设计，使得排查句柄的效率得到提升；Nginx、libevent、libev、NodeJS的libuv 都是基于Epoll方案；缺点：不能跨平台，Linux 使用 epoll, FreeDSD 使用 kqueue, windows 使用 IOCP; 2.4 Libevent库考虑到跨平台和移植，libevent 库将不同平台的 epoll、kqueue、IOCP 等进行了封装整合； 3. 采用协程的方式处理使用进程/线程的方式处理并发请求时，在进行系统调度时切换系统上下文的代价是很高的，且进程/线程作为处理单元太厚重，于是诞生了一种轻量级的处理单元，协程；如 Python、Lua 提供的 coroutine 模型，Go 提供的 goroutine 模型；特点：占用资源少、避免系统调度过程中的上下文切换、用户代码实现无需内核参与； C10K 问题引发的技术变革事件驱动与协程：基本概念介绍上一个10年，著名的C10K并发连接问题 什么是libuv? 在NodeJS中的作用是什么？libuv是一个高性能的事件驱动IO库，它封装了比libevent设计简练且性能更好的libev，但是由于libev对于windows平台支持有限，所以libuv又在库中加入了windows的IOCP，以支持跨平台；libuv 提供了非阻塞网络请求、各平台的事件轮询方法等； libuv 是 NodeJS 实现事件循环 Event-Loop 的关键； 什么是事件驱动？什么是事件循环？(1) 事件驱动事件驱动指持续事务管理过程中决策的一种策略，即跟随当前时间点上出现的事件，调动可用资源，执行相关任务，使不断出现的问题得以解决，防止事务堆积; 事件驱动并非NodeJS独有的功能，它是作为一概念引入到NodeJS; NodeJS主线程执行时，会启动事件循环（Event Loop）进行轮询监听和捕获事件，当一个IO事件触发时，会将其放入事件循环队列，同时会立即返回主线程而不会阻塞，这样保证了主线程可以处理其他事件而不会等待，此时，事件循环里的事件等待执行处理结果，若得到结果，则会调用相应的回调函数； NodeJS的事件循环是有libuv处理的； 什么是调用栈？调用堆栈是JS代码执行的基本机制。 当调用一个函数时，会将函数参数和返回值地址推送到堆栈。 这样允许运行时知道函数结束后继续执行代码的位置。 NodeJS的调用栈有V8处理； 由于调用栈的栈采用的是后进先出的方式，故在语法解析完毕后，主函数首先被push到栈底，文件头的首个函数被存入栈顶；而在执行过程中，则先执行栈顶的函数，然后依次执行后续函数，直到执行到最后的主函数； setImmediate 与 process.nextTick 的关系？首先，在NodeJS中事件执行的顺序是，同步任务先于异步任务执行，异步中本轮循环先于次轮循环执行； 本轮循环的异步任务按先后阶段顺序包括两个队列： nextTickQueue (如：process.nexTick()) microTaskQueue (如：Promise) 次轮循环的阶段顺序： timers (setTimeout() setInterval()) I/O callback (其他类型的回调) idle (libuv内部调用) poll (等待没有返回的IO事件) check (setImmdiate()) close callbacks (close关闭事件的回调) setImmediate()在此轮循环的check阶段，而process.nextTick()位于本轮循环的nextTickQueue当中； process.nextTick()先于setImmediate()执行； child_process 模块的 spawn、exec、fork主要区别是什么？child_process.spawn(): 异步的衍生子进程，不会阻塞NodeJS事件循环；child_process.spawnSync(): 同步的衍生子进程，会阻塞事件循环，直到子进程退出终止；child_process.exec(): 衍生一个shell并执行命令，完成时会输出stdout和stderr到回调函数；child_process.fork(): 衍生一个NodeJS进程，并通过IPC通道实现父子进程通信； cluster 模块的工作原理？由于NodeJS是单进程的，无法充分利用多核，因此需要启动多个进程满足高负载的处理需求； cluster 即可以创建并管理多个子进程并共享端口； Cluster 基于child_process.fork()实现工作进程的复制； cluster 通过一个主进程接收请求或连接，并通过循环的方式分发请求给创建的多个子进程进行处理，从而做到简单的负载均衡； 如何查看NodeJS进程占用的内存？程序外，通过进程内存查看工具程序内，通过process.memoryUsage()查看 如何在NodeJS进程退出前做最后的操作，操作是否可以异步执行？可以注册一个exit事件监听，通过process.on(&#39;exit&#39;,cb),该操作不可异步； NodeJS依赖有哪些？ 库依赖：v8、libuv、http-parser、c-ares、openSSL、zlib 工具依赖：npm、gyp、gtest 同样是实现异步操作，回调 callback 与事件 emitter 有什么区别?emitter 更加灵活，可以将执行的过程发送到别的地方，只要有方法监听这个事件就能够收到；callback 则只能在本函数或本文件内执行，相对比较固定； NodeJS性能如何优化？IO层面的优化性能的瓶颈往往在 IO，主要会涉及到数据层的优化； 添加索引索引具有占用空间小，数据结构B-tree方便查询的特点 首先开启数据库慢查询；其次获取profile信息找到具体问题；最后分析需要加索引的字段，并添加索引； 添加缓存内存IO要快于磁盘IO，所以可以使用redis缓存数据；同时缓存数据是有代价的，会存在缓存更新及失效等问题；所以，要选择访问频率高、生成代价高的数据进行缓存； 合并IO操作减少IO操作能够有效提高性能，特别对于循环操作IO的地方，以及多条冗余查询的操作，我们可以尽量使用更少的IO操作，达到同样的实现目的； CPU层面的优化NodeJS对于CPU密集的任务处理是不胜任的，所以要尽量减少CPU密集计算；可以将CPU密集计算交由其他擅长的方式来处理； MySQL性能优化技巧？查询优化 避免使用select *； 用JOIN代替子查询； 适当添加冗余字段减少表关联； 合理使用索引，如排序，分组字段； 使用索引 频繁作为查询条件的字段 关联其他表的字段 查询排序的字段 用于分组的字段 选择适合的数据类型 选择能存放数据最小的数据类型； 使用简单的数据类型，int代替varchar; 使用tinyint、smallint、mediumint代替int; 尽可能使用not null,由于null要占用4字节； 尽量少用text类型； 使用timestamp代替datetime； 单表字段不要太多，20个之内； 表结构拆分 垂直拆分将表按字段整理拆分成不同的表，如常访问的字段拆分为一张表，不常访问的拆分为另一张表； 水平拆分表条目数过多时，使用水平拆分，按照条目的id或日期等规则字段进行拆分； koa2的中间件原理// todo 对websocket的理解，及其在NodeJS中的应用// todo"},{"title":"Python开发环境预备","permalink":"https://erikjiang.github.io/2018/01/28/PythonEnv/","text":"以pyenv及pipenv + autoenv的方式开发Python; 所涉及到的工具有： pyenv 版本管理 pipenv 开发工作流管理 autoenv 基于目录的环境自动构建 Python版本管理Pyenv 是一种可以实现安装不同版本Python并可在不同版本间任意切换的管理工具。其功能类似于在Node.JS中的NVM版本管理工具； Python项目的环境及包管理Pipenv是python项目中的包管理工具，通过pipenv安装的pyhon包将被记录于pipfile当中，方便项目中包的依赖管理及安装部署;Pipenv工具是集pip, pipfile, virtualenv于一身，故它也实现了项目目录的虚拟环境创建，使得其项目的运行环境能够满足独立且隔离，不受系统及其他环境的影响;Pipenv功能类似于在Node.JS中的NPM包管理工具;123456# 1. 在第一次执行`pipenv install`时，会在该项目创建对应的虚拟环境;# 2. 同时生成Pipfile及Pipfile.lock文件，用于管理项目中Python包依赖;# 3. 第一次安装依赖包,需要指定`--two`或`--three`参数指定python2/3.x的运行环境$ pipenv install --three &lt;packageName&gt; Python项目环境的自启动Autoenv可以实现在进入项目目录时，将自启动需要的运行环境; 1 . 安装autoenv使用pip方式安装存在问题，会导致terminal异常退出，之后的版本可能会有更新;目前是以git方式进行安装：12$ git clone git://github.com/kennethreitz/autoenv.git ~/.autoenv$ echo 'source ~/.autoenv/activate.sh' &gt;&gt; ~/.bashrc 2 . 创建.env文件在已有的pipenv项目中运行pipenv shell,会得到虚拟环境路径信息，通过信息创建.env运行环境文件; 1$ echo \"source /home/&#123;userName&#125;/.local/share/virtualenvs/&#123;projectName&#125;/bin/activate\" &gt; &#123;projectName&#125;/.env 3 . 再次进入项目目录，将自启动该项目的虚拟环境"},{"title":"Serverless技术相关梳理","permalink":"https://erikjiang.github.io/2017/12/14/Serverless/","text":"在Serverless架构中，应用业务逻辑将基于FaaS（function as a service）结构形成多个相互独立的功能组件，并以API服务的形式向外提供服务；serverless让开发者无需关注资源的获取和运维，按需调用付费,节省成本； 注：FaaS属于serverless的子集，也是实现serverless化的核心服务；FaaS关键特征：事件驱动、细粒度调用、实现弹性伸缩、无需管理服务器等底层资源； 一、Serverless 优势： 节约使用成本：根据调用次数计费，节省成本，业务高峰弹性扩容，将资源分享给其他客户； 简化设备运维：底层硬件透明化，无需运维； 提升可维护性：serverless架构采用多种第三方功能服务模块组合，结构清晰，维护性高； 二、Serverless 行业场景： 低频请求场景：（物联网行业）物联网中，设备传输数据量小，且往往数据传输频率固定，所以采购serverless低频的资源即可满足计算需求； 流量突发场景：（移动互联网行业）互联网中，经常会遇到突发流量场景，如电商网站促销活动期间流量爆增，但平时的流量会回到均值，传统架构需要扩展硬件能力以支持突发情况，但平时这些资源会被浪费，采用serverless的弹性扩展，可以在高峰时扩容，高峰后释放不用的资源，节省成本； 基于事件的数据处理：在图片视频处理后端进行视频转码、图片处理等等任务时，可结合对象存储的触发器，执行severless的事件处理函数进行对应任务的处理； 三、Severless 与 微服务：两者都强调系统的解耦，微服务是以业务的粒度进行拆分，而serverless则是以函数function的粒度进行拆分，serverless的粒度更加细化； 四、serverless 与 运维：Serverless并不等于没有运维，只是运维的工作由机器设备运维转变为业务运维，运维不再关心过去的配置、操作系统、内核网络等问题，而去关心系统业务的合理性、业务间的依赖关系、业务的报警监控等，serverless将推动 DevOps 的发展； 五、Serverless 业内竞品: 亚马逊 AWS Lambda 微软 Azure Function 谷歌 Google Cloud Functions 阿里云 Function Compute 六、基于 serverless 的程序框架 https://github.com/serverless/serverless 七、阿里云 Serverless 生态：目前阿里云围绕着 Serverless 推出的平台服务： 1. 函数计算(function computer)事件驱动的全托管计算服务， 无需管理服务器基础设施：无需考虑服务器规格、配置、升级、宕机、负载均衡、等等因素； 弹性伸缩，按需付费 以事件驱动方式与对象存储、api网关无缝链接； 2. 对象存储 (Object Storage Service) 图片、音视频、日志等海量文件存储； 网页或移动应用静态文件的分离； 可对OSS云端数据进行媒体转码或图片处理； 对象存储触发函数计算事件处理函数： https://help.aliyun.com/document_detail/53097.html?spm=5176.doc51784.6.553.nvicSg 3. API网关 (API Gateway) API 录入维护统一管理 按实际调用付费 采用分布式部署，自动扩展，能够满足大规模访问场景 提供权限管理、流量管控、访问监控、安全预警、计量统计等功能 以函数计算作为Api网关的后端服务: 流程：创建事件处理函数 -&gt; 将函数授权给API网关 -&gt; 配置API https://help.aliyun.com/document_detail/54788.html?spm=5176.product29462.6.584.7YpH2m 4. 表格存储 (Table Store)一种NoSQL数据存储服务，提供海量结构化数据存储和实时访问；特性： 表格存储以实例和表的形式组织数据，通过分片和负载均衡实现规模的无缝扩展； 屏蔽底层硬件故障及错误，可快速恢复，提供高可用性； 数据被存储在SSD中且有多个备份，提供快速访问性能及可靠性； 通过表格存储触发器，可实现table store stream 和函数计算的自动对接： 流程：创建开通Stream流的数据表 -&gt; 创建函数计算事件处理函数 -&gt;在数据表下创建触发器 https://help.aliyun.com/document_detail/60304.html?spm=5176.doc60304.6.593.IoKlsE 5. 日志服务 (Log Service)提供日志类数据的采集、消费、投递及查询分析等功能 函数计算访问日志服务：https://help.aliyun.com/document_detail/61023.html?spm=5176.doc60984.6.558.fjRIfN 6. 批量计算 (BatchCompute)一种适用于大规模并行批处理作业的分布式云服务。可支持海量作业并发规模，系统自动完成资源管理、作业调度和数据加载，并按实际使用量计费； 批量计算采用对象存储服务(OSS)作为持久化存储，同时批量计算可以通过文件接口访问OSS中的资源； 批量计算的程序默认运行于VM中，也支持采用Docker容器； 批量计算中提交的作业(Job)包含多个任务(Task); 八、函数计算无状态服务实现有状态逻辑大体思路： 在函数计算初始化阶段在其本地启动一个Express服务； 创建一个函数计算事件处理函数； 前置服务API Gateway将关联 该函数计算事件处理函数； 当前端或移动端发送请求给Api网关时，网关会将请求以事件触发给函数计算； 此时函数计算的事件处理函数接收到了event事件数据源； 函数计算对此事件数据源进行解析，需要解析出URL、Header、Method、queryParams、body等请求信息，并向本地Express服务发送该http请求； 函数计算中启动的Express服务，处理完请求后返回结果，事件处理函数会对该结果进行一次输出格式的封装，以便Api网关能够识别，最终结果通过callback返回给Api网关； 代码实例：1234567891011121314151617181920212223242526272829303132333435363738394041424344&apos;use strict&apos;require(__dirname + &quot;/bin/www&quot;); // 初始化阶段在本地启动Express服务const request = require(&apos;request&apos;);const Promise = require(&apos;bulebird&apos;);const httpRequest = Promise.promisify(request);// 函数计算 事件处理函数入口module.exports.handler = (event, ctx, cb) =&gt; &#123; // 1. 入参判空等校验容错处理 ... // 2. 获取事件请求参数 let options = &#123; url: event.url, headers: event.headers, method: event.httpMethod, qs: event.queryParams, body: event.body ... &#125;; // 3. 发送请求给本地Express服务 return httpRequest(options) .then(res =&gt; &#123; ... // 4. 按照API网关识别格式返回结果 cb(null, &#123; statusCode: res.statusCode, isBase64Encoded: isBase64Encoded, headers: res.headers, body: res.body &#125;); &#125;) .catch(err =&gt; &#123; ... cb(null, &#123; statusCode: 500, isBase64Encoded: false, headers: &#123;duration: duration&#125;, body: JSON.stringify(err) &#125;); &#125;); &#125;;"},{"title":"关于信息过载","permalink":"https://erikjiang.github.io/2017/06/24/DesignP109/","text":"然后我明白了一些东西。虽然今天的世界据说在一种信息过剩的状态，实际上可能并不富裕。泛滥的只是媒介中零零碎碎的信息，每个碎片中的信息量实际上很小。在这种半生不熟的信息中，大脑怎么会不感到压抑呢？大脑的压力不是因为数量，而是因为有限的质量。在媒介进化及其对新闻材料和数据拼命搜集的背景下，世界上发生的所有事情都像草坪被割草机修剪一样，信息碎片如空中乱飞的草屑，在不同地方之间飞来飞去。这些破碎的信息附着在我们豆腐般的大脑上，像撒得太多的调料，把整个表面搞得含糊不清了。一时间，我们觉得自己知道了很多，但大脑表面的那些信息当被你加在一起时却并没有多少。相反，我们通过脚底感性，快乐的体验得到的信息量却是巨大的。人类的大脑喜欢任何需要大量信息的东西，其扩展能力焦急地等待着感知世界，以充分消耗它巨大的接受力。而此潜在能力在今天处于一种被极度压制的状态，这是我们都遭遇到的信息压力的一个来源。 《设计中的设计》原研哉 P109"},{"title":"Node 多进程初探","permalink":"https://erikjiang.github.io/2017/05/12/NodeForkProcess/","text":"前段时间在使用 Node 实现一个系统的定时任务功能，定时任务数量较多，且其中存在一些需要 CPU 计算的任务，一开始这些任务被放置在 Node 单线程中运行，可是随着任务数的增长，单一线程渐渐吃不消，任务运行高发时段，CPU 会飙至百分之九十以上，同时造成 Web 页面查询请求卡顿甚者请求无响应； 问题分析众所周知，Node.JS 的事件循环对于处理 I/O 密集型任务十分擅长，但是当遇到 CPU 密集型任务，由于事件循环是按照队列顺序执行，若队列中任何一个事件任务没有被完成，那么其他待进入队列的事件任务将不被执行，换句话说，CPU 密集型任务的耗时操作阻塞了事件队列后，将会使得事件任务的处理效率下降，严重时会使得事件循环停滞不动； 处理方案Node 作为一种的单线程语言，先天的无法发挥服务器多核 CPU 的性能，而要处理上述问题，一般有两种方式： 多进程: child_process、 cluster 多线程: threads-a-gogo 作为初探，先来讲讲多进程的通用方案 child_process，我们这里通过 fork 的方式创建一个独立负责定时任务处理的子进程，这样一来将任务处理的逻辑从主进程中分离，然后主进程要做的只是运用 IPC 进程间通讯管理分发要执行的任务给子进程； 目录概述项目大致目录结构如下：1234567891011121314151617.├── app.js├── modules│ └── childProcess.js├── swagger│ └── controller.js├── usecase│ └── worker│ └── index.js├── datasource│ ├── worker│ │ ├── task1Handler.js│ │ ├── task2Handler.js│ │ ├── task3Handler.js│ │ ├── task4Handler.js│ │ └── task5Handler.js│ └── 子进程的创建 创建工作子进程，并将子进程实例带入对应入口的 req 属性，便于获取；12345678910// modules/childProcess.jsvar child_process = require('child_process');var forkChild = child_process.fork(process.cwd() + '/usecase/task/wokerIndex.js');module.exports = function(req, res, next) &#123; var urlPath = /^\\/api\\/v\\d+\\/subProcessHandler/; if (urlPath.test(req.path) &amp;&amp; (req.method === 'POST')) &#123; req.childProcess = forkChild; &#125; return next();&#125;; 中间件添加 将子进程创建的中间件加入到服务应用的实例中；123// app.jsvar childProcess = require(process.cwd() + '/modules/childProcess');app.use(childProcess); 应用层接口 任务处理入口，运用进程间通信将执行参数传入子进程；123456// swagger/controller.jsexports.postSubProcessHandler = (req, res, next) =&gt; &#123; let childProcess = req.childProcess; childProcess.send(req.body); res.end();&#125; 定时任务分发 工作子进程通过接收主进程执行参数，分发执行任务；123456789101112// usecase/worker/index.jslet workerTaskList = &#123; taskType1: require('../../datasource/worker/task1Handler'), taskType2: require('../../datasource/worker/task2Handler'), taskType3: require('../../datasource/worker/task3Handler'), taskType4: require('../../datasource/worker/task4Handler'), taskType5: require('../../datasource/worker/task5Handler')&#125;;process.on('message', msg =&gt; &#123; workerTaskList[msg.taskType](msg.params);&#125;); 多进程的集群管理采用 fork 单独子进程处理任务，以上述的方式即可满足，可若要充分利用 CPU 多核性能资源，就需要 fork 多个工作进程，但是子进程数量变多，就存在诸如进程协调管理、负载均衡等问题； 此时就需要考虑使用 Node Cluster 模块，该模块为 Node 自身内置模块，用于构建多进程并行化程序，实现用于负载均衡的集群； 当然使用基于 Cluster 模块的 PM2 集群控制工具也是一种很好的选择； 关于 Cluster 与 PM2 ，待后续文章有机会讲解，本文就此告一段落，如有不妥之处，望指出，感谢。"},{"title":"以 Node 方式, CSV 转 JSON","permalink":"https://erikjiang.github.io/2017/05/07/NodeCsvToJson/","text":"简述需求:有时，我们会在项目中遇到导入文档数据的功能需求，具体的比如可能会需要将 excel 表格数据导入到项目系统的数据库中，通过读取文件内容，解析并转换数据，最终存储至数据库的表中； 可行方案：针对于上述的这一系列较为繁琐的处理过程，在 Node 开发中，NPM 提供了 csv to json 的工具模块，它能够将 CSV 标准文件转换成 JSON 数据，从而方便程序进一步的数据存储； 具体的思路： 需要将 Excel 表格文件转换为 CSV 标准文件； 使用 csvtojson 包对 CSV 标准文件进行 JSON 化数据转换； 将 JSON 数据存入数据库对应表当中； 注: 使用 Mac 版 MicroOffice 将 EXCEL 表格文件转为 CSV 格式文件后，在终端中打开 CSV 文件中文出现乱码? 也不知为何，微软在做 CSV 文件转换时，对字符集支持有限，导致中文乱码；该问题使用 Libre Office 则可以解决，Libreoffice 在转换 CSV 时提供字符编码转换设置，转换时设置 UTF-8 即可, 据说使用 Google Sheets 可以达到相同效果； 操作说明安装：npm install csvtojson --save 实例：123456789101112131415161718192021222324252627282930313233343536/** csv file account,password,company,username,phone test1@demo.com,123456,安徽公司,小明,13112310168 test2@demo.com,123456,浙江公司,小伟,13327223199**/const csvFilePath='&lt;path to csv file&gt;';const csv=require('csvtojson');let userData = [];csv().fromFile(csvFilePath).on('json',(jsonObj)=&gt;&#123; // 结合 CSV 头部属性逐行将内容转换为 JSON 对象 userData.push(jsonObj);&#125;).on('done',(error)=&gt;&#123; console.log('result: ' + JSON.stringify(userData)); /** [ &#123; account:'test1@demo.com', password:'123456', company:'安徽公司', username:'小明', phone: '13112310168' &#125;, &#123; account:'test2@demo.com', password:'123456', company:'浙江公司', username:'小伟', phone: '13327223199' &#125; ] **/&#125;)"},{"title":"Node 任务定时器的使用","permalink":"https://erikjiang.github.io/2017/04/02/NodeSchedule/","text":"什么是任务定时器： 能够在指定时间定期地执行命令、脚本或者程序，可以被用于系统的自动化维护及管理的工具； 最常见的任务定时器当属 Cron ，该词源于希腊语 chronos，原意是时间；在类 Unix 系统中，可以通过 crontab 命令设置定时任务执行的时间周期，然后 cron 的守护进程会在后台实时的检测是否有需要执行的任务，通常这些需要执行的任务被称为 cron jobs; 定时器的参数说明：Cron 在类 Unix 系统中有很多的实现，如：cronie、bcron、dcron、fcron 等；虽然实现有多种，但是基于 cron 风格的时序参数结构却是相似的； 这里展示了一个 cron 的时序参数格式： 123456789* * * * * *┬ ┬ ┬ ┬ ┬ ┬│ │ │ │ │ |│ │ │ │ │ └ day of week (0 - 7) (0 or 7 is Sun)│ │ │ │ └───── month (1 - 12)│ │ │ └────────── day of month (1 - 31)│ │ └─────────────── hour (0 - 23)│ └──────────────────── minute (0 - 59)└───────────────────────── second (0 - 59, OPTIONAL) * 表示通配符，匹配任意，当秒为 ‘*’ 时，代表任意秒数都会触发执行； 观察这组参数说明的过程中，发现了一个有趣的问题: Q: 在 day of week 参数上，0 或 7 其实都可以代表星期日(Sunday)，WHY?A: 原因在于 cron 的众多实现版本里，对于这个参数，部分实现的设置为：0 - 6 =&gt; Sunday - Saturday，而另一部分实现的设置为：1 - 7 =&gt; Monday - Sunday，为了兼容，确保两种设置都正确，所以如上； 具体参数设置的试例：123456789101112131415161718192021222324* * * * * # 例1: 每1分钟执行一次3,15 * * * * # 例2: 每小时的第3和第15分钟执行3,15 8-11 * * * # 例3: 在每天上午8点到11点的第3和第15分钟执行3,15 8-11 */2 * * # 例4：每隔两天的上午8点到11点的第3和第15分钟执行3,15 8-11 * * 1 # 例5：每周一的上午8点到11点的第3和第15分钟执行30 21 * * * # 例6：每晚的21时30分执行45 4 1,10,22 * * # 例7：每月的1、10、22日的4时45分执行 10 1 * * 6,0 # 例8：每周六、周日的1时10分执行0,30 18-23 * * * # 例9：每天 18:00 至 23:00 之间每隔30分钟执行0 23 * * 6 # 例10：每星期六晚 23:00 执行* */1 * * * # 例11：每隔1小时执行* 23-7/1 * * * # 例12：晚上23时到次日7时之间每隔1小时执行 任务定时器在 Node 中的使用在 Node.JS 中，可以使用 Node-Schedule 这个 npm 包来进行任务定时器的操作： 描述： A cron-like and not-cron-like job scheduler for Node. 安装方式：1npm install node-schdule 如何使用：任务定时器的创建：123456var schedule = require('node-schedule');var j = schedule.scheduleJob('42 * * * *', function()&#123; // 每小时的第42分钟被执行 console.log('The answer to life, the universe, and everything!');&#125;); scheduleJob() 方法的回调函数用于实现具体定时任务执行的内容; 任务定时器的注销：1j.cancel(); 基于JS Date类型的时间参数设置：1234567var schedule = require('node-schedule');// 2012年12月21日5时30分执行var date = new Date(2012, 11, 21, 5, 30, 0);var j = schedule.scheduleJob(date, function()&#123; console.log('The world is going to end today.');&#125;); 注意： 在使用 Date 设置参数是，月份的设定范围是 0~11 ，其中0代表一月，11代表十二月； 4.2.4 递归循环任务的设置：1234567891011121314var schedule = require('node-schedule');var rule = new schedule.RecurrenceRule();// 每小时的第 42 分钟执行rule.minute = 42;// rule.dayOfWeek = 5;// rule.month = 6;// rule.dayOfMonth = 15;// rule.hour = 1;// rule.second = 0var j = schedule.scheduleJob(rule, function()&#123; console.log('The answer to life, the universe, and everything!');&#125;); 指定时间范围的设置：123456789var rule = new schedule.RecurrenceRule();// 每个月的星期四、五、六、日的17点整执行rule.dayOfWeek = [0, new schedule.Range(4, 6)];rule.hour = 17;rule.minute = 0;var j = schedule.scheduleJob(rule, function()&#123; console.log('Today is recognized by Rebecca Black!');&#125;); RecurrenceRule 实例的每个cron属性可接受以数组的形式添加多个时间数值，Range()方法可指定一个范围的开始值及结束值; 通过对象字面量的方式设置：1234// 每周日的14时30分执行var j = schedule.scheduleJob(&#123;hour: 14, minute: 30, dayOfWeek: 0&#125;, function()&#123; console.log('Time for tea!');&#125;); 任务定时器的开始及结束时间设置：12345678910// 5秒后任务开始执行且10秒后任务结束，任务在此过程中每秒执行一次let startTime = new Date(Date.now() + 5000);let endTime = new Date(startTime.getTime() + 5000);var j = schedule.scheduleJob(&#123; start: startTime, end: endTime, rule: '*/1 * * * * *' &#125;, function()&#123; console.log('Time for tea!');&#125;); 参考：http://www.cnblogs.com/zhongweiv/p/node_schedule.htmlhttp://www.codexiu.cn/javascript/blog/16175/https://zh.wikipedia.org/wiki/Cron"},{"title":"使用 Gulp 构建工程","permalink":"https://erikjiang.github.io/2016/01/29/BuildingWithGulp/","text":"gulp 是什么？ 如果曾做过 Linux C 相关开发，一定对于编译构建工具 Automake 有一定的了解，我们通过在工程中编写 makefile 脚本文件，实现工程代码的检查、编译以及安装等等。 而在如今前端工程化的背景下，愈来愈强调前端的工作流程，如果有一种工具，能够如 automake 一般，能够帮助我们处理检测、编译、自动发布以及文件压缩、文件Hash、等等事宜，那一定能简化不少工作，而 gulp 即是这样的一种工具。 Gulp 是一个基于 Node.js 的流式构建工具，有别与 Grunt 基于 I/O 的构建方式，Grunt 在处理过程中会产生一些中间态的临时文件，而任务会最终基于这些临时文件生成构建后的结果文件。在这个过程中 Grunt 会进行大量的 I/O 读写操作，那么如果遇到大型项目工程，Grunt 的构建处理速度将大大拖长，这也就是为什么 Gulp 渐渐取代 Grunt 的原因之一，如果想了解更多，可以试着阅读 《 gulp vs grunt 》 这篇文章。 由于 Gulp 通过代码优于配置的策略不仅使得任务易于编写，同时也易于维护和阅读。Gulp 基于的 NodeJS 的流，使用管道 Pipe 的概念，这一级的输出作为下一级的输入，中间无需在硬盘上写入任何临时文件和目录，让任务构建的更加迅速。 Gulp 的基本安装 ：这里我默认您已经安装了 Node.js 以及 NPM 包管理工具，首先需要全局安装 Gulp 工具： $ npm install gulp -g 然后，需要在已有的项目中安装 gulp 的依赖包（前提是项目中已存在 package.json 文件）： $ npm init #如果项目中没有 package.json 文件，需要执行此命令生成，有则忽略； $ npm install gulp --save-dev #安装 gulp 依赖包 一切顺利，将会发现 package.json 文件中的 devDependencies 项里多了一个 gulp 子项，且 node_modules 中也多了一个 gulp 的依赖包； Gulp 插件的安装 ：要使用 Gulp 中的功能，必须依赖于 Gulp 的插件，在 Gulp 中每个插件完成一个具体的功能任务，而实际的构建工作往往是多个任务的流程，所以就要涉及到多个插件的安装，那么具体要用到哪些插件呢？这里我大概列出了常用的几项，仅供参考。 Sass compile (gulp-ruby-sass) # css 预处理器编译 Autoprefixer (gulp-autoprefixer) # css 浏览器兼容前缀处理 Minify CSS with cssnano (gulp-cssnano) # css 文本压缩 JSHint (gulp-jshint) # JS 代码检测 Concatenation (gulp-concat) # 代码文本合并 Uglify (gulp-uglify) # 代码文本压缩 Compress images (gulp-imagemin) # 图片资源压缩 LiveReload (gulp-livereload) # 文件变动页面重载 Caching of images so only changed images are compressed (gulp-cache) # 图片资源缓存 Notify of changes (gulp-notify) # 任务完成通知提示 Clean files for a clean build (del) # 清理构建目录及文件 安装方式与安装 Gulp 依赖包命令一致： $ npm install [package-name] --save-dev Gulp 任务文件的配置 ：这里我直接使用本人 Github 上的一个聊天室项目 node_chat 来进行讲解。 // 加载插件 var del = require(&apos;del&apos;), gulp = require(&apos;gulp&apos;), cache = require(&apos;gulp-cache&apos;), jshint = require(&apos;gulp-jshint&apos;), concat = require(&apos;gulp-concat&apos;), uglify = require(&apos;gulp-uglify&apos;), rename = require(&apos;gulp-rename&apos;), notify = require(&apos;gulp-notify&apos;), cssnano = require(&apos;gulp-cssnano&apos;), imagemin = require(&apos;gulp-imagemin&apos;), livereload = require(&apos;gulp-livereload&apos;), autoprefixer = require(&apos;gulp-autoprefixer&apos;); // 设置源路径 var srcPaths = { scripts: [&apos;./public/javascripts/*.js&apos;], images: &apos;./public/images/*&apos;, styles: &apos;./public/stylesheets/*.css&apos; }; // 设置目标路径 var destPaths = { all: &apos;./build/**&apos;, scripts: &apos;./build/script&apos;, images: &apos;./build/image&apos;, styles: &apos;./build/style&apos; }; // 任务：脚本代码验证 gulp.task(&apos;analysis&apos;, function() { return gulp.src(srcPaths.scripts) .pipe(jshint()) .pipe(jshint.reporter(&apos;default&apos;)) .pipe(notify({message: &apos;Analysis task complete.&apos;})); }); // 任务1. 脚本合并与压缩 gulp.task(&apos;scripts&apos;, [&apos;analysis&apos;], function() { return gulp.src(srcPaths.scripts) .pipe(concat(&apos;main.js&apos;)) .pipe(gulp.dest(destPaths.scripts)) .pipe(rename({suffix: &apos;.min&apos;})) .pipe(uglify()) .pipe(gulp.dest(destPaths.scripts)) .pipe(notify({message: &apos;Scripts task complete.&apos;})); }); // 任务2. 资源图片压缩 gulp.task(&apos;images&apos;, function() { return gulp.src(srcPaths.images) // 压缩 imagemin 并缓存 cache 图片 .pipe(cache(imagemin({ optimizationLevel: 3, progressive: true, interlaced: true }))) .pipe(gulp.dest(destPaths.images)) .pipe(notify({message: &apos;Images task complete.&apos;})); }); // 任务3. 样式表的压缩 gulp.task(&apos;styles&apos;, function() { return gulp.src(srcPaths.styles) .pipe(autoprefixer(&apos;last 2 version&apos;, &apos;safari 5&apos;, &apos;ie 8&apos;, &apos;ie 9&apos;, &apos;opera 12.1&apos;, &apos;ios 6&apos;, &apos;android 4&apos;)) .pipe(gulp.dest(destPaths.styles)) .pipe(rename({suffix: &apos;.min&apos;})) .pipe(cssnano()) .pipe(gulp.dest(destPaths.styles)) .pipe(notify({message: &apos;Styles task complete.&apos;})); }); // 任务：清理构建目录 gulp.task(&apos;clean&apos;, function(cb) { // 清空 build 目录 return del([destPaths.scripts, destPaths.images, destPaths.styles], cb); }); // 默认任务 gulp.task(&apos;default&apos;, [&apos;clean&apos;], function() { gulp.start(&apos;scripts&apos;, &apos;images&apos;, &apos;styles&apos;); }); // 任务：监听文件变动 gulp.task(&apos;watch&apos;, function() { // 监听文件变动 gulp.watch(srcPaths.scripts, [&apos;scripts&apos;]); gulp.watch(srcPaths.images, [&apos;images&apos;]); gulp.watch(srcPaths.styles, [&apos;styles&apos;]); // 创建 livereload 监听服务 livereload.listen(); // 若有改变自动重载刷新 gulp.watch([destPaths.all]).on(&apos;change&apos;, livereload.changed); });"},{"title":"WebSocket学习笔记","permalink":"https://erikjiang.github.io/2016/01/19/WebSocketNote/","text":"HTTP 协议是一种无状态协议，服务器端本身不具备识别客户端的能力，必须借助外部机制，比如 session 和 cookie 才能与特定客户端保持对话。那么如果我们遇到一种需求，它要求保证服务器端与客户端持续的数据交换，比如网络聊天，那再使用 HTTP 协议就会有不便，这时我们就需要用到 Websocket API了。 概述 ：WebSocket 的主要作用：允许服务器端与客户端进行全双工（full-duplex）的通信。举例来说，HTTP 协议有点像发电子邮件，发出后必须等待对方回信；WebSocket 则是像打电话，服务器端和客户端可以同时向对方发送数据，它们之间存着一条持续打开的数据通道。 WebSocket 协议完全可以取代 Ajax 方法，用来向服务器端发送文本和二进制数据，而且还没有“同域限制”。 WebSocket 不使用 HTTP 协议，浏览器在发送 Websocket 请求时，会将 HTTP 协议升级（Upgrade）为 webSocket 自己的协议。 WebSocket 协议用 ws 表示。此外，还有 wss 协议，表示加密的 WebSocket 协议，对应 HTTPs 协议。 在完成TCP协议三次握手之后，WebSocket 协议就在 TCP 协议之上，开始数据的传送。 WebSocket 协议需要服务器支持，目前比较流行的实现是基于 node.js 的 socket.io ，至于浏览器端，目前主流浏览器都支持 WebSocket 协议。 客户端 ：浏览器端对WebSocket协议的处理，主要归纳为三件事： 建立连接和断开连接 发送数据和接收数据 处理错误 建立连接和断开连接首先，客户端要检查浏览器是否支持 WebSocket ，使用的方法是查看 window 对象是否具有 WebSocket 属性。 if(window.WebSocket != undefined) { // WebSocket代码 } 然后，开始与服务器建立连接（这里假定服务器就是本机的1740端口，需要使用ws协议）。 if(window.WebSocket != undefined) { var connection = new WebSocket(&apos;ws://localhost:1740&apos;); } 建立连接以后的 WebSocket 实例对象（即上面代码中的connection），有一个readyState属性，表示目前的状态，可以取4个值： 0： 正在连接 1： 连接成功 2： 正在关闭 3： 连接关闭 握手协议成功以后，readyState 就从 0 变为 1 ，并触发 open 事件，这时就可以向服务器发送信息了。我们可以指定 open 事件的回调函数。 connection.onopen = wsOpen; function wsOpen (event) { console.log(&apos;Connected to: &apos; + event.currentTarget.URL); } 关闭 WebSocket 连接，会触发 close 事件。 connection.onclose = wsClose; function wsClose () { console.log(&quot;Closed&quot;); } connection.close(); 发送数据和接收数据连接建立后，客户端通过 send 方法向服务器端发送数据。 connection.send(message); 除了发送字符串，也可以使用 Blob 或 ArrayBuffer 对象发送二进制数据。 // 使用ArrayBuffer发送canvas图像数据 var img = canvas_context.getImageData(0, 0, 400, 320); var binary = new Uint8Array(img.data.length); for (var i = 0; i &lt; img.data.length; i++) { binary[i] = img.data[i]; } connection.send(binary.buffer); // 使用Blob发送文件 var file = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;).files[0]; connection.send(file); 客户端收到服务器发送的数据，会触发 message 事件。可以通过定义 message 事件的回调函数，来处理服务端返回的数据。 connection.onmessage = wsMessage; function wsMessage (event) { console.log(event.data); } 上面代码的回调函数 wsMessage 的参数为事件对象 event ，该对象的 data 属性包含了服务器返回的数据。 如果接收的是二进制数据，需要将连接对象的格式设为 blob 或 arraybuffer。 connection.binaryType = &apos;arraybuffer&apos;; connection.onmessage = function(e) { // ArrayBuffer对象有byteLength属性 console.log(e.data.byteLength); }; 处理错误如果出现错误，浏览器会触发WebSocket实例对象的error事件。 connection.onerror = wsError; function wsError(event) { console.log(&quot;Error: &quot; + event.data); } 服务端 ：服务器端需要单独部署处理 WebSocket 的代码。下面用 node.js 搭建一个服务器环境。 var http = require(&apos;http&apos;); var server = http.createServer(function(request, response) {}); 假设监听 1740 端口。 server.listen(1740, function() { console.log((new Date()) + &apos; Server is listening on port 1740&apos;); }); 接着启动 WebSocket 服务器。这需要加载websocket库，如果没有安装，可以先使用npm命令安装。 var WebSocketServer = require(&apos;websocket&apos;).server; var wsServer = new WebSocketServer({ httpServer: server }); WebSocket服务器建立 request 事件的回调函数。 var connection; wsServer.on(&apos;request&apos;, function(req){ connection = req.accept(&apos;echo-protocol&apos;, req.origin); }); 上面代码的回调函数接受一个参数 req ，表示 request 请求对象。然后，在回调函数内部，建立WebSocket连接connection。接着，就要对connection的message事件指定回调函数。 wsServer.on(&apos;request&apos;, function(r){ connection = req.accept(&apos;echo-protocol&apos;, req.origin); connection.on(&apos;message&apos;, function(message) { var msgString = message.utf8Data; connection.sendUTF(msgString); }); }); 最后，监听用户的 disconnect 事件。 connection.on(&apos;close&apos;, function(reasonCode, description) { console.log(connection.remoteAddress + &apos; disconnected.&apos;); }); 使用 ws 模块，部署一个简单的 WebSocket 服务器非常容易。 var WebSocketServer = require(&apos;ws&apos;).Server; var wss = new WebSocketServer({ port: 8080 }); wss.on(&apos;connection&apos;, function connection(ws) { ws.on(&apos;message&apos;, function incoming(message) { console.log(&apos;received: %s&apos;, message); }); ws.send(&apos;something&apos;); }); Socket.IO ：Socket.io 是目前最流行的 WebSocket 实现，包括服务器和客户端两个部分。它不仅简化了接口，使得操作更容易，而且对于那些不支持 WebSocket 的浏览器，会自动降为 Ajax 连接，最大限度地保证了兼容性。它的目标是统一通信机制，使得所有浏览器和移动设备都可以进行实时通信。 第一步，在服务器端的项目根目录下，安装 socket.io 模块。 $ npm install socket.io 第二步，在根目录下建立 app.js ，并写入以下代码（假定使用了Express框架）。 var app = require(&apos;express&apos;)(); var server = require(&apos;http&apos;).createServer(app); var io = require(&apos;socket.io&apos;).listen(server); server.listen(80); app.get(&apos;/&apos;, function (req, res) { res.sendfile(__dirname + &apos;/index.html&apos;); }); 上面代码表示，先建立并运行 HTTP 服务器。Socket.io 的运行建立在 HTTP 服务器之上。 第三步，将 Socket.io 插入客户端网页。 &lt;script src=&quot;/socket.io/socket.io.js&quot;&gt;&lt;/script&gt; 然后，在客户端脚本中，建立 WebSocket 连接。 var socket = io.connect(&apos;http://localhost&apos;); 由于本例假定 WebSocket 主机与客户端是同一台机器，所以 connect 方法的参数是 http://localhost。接着，指定 news 事件（即服务器端发送news）的回调函数。 socket.on(&apos;news&apos;, function (data){ console.log(data); }); 最后，用 emit 方法向服务器端发送信号，触发服务器端的 anotherNews 事件。 socket.emit(&apos;anotherNews&apos;); 请注意，emit 方法可以取代 Ajax 请求，而on方法指定的回调函数，也等同于 Ajax 的回调函数。 第四步，在服务器端的 app.js，加入以下代码。 //广播信息给除当前用户之外的用户 socket.broadcast.emit(&apos;user connected&apos;); //广播给全体客户端 io.sockets.on(&apos;connection&apos;, function (socket) { socket.emit(&apos;news&apos;, { hello: &apos;world&apos; }); socket.on(&apos;anotherNews&apos;, function (data) { console.log(data); }); }); 上面代码的 io.sockets.on 方法指定 connection 事件（ WebSocket 连接建立）的回调函数。在回调函数中，用 emit 方法向客户端发送数据，触发客户端的 news 事件。然后，再用 on 方法指定服务器端anotherNews 事件的回调函数。 不管是服务器还是客户端，socket.io 提供两个核心方法：emit方法用于发送消息，on方法用于监听对方发送的消息。 参考： JavaScript标准参考教程"},{"title":"NodeJS应用部署初探","permalink":"https://erikjiang.github.io/2016/01/13/DeployNodeApp/","text":"尝试使用 MongoLab、Redis4You、Heroku 来部署一个 node 应用。 首先，使用 MongoLab ：MongoLab 与 MongoHQ 是当今比较流行的 MongoDB 云数据库提供商。MongoLab 的免费套餐提供了 500MB 的存储空间，MongoLab 的使用比较简单明了，这里简要概述一二。 注册mongolab.com/signup 创建数据库注册成功后，此时进入到自己的控制面板页，点击 Create new 创建一个数据库。在 Plan 选项选择 Single-node 的 Sandbox 免费套餐。在 Darebase name 除填写数据库的名字，点击 Create new MongoDB deployment 完成创建。 创建成功后跳转到了控制面板页，点击刚才创建的数据库进入该数据库配置页面。我们需要先为数据库创建一个拥有读写权限的用户，点击 Click here 添加一个用户。 打开 mongodb.js ，将： mongoose.connect(&apos;mongodb://localhost/drifter&apos;, {server: {poolSize: 10}}); 修改为： mongoose.connect(&apos;your_mongolab_url&apos;, {server: {poolSize: 10}}); 注意：将 &lt;dbuser&gt; 和 &lt;dbpassword&gt; 分别替换为刚才为数据库创建的用户名和密码。 然后，使用 Redis4You ：redis4you 是一家 Redis 云服务提供商，提供多个档次的 Redis 实例供选择，所有类型的服务包括免费的 5M 试用版，都提供诸如多 DB，持久化等功能。这里我们仅供测试使用。 注册redis4you.com/login_new 创建数据库验证邮箱完成注册。登录 Redis4You，点击右上角的 My Redis Instanses 进入 Redis 管理页面，点击 Provision new instance 创建一个新的数据库。点击 5MB 的免费套餐右边的 Select this instance 完成创建。 此时跳转到 Redis 管理页面，点击 Information 查看数据库详情。我们只会用到 Host、Port 和 Auth 这三项，其中 Auth 为随机生成的访问数据库的凭证。 注意：一定要点击左上角的 start 启动数据库。 打开 redis.js ，将： var client = redis.createClient(); 修改为： var client = redis.createClient(Port, Host, {auth_pass: Auth}); 注意：将 Host、Port 和 Auth 分别替换为给定的值。 最后，部署到 HeroKu ：Heroku 是一个主流的 PaaS 提供商，在开发人员中广受欢迎。这个服务围绕着基于 Git 的工作流设计，假如你熟悉 Git ，那部署就十分简单。这个服务原本是为托管 Ruby 应用程序而设计的，但 Heroku 之后加入了对 Node.js 、Clojure 、Scala 、Python 和 Java 等语言的支持。Heroku 的基础服务是免费的。 注册heroku.com 创建一个应用右上角 “＋” 中点击 Create a new app ，填写独一无二的应用名称后，点击 creat app 即创建成功，然后点击 Open app 即可打开应用 URL 链接 。 安装 Heroku ToolbeltHeroku 官方提供了 Heroku Toolbelt 工具更方便地部署和管理应用。它包含三个部分： Heroku client ：创建和管理 Heroku 应用的命令行工具 Foreman ：一个在本地运行你的 app 的不错的选择 Git ：分布式版本控制工具，用来把应用推送到 Heroku Heroku Toolbelt 下载地址：toolbelt.heroku.com 。 注意：假如你的电脑上已经安装了 Git ，那么在安装的时候选择 Custom Installation 并去掉安装 Git 的选项，否则选择 Full Installation 。 安装成功后，打开终端 shell ，输入 heroku login ，然后输入在 Heroku 注册的帐号和密码进行登录。Git 会检测是否有 SSH 密钥，如果有，则使用此密钥并上传，如果没有，则创建一个密钥并上传。 Tips：SSH 密钥通常用于授予用户访问服务器的权限。可将它们用于某些配置中，以便无需密码即可访问服务器。许多 PaaS 提供商都使用了此功能。 Procfile在工程的根目录下新建一个 Procfile 文件，添加如下内容： web: ./bin/www Procfile 文件告诉了服务器该使用什么命令启动一个 web 服务，这里我们通过 ./bin/www 执行 Node 脚本。为什么这里声明了一个 web 类型呢？官方解释为： The name “web” is important here. It declares that this process type will be attached to the HTTP routing stack of Heroku, and receive web traffic when deployed. 上传应用首先 clone 一份 git 仓库源码到你的本地机器中； $ heroku git:clone -a web_app_name $ cd web_app_name 使用 git 将源码变动的部分更新以及部署到 heroku 当中. $ git add . $ git commit -am &quot;make it better&quot; $ git push heroku master 在 push 到 heroku 服务器之前，我们还需要做一个工作。由于我国某些政策的原因，我们需到 ~/.ssh/ 目录下，新建一个 config 文件，内容如下： Host git.heroku.com User yourName Hostname 107.21.95.3 PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa port 22 然后回到 Git Bash ，输入： $ git push heroku master 稍等片刻即上传成功。 参考： nswbmw github"},{"title":"Redis学习笔记","permalink":"https://erikjiang.github.io/2016/01/05/RedisNote/","text":"Redis是一个开源、支持网络、基于内存、键值对存储数据库，使用ANSI C编写。Redis是时下最流行的键值对存储数据库。 历史： 2015年6月至今：Redis 的开发由Redis Labs赞助。 2013年5月至2015年6月期间：其开发由Pivotal赞助。 2013年5月之前：其开发由VMware赞助。 如何安装 Redis ：在 Linux、Unix、Mac 中源码编译的安装方式： $ wget http://download.redis.io/releases/redis-3.0.6.tar.gz $ tar xzf redis-3.0.6.tar.gz $ cd redis-3.0.6 $ make 在 Mac 中可使用 Homebrew 管理工具进行安装： $ brew install redis 如何运行 Redis ：首先需要运行 redis 服务端： $ redis-server 然后切换到另一终端命令行，运行客户端可执行程序： $ redis-cli 注意，redis默认使用 6379 端口，如果想指定其它端口： $ redis-cli --port 1234 //指定端口为：1234 然后可输入 info 命令查看所有信息，确认 redis 是否已被正确安装； 同时，还可以使用 redis 提供的 ping 命令来测试客户端与 redis 服务是否连接正常： redis 127.0.0.1:6379&gt; PING PONG 关于 Redis 的数据类型：redis 存在五种数据类型，包括 string（字符串类型），hash（哈希类型），list（链表类型），set（无序集合类型）及 zset（有序集合类型）； 字符串类型 [string] 是 Redis 中最基本的数据类型，也是其他 4 种数据类型的基础，它能存储任何形式的字符串，包括二进制数据。 哈希类型 [hash] 是一个 string 类型的字段和字段值的映射表。哈希类型适合存储对象。相较于将对象的每个字段存成单个 string 类型，将一个对象存储在 hash 类型中会占用更少的内存，并且可以更方便的存取整个对象。 链表类型 [list] 可以存储一个有序的字符串列表，内部是使用双向链表实现的。所以我们通常用链表类型的 LPUSH 和 LPOP 或者 RPUSH 和 RPOP 实现栈的功能，用 LPUSH 和 RPOP 或者 RPUSH 和 LPOP 实现队列的功能。 集合类型 [set] 和数学中的集合概念相似，比如集合中的元素是唯一的、无序的，集合与集合之间可以进行交并差等操作。 有序集合类型 [zset] 与集合类型一样，只不过多了个 “有序” ，有序集合中每一个元素都关联了一个分数，虽然集合中每个元素都是不同的，但是它们的分数却可以相同。 关于 Redis 的基本使用：string［字符串类型］：字符串类型的操作中，SET、GET 以及 INCR 是三个常用的命令。 用法： SET key value : 设置 key 的值为 value，成功时返回 OK。 GET key : 获取 key 的值，成功时返回 key 的值。 INCR key : 给 key 的值加 1，成功时返回更新后的 key 值。 KEYS pattern : 查找当前所有符合给定模式 pattern 的 key（该命令适用于 redis 五种数据类型） hash［哈希类型］：哈希类型适合于存储对象。 用法： HSET key field value : 设置 key 的 field 字段值为 value，成功时返回 1，如果 key 中 field 字段已经存在且旧值已被新值覆盖，则返回 0 。 HGET key field : 获取 key 的 field 字段的值，成功时返回 field 字段的值。 HGETALL key : 获取 key 中所有的字段和其值，若 key 不存在，返回空。 list［链表类型］：链表类型适合存储如社交网站的新鲜事，假如有一个存储了几千万个新鲜事的链表，获取头部或尾部的 10 条记录也是极快的。以下是链表类型常用的几个命令： LPUSH key value [value …] : 往 key 链表左边添加元素，返回链表的长度。 RPUSH key value [value …] : 往 key 链表右边添加元素，返回链表的长度。 LPOP key : 移除 key 链表左边第一个元素，并返回被移除元素的值。 RPOP key : 移除 key 链表右边第一个元素，并返回被移除元素的值。 LRANGE key start stop : 获取链表中某一片段，但不会修改原链表的值。另外，与很多语言中截取数组片段的 slice 方法不同的是 LRANGE 返回的值包含最右边的元素。LRANGE 命令也支持负索引，即 -1 表示最后一个元素，所以 LRANGE key 0 -1 可以获取链表 key 中所有的元素。如果 start 的位置比 stop 的位置靠后，则返回空，如果 stop 大于实际链表范围，则会返回到链表最后的元素。 set［集合类型］：集合类型非常适合存储如文章的标签，这样我们在获取标签的时候就可以避免获取重复的标签了。以下是集合类型常用的几个命令： SADD key member [member …] : 向集合中添加一个或多个元素，若要添加的元素集合中已经存在，则忽略这个元素。返回成功加入的元素数量（不包含忽略的）。 SREM key member [member …] : 从集合中删除一个或多个元素，返回成功移除的元素的数量。 SMEMBERS key : 返回集合中所有元素。 集合类型还支持交集、差集和并集的运算，命令如下： SINTER key [key …] : 多个集合执行交集运算，并返回运算结果。 SDIFF key [key …] : 多个集合执行差集运算，并返回运算结果。 SUNION key [key …] : 多个集合执行并集运算，并返回运算结果。 zset［有序集合类型］：有序集合类型中每一个元素都关联了一个分数。这使得我们不仅可以进行大部分集合类型的操作，还可以通过分数获取指定分数范围内的元素等与分数有关的操作。有序集合类型适用于比如通过文章访问量排序的功能。常用命令如下： ZADD key score member [score member …] ： 向有序集合中加入一个或多个元素和该元素的值，如果该元素已存在则用新的分数替换原来的分数。返回新加入的元素个数（不包含已存在的元素）。 ZREM key member [member …] ： 删除集合中一个或多个元素，返回成功删除的元素的个数（不包含本来就不存在的元素）。 ZRANGE key start stop [WITHSCORES] ： 按元素分数从小到大顺序返回从 start 到 stop 之间所有的元素（同样包含两端的元素）。如果需要同时获得对应元素的分数的话，在尾部加上 WITHSCORES 参数。如果两个元素分数相同，则按照元素的字典序排列。 ZREVRANGE key start stop [WITHSCORES] ： 同上，只不过 ZREVRANGE 是按照元素分数从大到小顺序给出结果的。 参考： ZhiCun Github"},{"title":"关于Node版本切换","permalink":"https://erikjiang.github.io/2015/12/22/ChangeNodeVersion/","text":"今天在安装yeoman时，警告提示node版本过低，那么好吧，咱们就来更新node版本。顺带把整个过程梳理一遍，做一个有节操的程序猿。 提到node的安装，就不得不说这个 NVM （ Node Version Manager ）Node版本管理器，一般情况下，不建议直接下载node安装程序进行安装，为了便于管理更新频繁的node，比较妥当的做法是通过nvm工具进行安装，而nvm工具本身的安装就不多加细究，在Mac上的方式是通过homebrew管理工具进行安装的，其他平台可Google寻找答案。 NVM的安装及简单使用：Mac上安装nvm的命令：1$ brew install nvm 我们可以查看nvm是否正常安装：1$ nvm —version 如果安装正常，我们可以通过如下命令，查看当前哪些版本可供安装：1$ nvm ls-remote 然后可以通过下面命令安装指定版本的node：1$ nvm install &lt;version&gt; 同样我们可以卸载指定版本的node：1$ nvm uninstall &lt;version&gt; 查看当前系统中可管理的所有版本node信息：1$ nvm ls Node安装及版本切换：我这里安装了v0.12.9版本：123$ nvm install v0.12.9#################################################### 100.0%Now using node v0.12.9 当前我使用v0.12.9版本即可，但如果此时我需要切换至其它版本，就可以使用如下命令：1$ nvm use &lt;version&gt; 但是使用 nvm use 命令有一个问题，即只对当前shell窗口生效，如果我们此时关闭该shell窗口，重新打开命令行就会发现node的版本又恢复到之前的版本状态；所以针对这种状况，我们需要设置一个默认node版本，命令如下：1$ nvm alias default &lt;version&gt; 此时打开shell窗口，就会使用默认版本的node执行； 噢，对了，更新了node版本过后，别忘记更新npm：1npm install -g npm"},{"title":"Node调试工具的使用","permalink":"https://erikjiang.github.io/2015/05/11/NodeInspector/","text":"今天发现一个在浏览器上调试node代码的工具，分享一下.这个工具名叫：Node Inspector感兴趣的话可以点链接到Github看看详细文档说明，这里就简要介绍一二； 安装步骤安装命令：1npm install –g node-inspector 运行步骤1）首先，需要启动node.js应用命令如下：1node --debug-brk=[app port] [app name] 或者1node --debug=[app port] [app name] 例如：1node --debug-brk=3001 bin/www [debug与debug-brk两种配参见说明] 2）然后，启动node-inspector调试工具：1node-inspector --web-port [web port] --debug-port [app port] 例如：1node-inspector --web-port 8081 --debug-port 3001 [注：8081为web访问端口；3001为应用调试端口] 3)最后，访问chrome浏览器：具体访问的网址，其实在启动调试工具后会有输出提示，大致如下：1http://127.0.0.1:8081/debug?ws=127.0.0.1:8081&amp;port=3001 具体也可参考如下： 百度经验 用node-inspector调试Node.js"},{"title":"MEAN学习笔记","permalink":"https://erikjiang.github.io/2015/04/08/MEANNote/","text":"(M)ongoDB——NoSQL的文档数据库，使用JSON风格来存储数据，甚至也是使用JS来进行sql查询；(E)xpress——基于Node的Web开发框架；(A)agular——JS的前端开发框架，提供了声明式的双向数据绑定；(N)ode——基于V8的运行时环境（JS语言开发），可以构建快速响应、可扩展的网络应用。 Express安装：1、如果需要安装express 3.x版本，可直接在待安装的包名后添加@版本数，命令行如下：1$ npm install -g express-generator@3 2、Express 4.x版本后，将express命令行工具分离了出来，我们使用全局方式安装时，其命令行如下：1$ npm install -g express-generator 3、在安装结束，我们想要创建一个应用app并进入工程目录：1$ express -e app &amp;&amp; cd app 4、初始化依赖环境：1$ npm install 5、运行这个应用：1$ npm start MongoDB安装：这里没有用源码进行安装，用的是HomeBrew，它是一个缺省包管理器，类似于apt-get、yum这样的工具。 1、我们首先需要更新一下homebrew的包数据库，这个过程可能需要几分钟，终端命令如下：1$ brew update 2、可以安装MongoDB了，命令行如下：1$ brew install mongodb 3、安装成功后，终端会提示如何启动，大概命令如下：1$ mongod --config /usr/local/etc/mongod.conf 4、若已启动，可以再启一个终端，命令行中键入“mongo”即可直接连接到MongoDB； 问题汇总：新工程创建标准：创建一个标准的package.json文件：1$ npm init 该命令将以文本互动的方式生成一份简易的 package.json 文件； 工程内模块的依赖生成；1$ npm install modules_name --save 通过 –save 参数，就能够在你安装模块的同时，自动将模块的依赖写入 package.json。安装完成后，查看 package.json，会多了一项 dependencies 字段，该字段内将罗列具体的依赖模块； package.json中的dependencies字段与devDependencies字段：node package存在两种依赖项，一种是dependencies，而另一种是devDependencies，其中前者的依赖项是正常运行该包时所需要的依赖项，而后者则是进行开发时所需要的依赖项，如一些单元测试的模块包。12\"dependencies\": &#123;&#125; //生产环境\"devDependencies\": &#123;&#125; //开发环境 在package.json所在目录执行npm install的时候，devDependencies里的模块同样会被安装。如果我们只想安装dependencies里面的包，可以执行1npm install –production 如果只安装devDependencies，可以执行1npm install –dev 同理自动更新dependencies字段值的依赖模块：1npm install node_module –save 自动更新devDependencies字段值的依赖模块：1npm install node_module –save-dev。 Most middleware (like session) is no longer bundled with Express and must be installed ……解决方法：大多数的中间件如session，在Express4.x版本之后不会再随着Express包本身一并被安装，而是被分离出来；原本3.x版本的写法，在这里将不再适用：123456789var express = require('express');var MongoStore = require('connect-mongo')(express);app.use(express.session(&#123; secret: settings.cookie_secret, store: new MongoStore(&#123; db: settings.db &#125;)&#125;)); 正确的做法应该首先安装中间件express-session，在package.json文件中添加中间件包名及版本，然后，命令行通过 npm install 来安装；最后修改代码：123456789var session = require('express-session');var MongoStore = require('connect-mongo')(session);app.use(session(&#123; secret: settings.cookie_secret, store: new MongoStore(&#123; db: settings.db &#125;)&#125;)); Redis的安装：到官网下载最新源码，或者通过命令下载：1$ wget http://download.redis.io/releases/redis-3.0.1.tar.gz 解压源码包并编译安装：123$ tar xzf redis-3.0.1.tar.gz$ cd redis-3.0.1$ make 这里比较有意思的是，redis的源码编译与安装只需“make”即可，不像之前编译其他源码要再进行一步“make install”才能安装；安装好后将会在/usr/local/bin/路径下看得到几个命令执行程序：123456redis-benchmark redis-cliredis-check-aof redis-sentinelredis-check-dump redis-server"},{"title":"不要活在假相之中","permalink":"https://erikjiang.github.io/2015/04/06/DontLiveInIllusion/","text":"The Walking Dead 「season 5」I didn’t bring it in. It got inside on its own.They always will…They dead and the living, Because we’re in here.And the ones out there…They’ll hunt us. They’ll find us. They’ll try to use us. They’ll try to kill us.But we’ll kill them. We’ll survive. I’ll show you how.You know, I was thinking…I was thinking how many of you do I have to kill. to save your lives?But I’m not gonna do that. You’re gonna change. 那個你所不知的真相，或者那個你不願接受的真相，也許並沒有想像那樣糟。 真正的糟糕，是你誤陷於假相，卻認為那才是生活的原貌。 真正的糟糕，是你的固執己見，追求著這時空本不存在的絕對。 真正的糟糕，是你已被裹挾，再也沒有勇氣去面對這個亦真亦幻、或好或壞、非敵非友的真實世界。 我們的生活，我們所經歷的，每天都在變動著。 如果你還沒有做出絲毫改變，卻認為一切終會好轉。 那麼假相，它會尾隨你、找到你，然後試著利用你，最終殺掉你。 不要認為你的世界已築起高牆，不會有任何異端可以進入。 要知道它們終將會進來，因為這個世界你在其中。 你，必須做出改變！ 那些刺痛的真相，並不是要阻擋你繼續前行， 而是要告訴你，前行的道路上需要更大的決心。"},{"title":"Mac共享目录挂载","permalink":"https://erikjiang.github.io/2015/04/05/MountMacSharedDir/","text":"最近，这个共享目录挂载的问题一直困扰着我，妈蛋！我的目标很简单，就是需要将一个Mac共享目录mount到Linux中去，但为什么这般折磨……「上帝啊！請寬恕這個狂躁的機智少年！」 共享目录：1、 Mac系统“系统偏好设置”中，有个“共享”功能；2、进入“共享”后，勾选“文件共享”服务，然后在右侧，我们可以添加需要共享的文件夹以及相关用户访问权限；3、如果你需要“SMB”和“AFP”，那就点击“选项…”，添加该服务以及账户； 创建用户：1、Mac系统“系统偏好设置”中，有个“用户与群组”功能，添加一个共享用户；2、在添加前，需要解锁才能操作，解锁后，点击“＋”号，新账户选择“仅限共享”，其他随意，然后“创建用户”； 其余操作：1、需要讲你新创建的“仅限共享”的用户，添加到共享文件夹的用户中去，设置“读与写”权限，这样，该用户才能够访问该文件夹；2、在Linux终端中，首先需要安装cifs： ``` bash yum install cifs-utils ``` 3、Linux创建挂载点： ``` bash sudo mkdir -p /mnt/MountPointName ``` 4、开始挂载： ``` bash mount -t cifs //MacHostOrMacIP/SharedDirectory /mnt/MountPointName/ -o username=UserName,password=Password,nounix,sec=ntlmssp ``` 但是，最终当我执行的时候，它不好使了～``` bash # mount -t cifs //192.168.1.101/Github /mnt/jiangink/ -o username=jiangink,password=123456,nounix,sec=ntlmssp mount error(13): Permission denied Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) ```"},{"title":"二零一五書單","permalink":"https://erikjiang.github.io/2015/03/26/Booklist2015/","text":"Do not go gentle into that good night.不要溫和地踏入那個良夜。Rage, rage against the dying of the light.怒斥、怒斥將逝的時光。 專業相關： 《Node.JS開發指南》 BYVoid 「2015年03月30日 已讀完」 《Javascript語言精萃》 Douglas Crockford 「2015年04月25日 已讀完」 《Node.JS實戰》 CNode社區 《Essential C++》 Stanley B. Lippman 《Effective C++》 Scott Meyers 興趣相關： 《時間的形狀》 汪潔 《量子物理史話》 曹天元 沉積書角： 《生活在別處》 Milan Kundera 《Python基礎教程》 Magnus Lie Hetland 《Head First HTML&amp;CSS》 Elisabeth Robson/Eric Freeman 2015-03-26 執行情況：《Node.JS開發指南》 已讀95頁 第五章 Web開發《時間的形狀》 已讀135頁 第四章 狹義相對論 光速極限 2015-03-30 執行情況：《Node.JS開發指南》 已讀完《時間的形狀》 已讀163頁 第五章 廣義相對論的宇宙 等效原理 2015-04-25 執行情況：《Javascript語言精萃》 已讀完 2015-05-03 執行情況：《時間的形狀》 已讀182頁 第五章 廣義相對論的宇宙 時空彎曲《Node.JS實戰》 第一章 1.4"},{"title":"有关域名解析","permalink":"https://erikjiang.github.io/2015/03/18/ResolveDomain/","text":"这里纪录一下服务端环境，单个域名在对应的多条IP信息情况下，配置与解析处理。 关于hosts允许主机对应多条IP的参数配置 我们知道 gethostbyname() 函数会使用到 hosts 文件进行域名解析，而在解析过程中将按照顺序读取文件内容； /etc/hosts 文件内容如下所示： 1234192.168.1.1 www.dns.com192.168.1.2 www.dns.com192.168.1.3 www.dns.com192.168.1.4 www.dns.com 一般情况下，若未做任何host参数配置的话，仅能够解析到第一条内容(即：192.168.1.1)，但如果在 /etc/host.conf 配置文件中添加 multi on (允许主机存在多条IP)，则能够解析到关于该域名的多条IP信息； 主机信息及IP地址列表 hostent(host entry)，该结构体记录主机的信息，包括主机名、别名、地址类型、地址长度和地址列表。之所以主机的地址是一个列表的形式，原因是当一个主机对应多个网络接口时，就存在多个地址。 123456789 struct hostent &#123; char* h_name; /* official name of host */ char* h_aliases; /* alias list */ short h_addrtype; /* host address type */ short h_length; /* length of address */ char** h_addr_list; /* list of addresses */#define h_addr h_addr_list[0] /* address, for backward compat */ &#125;; h_addr_list 为指针数组，数组中每个元素都是 in_addr 型指针。 具体域名IP解析的代码如下： void resolve_domain_name(const unsigned char* host_name) { struct hostent *host_info = NULL; char target_ip[32]; char **pptr; char *address = NULL; memset(target_ip, 0, sizeof(target_ip)); //获取主机信息 host_info = gethostbyname(host_name); if (NULL == host_info) { printf(\"DNS resolve error![%s]\\n\", host_name); return; } //获取IP地址列表 pptr = host_info-&gt;h_addr_list; while(NULL != *pptr) { //[二进制整数] 转换为 [点分十进制] address = inet_ntoa(*(struct in_addr *)*pptr); //address = inet_ntop(host_info-&gt;h_addrtype, *pptr, target_ip, sizeof(target_ip)); if (NULL == address) { printf(\"Addr convert error!\\n\"); return; } printf(\"IP Address = [%s]\\n\", address); pptr ++; } } 嗯，到这里，另外需要补充的是二进制整数转点分十进制时，“inet_ntoa函数陷阱”的问题，inet_ntoa() 函数返回值为一个指向字符的指针，且是一个由inet_ntoa()函数控制的静态固定指针，在每次调用 inet_ntoa() 后，它将会覆盖上次调用时所得的IP地址。所以如果上一次返回的结果仍然需要，则最好定义一个缓存来做保留。"},{"title":"SSL编程小札","permalink":"https://erikjiang.github.io/2015/03/09/CodingWithSSL/","text":"SSL简介： 安全套接层(Secure Socket Layer，SSL)是一种在两台机器之间提供安全通道的协议。它具有保护传输数据以及识别通信机器的功能。安全通道是透明的，意思就是说它对传输的数据不加变更。客户端与服务器之间的数据是经过加密的，一端写入的数据完全是另一端读取的内容。透明性使得几乎所有基于TCP的协议稍加改动就可以在SSL上运行，非常方便。 SSL最初的版本是由网景(Netscape)设计的，但1996年由于市场份额下滑等因素，网景决定将SSL移交给因特网工程任务组（IETF）进行标准化制定，随后便产生了TLS，也被称作SSL3.1版本； SSL/TLS的特性确保了通信双方可以进行端点认证(ndpoint authentication)互验对方身份，并保证了数据的消息完整性(message integrity)以及私密性(confidentiality)。 SSL编程套路： SSL通信模型采用的是标准C/S结构，因此基于OpenSSL的程序就被分成两个部分：Client和Server。且程序遵循着以下几个重要步骤： OpenSSL初始化在使用OpenSSL之前，必须进行相应的初始化工作。完成初始化功能的函数原型如下：123int SSL_library_int(void); // 初始化SSL算法库函数，调用SSL函数之前必须调用此函数void OpenSSL_add_all_algorithms(void); //加载SSL所有算法void SSL_load_error_strings(void); //错误信息的初始化 在建立SSL连接之前，要为Client和Server分别指定本次连接采用的协议及其版本，目前能够使用的协议版本包括SSLv2、SSLv3、SSLv2/v3和TLSv1.0。SSL连接若要正常建立，则要求Client和Server必须使用相互兼容的协议。 创建CTX在OpenSSL中，CTX是指SSL会话环境。建立连接时使用不同的协议（ 详见 ），其CTX也不一样。创建CTX的过程中会依次用到以下OpenSSL函数：1234567891011//客户端、服务端都需要调用的SSL_CTX_new() //申请SSL会话环境//若有验证对方证书的需求，则需调用SSL_CTX_set_verify() //指定证书验证方式SSL_CTX_load_verify_location() //为SSL会话环境加载本应用所信任的CA证书列表//若有加载证书的需求，则需调用SSL_CTX_use_certificate_file() //为SSL会话加载本应用的证书SSL_CTX_use_certificate_chain_file() //为SSL会话加载本应用的证书所属的证书链SSL_CTX_set_default_passwd_cb_userdata() //设置私钥的密码(如果有的话);注：需要在加载私钥之前设置SSL_CTX_use_PrivateKey_file() //为SSL会话加载本应用的私钥SSL_CTX_check_private_key() //验证所加载的私钥和证书是否相匹配 对于SSL证书的双向认证，这里需要补充的是，可能需要client以及server配置如下的接口函数：12345678/* 设置验证证书链的最大长度 */void SSL_CTX_set_verify_depth(SSL_CTX *ctx,int depth);/* SSL_CTX_set_verify的回调函数，需要自己去写实现 */int verify_callback(int preverify_ok, X509_STORE_CTX *x509_ctx);/* 设置认证的模式及是否开启认证回调 */SSL_CTX_set_verify(SSL_CTX* ctx,int mode,int (*verify_callback)(int,X509_STORE_CTX*));/* 加载CA证书文件 */SSL_CTX_load_verify_locations(SSL_CTX* ctx,const char* CAfile,const char* CApath); 另外还需要注意认证的模式（ 详见 ），常用的模式如下： SSL_VERIFY_NONE: 不会进行证书验证 SSL_VERIFY_PEER: 要求对方提供证书进行验证的模式，该模式在服务端类似于一种自愿验证的模式，当遇到客户端不提供证书时，是否继续通信可自行决定；但该模式运用于客户端，若服务端无证书则会导致SSL握手失败。 SSL_VERIFY_FAIL_IF_NO_PEER_CERT: 服务端使用的一种的模式，此模式下，相当于强制验证客户端的证书，若客户端无证书则SSL握手失败。 建立SSL套接字在此之前要先创建普通的流套接字，完成TCP三次握手，建立普通的TCP连接。然后创建SSL套接字，并将其与流套接字绑定。这一过程中会使用以下几个函数：1234SSL *SSl_new(SSL_CTX *ctx); //申请一个SSL套接字int SSL_set_fd(SSL *ssl,int fd); //绑定读写套接字int SSL_set_rfd(SSL *ssl,int fd); //绑定只读套接字int SSL_set_wfd(SSL *ssl,int fd); //绑定只写套接字 完成SSL握手在这一步，我们需要在普通TCP连接的基础上，建立SSL连接。与普通流套接字建立连接的过程类似：Client使用函数SSL_connect()发起握手（ 类似于流套接字中用的connect() ），而Server使用函数SSL_accept()对握手进行响应（ 类似于流套接字中用的accept() ），从而完成握手过程。两函数原型如下：12int SSL_connect(SSL *ssl);int SSL_accept(SSL *ssl); 鉴别证书信息握手过程完成以后，Client以及Server通常能够获取到对方的证书信息，并对信息进行鉴别。具体实现中会用到以下函数:123X509 *SSL_get_peer_certificate(SSL *ssl); //从SSL套接字中获取对方的证书信息X509_NAME *X509_get_subject_name(X509 *a); //获取证书所有者名字X509_NAME *X509_get_issuer_name(X509 *a); //获取证书颁发者名字 进行数据传输经过前面的一系列过程后，就可以进行安全的数据传输了。在数据传输阶段，需要使用SSL_read( )和SSL_write( )来代替普通流套接字所使用的read( )和write( )函数，以此完成对SSL套接字的读写操作,两个新函数的原型分别如下：12int SSL_read(SSL *ssl,void *buf,int num); //从SSL套接字读取数据int SSL_write(SSL *ssl,const void *buf,int num); //向SSL套接字写入数据 会话结束当Client和Server之间的通信过程完成后，就使用以下函数来释放前面过程中申请的SSL资源：123int SSL_shutdown(SSL *ssl); //关闭SSL套接字void SSl_free(SSL *ssl); //释放SSL套接字void SSL_CTX_free(SSL_CTX *ctx); //释放SSL会话环境 具体源码示范： SSLSocketDemo 本文参考如下资料整理： 1. 《SSL与TLS》 2. 《使用OpenSSL API 建立SSL安全通信的一般流程》"},{"title":"遺忘的事","permalink":"https://erikjiang.github.io/2015/02/04/ForgetThing/","text":"遺忘，是最好的事，有時，也卻是最壞的事……回到成都兩個月有餘，夜裏時常會醒來，每當望向窗外，看著遠處華陽河畔寂靜的街景，心中就會莫名的忐忑，略感缺失。或許是因為離開了北京，沒有了熟悉車流聲和地鐵的震動，讓我覺得反倒有些不適，又或許，是因為別的什麼⋯⋯這種感受若有若無反覆著，無法平息，無法清晰描述。就是在這樣的夜，在無法入眠時，我會常常想起過去的種種，關於已屬於曾經的，那些我以為遺忘的事。 選擇 想到三年之前，決定去北京，那個時候，應算第一次為了自己的想法去做的重要選擇，回顧這三年間的成長，苦悶不堪在所難免，但一路行走堅持到了現在，也有所獲得。時間開始漸漸讓我相信，專注於自己的想法不言放棄，定能獲得存在的真實價值。選擇，得失的權衡，不可避免的失去，有時是無可奈何，而有時是隱忍地堅持，即便會不捨會疼痛，但最終還是做了決斷。一次又一次，你說已找不到原來的樣子，他們卻說這才是生活的原樣。是否仍有意義去質問誰說了謊，可是選擇又從來都沒有對錯⋯⋯ 遠方 路經西安，來到北京，然後輾轉成都，穿梭於一座座城池，活著卻仍然像個孩子。迷惘了問，下一站會是哪兒？徬徨無果後卻隱約聽到『亡命之徒』，李宗盛在耳邊喃喃說詞：“對此我並無更高明的解釋”。說道遠方，是歲月靜好，現世安穩，是晨鐘暮鼓，安之若素⋯⋯但想想或許，又並不只是這些。為何，要如一葉浮萍隨處游離，大概是因為，無法接受妥協於平庸的現在。"},{"title":"離開，為了更好的前行","permalink":"https://erikjiang.github.io/2015/01/27/MoveForward/","text":"在不經意的某個時刻，我所追逐的，或許已不再是你，而是我以為的你，再見，北京。 甲：“我說，還記得我們剛來這裏的時候嗎？天氣像今天一樣好。” 乙：“對啊，可是今天，卻是要離開。” 甲：“來的時候就會知道有離開的一天，這個心理準備我以為我們都有。” 乙：“你好像看的很透，但我覺得，你不會像你說的那樣捨得。” 甲：“這幾年間的記憶都在這裏了，而這裏，將要成為你我的舊時光，說捨得，怎麼可能。” 乙：“我這段時間有時在想，這裏的舊時光會不會只是場夢，只是夢醒了，你我也該走了。” 甲：“如果是場夢，那我倒覺得連夢本身都不屬於我，長達幾年的夢，最終醒來，不多不少，我還是我。” 乙：“那你覺得，這場夢是為了什麼？” 甲：“拜託，不要什麼事都要問為什麼，那你告訴我人的一生是為了什麼？” 沈默⋯⋯ 甲：“我不知道為了什麼，我也不記得來到這裏是為了什麼？” 乙：“我還記得，但我已不想在這裏提及，再也沒有這個必要了⋯⋯” 甲：“是因為要離開嗎？” 乙：“離開只是一個結果。” 甲：“那麼原因呢？” 乙：“因為我所追逐的，已不再是我以為的樣子。”"},{"title":"Hello, World!","permalink":"https://erikjiang.github.io/2015/01/23/HelloWorld/","text":"歡迎來到我的博客，這是我的第一篇Hexo日誌，請多多指教！ 作為開篇日誌，依照慣例，不出所料即定為 Hello World 。正所謂一千隻程序猿，就會有一千種 Hello World ，而在我理解，「Hello World」它代表的是一種探索趣味世界的精神；所以敲出這幾個字母，即是希望這種精神能夠成為我接下來每一天的生活基調，將每一天都過的更有意義，也更有趣味。開設博客，就是這樣一個簡單的初衷，同時也是希望能夠結交更多志同道合的朋友。 進入編程這個行當也有些年月，或許你不會相信，只是在最近的這段時間，我才稍稍發覺，寫程序這個苦逼行當也存在著趣味可言。那麼過去一直以來，為什麼我就沒有過現在這麼高的覺悟呢？ 細細回想，覺得極有可能是被思維模式給“搞拐”了，過去的多數時間都耗費在碎小雜亂的結果上，為了短期功利的結果而去妥協做一件事，甚至於應付一件事，這樣的狀態是最糟糕的，這種背離初衷的做法最終也只會潛藏隱病，慶幸年末果斷的選擇，離開了這種畸形的思維模式，從而避免了捲入漩渦，寫這些只是希望今後自己能夠引以為戒，也希望給小夥伴們一個做事的考量。 好了，我們的主題仍然是關於生活、關於探索趣味世界，我想接下來，會有更多的經歷值得分享，所以我要記錄下去，拭目以待吧！ 「注」：“Hello, World!”最早出現於1972年由貝爾實驗室成員Brian Kernighan撰寫的內部技術資料《Introduction to the Language B》中。對於程序猿而言，它意味著美好的開始。"}]}